// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
use std::fmt::Write;

impl StartCrawlerScheduleInput {
    /// Consumes the builder and constructs an Operation<[`StartCrawlerSchedule`](crate::operation::start_crawler_schedule::StartCrawlerSchedule)>
    #[allow(unused_mut)]
    #[allow(clippy::let_and_return)]
    #[allow(clippy::needless_borrow)]
    pub async fn make_operation(
        &self,
        _config: &crate::config::Config,
    ) -> std::result::Result<
        aws_smithy_http::operation::Operation<
            crate::operation::start_crawler_schedule::StartCrawlerSchedule,
            aws_http::retry::AwsResponseRetryClassifier,
        >,
        aws_smithy_http::operation::error::BuildError,
    > {
        let params_result = crate::endpoint::Params::builder()
            .set_region(_config.region.as_ref().map(|r| r.as_ref().to_owned()))
            .set_use_dual_stack(_config.use_dual_stack)
            .set_use_fips(_config.use_fips)
            .set_endpoint(_config.endpoint_url.clone())
            .build()
            .map_err(|err| {
                aws_smithy_http::endpoint::ResolveEndpointError::from_source(
                    "could not construct endpoint parameters",
                    err,
                )
            });
        let (endpoint_result, params) = match params_result {
            Ok(params) => (
                _config.endpoint_resolver.resolve_endpoint(&params),
                Some(params),
            ),
            Err(e) => (Err(e), None),
        };
        let mut request = {
            fn uri_base(
                _input: &crate::operation::start_crawler_schedule::StartCrawlerScheduleInput,
                output: &mut String,
            ) -> std::result::Result<(), aws_smithy_http::operation::error::BuildError>
            {
                write!(output, "/").expect("formatting should succeed");
                Ok(())
            }
            #[allow(clippy::unnecessary_wraps)]
            fn update_http_builder(
                input: &crate::operation::start_crawler_schedule::StartCrawlerScheduleInput,
                builder: http::request::Builder,
            ) -> std::result::Result<
                http::request::Builder,
                aws_smithy_http::operation::error::BuildError,
            > {
                let mut uri = String::new();
                uri_base(input, &mut uri)?;
                Ok(builder.method("POST").uri(uri))
            }
            let mut builder = update_http_builder(&self, http::request::Builder::new())?;
            builder = aws_smithy_http::header::set_request_header_if_absent(
                builder,
                http::header::CONTENT_TYPE,
                "application/x-amz-json-1.1",
            );
            builder = aws_smithy_http::header::set_request_header_if_absent(
                builder,
                http::header::HeaderName::from_static("x-amz-target"),
                "AWSGlue.StartCrawlerSchedule",
            );
            builder
        };
        let mut properties = aws_smithy_http::property_bag::SharedPropertyBag::new();
        #[allow(clippy::useless_conversion)]
        let body = aws_smithy_http::body::SdkBody::from(
            crate::protocol_serde::shape_start_crawler_schedule::ser_start_crawler_schedule_input(
                &self,
            )?,
        );
        if let Some(content_length) = body.content_length() {
            request = aws_smithy_http::header::set_request_header_if_absent(
                request,
                http::header::CONTENT_LENGTH,
                content_length,
            );
        }
        let request = request.body(body).expect("should be valid request");
        let mut request = aws_smithy_http::operation::Request::from_parts(request, properties);
        request.properties_mut().insert(endpoint_result);
        if let Some(params) = params {
            request.properties_mut().insert(params);
        }
        request
            .properties_mut()
            .insert(aws_smithy_http::http_versions::DEFAULT_HTTP_VERSION_LIST.clone());
        let mut user_agent = aws_http::user_agent::AwsUserAgent::new_from_environment(
            aws_types::os_shim_internal::Env::real(),
            crate::meta::API_METADATA.clone(),
        );
        if let Some(app_name) = _config.app_name() {
            user_agent = user_agent.with_app_name(app_name.clone());
        }
        request.properties_mut().insert(user_agent);
        let mut signing_config = aws_sig_auth::signer::OperationSigningConfig::default_config();
        request.properties_mut().insert(signing_config);
        request
            .properties_mut()
            .insert(aws_types::SigningService::from_static(
                _config.signing_service(),
            ));
        if let Some(region) = &_config.region {
            request
                .properties_mut()
                .insert(aws_types::region::SigningRegion::from(region.clone()));
        }
        if let Some(region) = &_config.region {
            request.properties_mut().insert(region.clone());
        }
        aws_http::auth::set_credentials_cache(
            &mut request.properties_mut(),
            _config.credentials_cache.clone(),
        );
        let op = aws_smithy_http::operation::Operation::new(
            request,
            crate::operation::start_crawler_schedule::StartCrawlerSchedule::new(),
        )
        .with_metadata(aws_smithy_http::operation::Metadata::new(
            "StartCrawlerSchedule",
            "glue",
        ));
        let op = op.with_retry_classifier(aws_http::retry::AwsResponseRetryClassifier::new());
        Ok(op)
    }
}
/// `ParseStrictResponse` impl for `StartCrawlerSchedule`.
#[derive(std::clone::Clone, std::default::Default, std::fmt::Debug)]
#[non_exhaustive]
#[doc(hidden)]
pub struct StartCrawlerSchedule;
impl StartCrawlerSchedule {
    #[doc(hidden)]
    pub fn new() -> Self {
        Self
    }
}
impl aws_smithy_http::response::ParseStrictResponse for StartCrawlerSchedule {
    type Output = std::result::Result<
        crate::operation::start_crawler_schedule::StartCrawlerScheduleOutput,
        crate::operation::start_crawler_schedule::StartCrawlerScheduleError,
    >;
    fn parse(&self, response: &http::Response<bytes::Bytes>) -> Self::Output {
        tracing::debug!(request_id = ?aws_http::request_id::RequestId::request_id(response));
        if !response.status().is_success() && response.status().as_u16() != 200 {
            crate::protocol_serde::shape_start_crawler_schedule::de_start_crawler_schedule_http_error(response)
        } else {
            crate::protocol_serde::shape_start_crawler_schedule::de_start_crawler_schedule_http_response(response)
        }
    }
}

/// Do not use this.
///
/// Operation `*Error/*ErrorKind` types were combined into a single `*Error` enum. The `.kind` field on `*Error` no longer exists and isn't needed anymore (you can just match on the error directly since it's an enum now).
#[deprecated(
    note = "Operation `*Error/*ErrorKind` types were combined into a single `*Error` enum. The `.kind` field on `*Error` no longer exists and isn't needed anymore (you can just match on the error directly since it's an enum now)."
)]
pub type StartCrawlerScheduleErrorKind = StartCrawlerScheduleError;
/// Error type for the `StartCrawlerScheduleError` operation.
#[non_exhaustive]
#[derive(std::fmt::Debug)]
pub enum StartCrawlerScheduleError {
    /// <p>A specified entity does not exist</p>
    EntityNotFoundException(crate::types::error::EntityNotFoundException),
    /// <p>There is no applicable schedule.</p>
    NoScheduleException(crate::types::error::NoScheduleException),
    /// <p>The operation timed out.</p>
    OperationTimeoutException(crate::types::error::OperationTimeoutException),
    /// <p>The specified scheduler is already running.</p>
    SchedulerRunningException(crate::types::error::SchedulerRunningException),
    /// <p>The specified scheduler is transitioning.</p>
    SchedulerTransitioningException(crate::types::error::SchedulerTransitioningException),
    /// An unexpected error occurred (e.g., invalid JSON returned by the service or an unknown error code).
    Unhandled(aws_smithy_types::error::Unhandled),
}
impl aws_smithy_http::result::CreateUnhandledError for StartCrawlerScheduleError {
    fn create_unhandled_error(
        source: Box<dyn std::error::Error + Send + Sync + 'static>,
        meta: std::option::Option<aws_smithy_types::error::ErrorMetadata>,
    ) -> Self {
        Self::Unhandled({
            let mut builder = aws_smithy_types::error::Unhandled::builder().source(source);
            builder.set_meta(meta);
            builder.build()
        })
    }
}
impl std::fmt::Display for StartCrawlerScheduleError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::EntityNotFoundException(_inner) => _inner.fmt(f),
            Self::NoScheduleException(_inner) => _inner.fmt(f),
            Self::OperationTimeoutException(_inner) => _inner.fmt(f),
            Self::SchedulerRunningException(_inner) => _inner.fmt(f),
            Self::SchedulerTransitioningException(_inner) => _inner.fmt(f),
            Self::Unhandled(_inner) => _inner.fmt(f),
        }
    }
}
impl aws_smithy_types::error::metadata::ProvideErrorMetadata for StartCrawlerScheduleError {
    fn meta(&self) -> &aws_smithy_types::error::ErrorMetadata {
        match self {
            Self::EntityNotFoundException(_inner) => {
                aws_smithy_types::error::metadata::ProvideErrorMetadata::meta(_inner)
            }
            Self::NoScheduleException(_inner) => {
                aws_smithy_types::error::metadata::ProvideErrorMetadata::meta(_inner)
            }
            Self::OperationTimeoutException(_inner) => {
                aws_smithy_types::error::metadata::ProvideErrorMetadata::meta(_inner)
            }
            Self::SchedulerRunningException(_inner) => {
                aws_smithy_types::error::metadata::ProvideErrorMetadata::meta(_inner)
            }
            Self::SchedulerTransitioningException(_inner) => {
                aws_smithy_types::error::metadata::ProvideErrorMetadata::meta(_inner)
            }
            Self::Unhandled(_inner) => {
                aws_smithy_types::error::metadata::ProvideErrorMetadata::meta(_inner)
            }
        }
    }
}
impl aws_http::request_id::RequestId
    for crate::operation::start_crawler_schedule::StartCrawlerScheduleError
{
    fn request_id(&self) -> Option<&str> {
        self.meta().request_id()
    }
}
impl aws_smithy_types::retry::ProvideErrorKind for StartCrawlerScheduleError {
    fn code(&self) -> std::option::Option<&str> {
        aws_smithy_types::error::metadata::ProvideErrorMetadata::code(self)
    }
    fn retryable_error_kind(&self) -> std::option::Option<aws_smithy_types::retry::ErrorKind> {
        None
    }
}
impl StartCrawlerScheduleError {
    /// Creates the `StartCrawlerScheduleError::Unhandled` variant from any error type.
    pub fn unhandled(err: impl Into<Box<dyn std::error::Error + Send + Sync + 'static>>) -> Self {
        Self::Unhandled(
            aws_smithy_types::error::Unhandled::builder()
                .source(err)
                .build(),
        )
    }

    /// Creates the `StartCrawlerScheduleError::Unhandled` variant from a `aws_smithy_types::error::ErrorMetadata`.
    pub fn generic(err: aws_smithy_types::error::ErrorMetadata) -> Self {
        Self::Unhandled(
            aws_smithy_types::error::Unhandled::builder()
                .source(err.clone())
                .meta(err)
                .build(),
        )
    }
    ///
    /// Returns error metadata, which includes the error code, message,
    /// request ID, and potentially additional information.
    ///
    pub fn meta(&self) -> &aws_smithy_types::error::ErrorMetadata {
        use aws_smithy_types::error::metadata::ProvideErrorMetadata;
        match self {
            Self::EntityNotFoundException(e) => e.meta(),
            Self::NoScheduleException(e) => e.meta(),
            Self::OperationTimeoutException(e) => e.meta(),
            Self::SchedulerRunningException(e) => e.meta(),
            Self::SchedulerTransitioningException(e) => e.meta(),
            Self::Unhandled(e) => e.meta(),
        }
    }
    /// Returns `true` if the error kind is `StartCrawlerScheduleError::EntityNotFoundException`.
    pub fn is_entity_not_found_exception(&self) -> bool {
        matches!(self, Self::EntityNotFoundException(_))
    }
    /// Returns `true` if the error kind is `StartCrawlerScheduleError::NoScheduleException`.
    pub fn is_no_schedule_exception(&self) -> bool {
        matches!(self, Self::NoScheduleException(_))
    }
    /// Returns `true` if the error kind is `StartCrawlerScheduleError::OperationTimeoutException`.
    pub fn is_operation_timeout_exception(&self) -> bool {
        matches!(self, Self::OperationTimeoutException(_))
    }
    /// Returns `true` if the error kind is `StartCrawlerScheduleError::SchedulerRunningException`.
    pub fn is_scheduler_running_exception(&self) -> bool {
        matches!(self, Self::SchedulerRunningException(_))
    }
    /// Returns `true` if the error kind is `StartCrawlerScheduleError::SchedulerTransitioningException`.
    pub fn is_scheduler_transitioning_exception(&self) -> bool {
        matches!(self, Self::SchedulerTransitioningException(_))
    }
}
impl std::error::Error for StartCrawlerScheduleError {
    fn source(&self) -> std::option::Option<&(dyn std::error::Error + 'static)> {
        match self {
            Self::EntityNotFoundException(_inner) => Some(_inner),
            Self::NoScheduleException(_inner) => Some(_inner),
            Self::OperationTimeoutException(_inner) => Some(_inner),
            Self::SchedulerRunningException(_inner) => Some(_inner),
            Self::SchedulerTransitioningException(_inner) => Some(_inner),
            Self::Unhandled(_inner) => Some(_inner),
        }
    }
}

pub use crate::operation::start_crawler_schedule::_start_crawler_schedule_output::StartCrawlerScheduleOutput;

pub use crate::operation::start_crawler_schedule::_start_crawler_schedule_input::StartCrawlerScheduleInput;

mod _start_crawler_schedule_input;

mod _start_crawler_schedule_output;

/// Builders
pub mod builders;
