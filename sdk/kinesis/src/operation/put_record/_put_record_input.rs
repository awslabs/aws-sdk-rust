// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Represents the input for <code>PutRecord</code>.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct PutRecordInput  {
    /// <p>The name of the stream to put the data record into.</p>
    #[doc(hidden)]
    pub stream_name: std::option::Option<std::string::String>,
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    #[doc(hidden)]
    pub data: std::option::Option<aws_smithy_types::Blob>,
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    #[doc(hidden)]
    pub partition_key: std::option::Option<std::string::String>,
    /// <p>The hash value used to explicitly determine the shard the data record is assigned to by overriding the partition key hash.</p>
    #[doc(hidden)]
    pub explicit_hash_key: std::option::Option<std::string::String>,
    /// <p>Guarantees strictly increasing sequence numbers, for puts from the same client and to the same partition key. Usage: set the <code>SequenceNumberForOrdering</code> of record <i>n</i> to the sequence number of record <i>n-1</i> (as returned in the result when putting record <i>n-1</i>). If this parameter is not set, records are coarsely ordered based on arrival time.</p>
    #[doc(hidden)]
    pub sequence_number_for_ordering: std::option::Option<std::string::String>,
    /// <p>The ARN of the stream.</p>
    #[doc(hidden)]
    pub stream_arn: std::option::Option<std::string::String>,
}
impl PutRecordInput {
    /// <p>The name of the stream to put the data record into.</p>
    pub fn stream_name(&self) -> std::option::Option<& str> {
        self.stream_name.as_deref()
    }
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    pub fn data(&self) -> std::option::Option<& aws_smithy_types::Blob> {
        self.data.as_ref()
    }
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    pub fn partition_key(&self) -> std::option::Option<& str> {
        self.partition_key.as_deref()
    }
    /// <p>The hash value used to explicitly determine the shard the data record is assigned to by overriding the partition key hash.</p>
    pub fn explicit_hash_key(&self) -> std::option::Option<& str> {
        self.explicit_hash_key.as_deref()
    }
    /// <p>Guarantees strictly increasing sequence numbers, for puts from the same client and to the same partition key. Usage: set the <code>SequenceNumberForOrdering</code> of record <i>n</i> to the sequence number of record <i>n-1</i> (as returned in the result when putting record <i>n-1</i>). If this parameter is not set, records are coarsely ordered based on arrival time.</p>
    pub fn sequence_number_for_ordering(&self) -> std::option::Option<& str> {
        self.sequence_number_for_ordering.as_deref()
    }
    /// <p>The ARN of the stream.</p>
    pub fn stream_arn(&self) -> std::option::Option<& str> {
        self.stream_arn.as_deref()
    }
}
impl PutRecordInput {
    /// Creates a new builder-style object to manufacture [`PutRecordInput`](crate::operation::put_record::PutRecordInput).
    pub fn builder() -> crate::operation::put_record::builders::PutRecordInputBuilder {
        crate::operation::put_record::builders::PutRecordInputBuilder::default()
    }
}

/// A builder for [`PutRecordInput`](crate::operation::put_record::PutRecordInput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct PutRecordInputBuilder {
    pub(crate) stream_name: std::option::Option<std::string::String>,
    pub(crate) data: std::option::Option<aws_smithy_types::Blob>,
    pub(crate) partition_key: std::option::Option<std::string::String>,
    pub(crate) explicit_hash_key: std::option::Option<std::string::String>,
    pub(crate) sequence_number_for_ordering: std::option::Option<std::string::String>,
    pub(crate) stream_arn: std::option::Option<std::string::String>,
}
impl PutRecordInputBuilder {
    /// <p>The name of the stream to put the data record into.</p>
    pub fn stream_name(mut self, input: impl Into<std::string::String>) -> Self {
        self.stream_name = Some(input.into());
        self
    }
    /// <p>The name of the stream to put the data record into.</p>
    pub fn set_stream_name(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.stream_name = input; self
    }
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    pub fn data(mut self, input: aws_smithy_types::Blob) -> Self {
        self.data = Some(input);
        self
    }
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    pub fn set_data(mut self, input: std::option::Option<aws_smithy_types::Blob>) -> Self {
        self.data = input; self
    }
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    pub fn partition_key(mut self, input: impl Into<std::string::String>) -> Self {
        self.partition_key = Some(input.into());
        self
    }
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    pub fn set_partition_key(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.partition_key = input; self
    }
    /// <p>The hash value used to explicitly determine the shard the data record is assigned to by overriding the partition key hash.</p>
    pub fn explicit_hash_key(mut self, input: impl Into<std::string::String>) -> Self {
        self.explicit_hash_key = Some(input.into());
        self
    }
    /// <p>The hash value used to explicitly determine the shard the data record is assigned to by overriding the partition key hash.</p>
    pub fn set_explicit_hash_key(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.explicit_hash_key = input; self
    }
    /// <p>Guarantees strictly increasing sequence numbers, for puts from the same client and to the same partition key. Usage: set the <code>SequenceNumberForOrdering</code> of record <i>n</i> to the sequence number of record <i>n-1</i> (as returned in the result when putting record <i>n-1</i>). If this parameter is not set, records are coarsely ordered based on arrival time.</p>
    pub fn sequence_number_for_ordering(mut self, input: impl Into<std::string::String>) -> Self {
        self.sequence_number_for_ordering = Some(input.into());
        self
    }
    /// <p>Guarantees strictly increasing sequence numbers, for puts from the same client and to the same partition key. Usage: set the <code>SequenceNumberForOrdering</code> of record <i>n</i> to the sequence number of record <i>n-1</i> (as returned in the result when putting record <i>n-1</i>). If this parameter is not set, records are coarsely ordered based on arrival time.</p>
    pub fn set_sequence_number_for_ordering(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.sequence_number_for_ordering = input; self
    }
    /// <p>The ARN of the stream.</p>
    pub fn stream_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.stream_arn = Some(input.into());
        self
    }
    /// <p>The ARN of the stream.</p>
    pub fn set_stream_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.stream_arn = input; self
    }
    /// Consumes the builder and constructs a [`PutRecordInput`](crate::operation::put_record::PutRecordInput).
    pub fn build(self) -> Result<crate::operation::put_record::PutRecordInput, aws_smithy_http::operation::error::BuildError> {
        Ok(
            crate::operation::put_record::PutRecordInput {
                stream_name: self.stream_name
                ,
                data: self.data
                ,
                partition_key: self.partition_key
                ,
                explicit_hash_key: self.explicit_hash_key
                ,
                sequence_number_for_ordering: self.sequence_number_for_ordering
                ,
                stream_arn: self.stream_arn
                ,
            }
        )
    }
}

