// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Represents the output for <code>PutRecords</code>.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct PutRecordsRequestEntry {
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    #[doc(hidden)]
    pub data: std::option::Option<aws_smithy_types::Blob>,
    /// <p>The hash value used to determine explicitly the shard that the data record is assigned to by overriding the partition key hash.</p>
    #[doc(hidden)]
    pub explicit_hash_key: std::option::Option<std::string::String>,
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    #[doc(hidden)]
    pub partition_key: std::option::Option<std::string::String>,
}
impl PutRecordsRequestEntry {
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    pub fn data(&self) -> std::option::Option<&aws_smithy_types::Blob> {
        self.data.as_ref()
    }
    /// <p>The hash value used to determine explicitly the shard that the data record is assigned to by overriding the partition key hash.</p>
    pub fn explicit_hash_key(&self) -> std::option::Option<&str> {
        self.explicit_hash_key.as_deref()
    }
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    pub fn partition_key(&self) -> std::option::Option<&str> {
        self.partition_key.as_deref()
    }
}
impl PutRecordsRequestEntry {
    /// Creates a new builder-style object to manufacture [`PutRecordsRequestEntry`](crate::types::PutRecordsRequestEntry).
    pub fn builder() -> crate::types::builders::PutRecordsRequestEntryBuilder {
        crate::types::builders::PutRecordsRequestEntryBuilder::default()
    }
}

/// A builder for [`PutRecordsRequestEntry`](crate::types::PutRecordsRequestEntry).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct PutRecordsRequestEntryBuilder {
    pub(crate) data: std::option::Option<aws_smithy_types::Blob>,
    pub(crate) explicit_hash_key: std::option::Option<std::string::String>,
    pub(crate) partition_key: std::option::Option<std::string::String>,
}
impl PutRecordsRequestEntryBuilder {
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    pub fn data(mut self, input: aws_smithy_types::Blob) -> Self {
        self.data = Some(input);
        self
    }
    /// <p>The data blob to put into the record, which is base64-encoded when the blob is serialized. When the data blob (the payload before base64-encoding) is added to the partition key size, the total size must not exceed the maximum record size (1 MiB).</p>
    pub fn set_data(mut self, input: std::option::Option<aws_smithy_types::Blob>) -> Self {
        self.data = input;
        self
    }
    /// <p>The hash value used to determine explicitly the shard that the data record is assigned to by overriding the partition key hash.</p>
    pub fn explicit_hash_key(mut self, input: impl Into<std::string::String>) -> Self {
        self.explicit_hash_key = Some(input.into());
        self
    }
    /// <p>The hash value used to determine explicitly the shard that the data record is assigned to by overriding the partition key hash.</p>
    pub fn set_explicit_hash_key(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.explicit_hash_key = input;
        self
    }
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    pub fn partition_key(mut self, input: impl Into<std::string::String>) -> Self {
        self.partition_key = Some(input.into());
        self
    }
    /// <p>Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.</p>
    pub fn set_partition_key(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.partition_key = input;
        self
    }
    /// Consumes the builder and constructs a [`PutRecordsRequestEntry`](crate::types::PutRecordsRequestEntry).
    pub fn build(self) -> crate::types::PutRecordsRequestEntry {
        crate::types::PutRecordsRequestEntry {
            data: self.data,
            explicit_hash_key: self.explicit_hash_key,
            partition_key: self.partition_key,
        }
    }
}
