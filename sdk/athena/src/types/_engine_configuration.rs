// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Contains data processing unit (DPU) configuration settings and parameter mappings for a notebook engine.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct EngineConfiguration {
    /// <p>The number of DPUs to use for the coordinator. A coordinator is a special executor that orchestrates processing work and manages other executors in a notebook session. The default is 1.</p>
    pub coordinator_dpu_size: ::std::option::Option<i32>,
    /// <p>The maximum number of DPUs that can run concurrently.</p>
    pub max_concurrent_dpus: i32,
    /// <p>The default number of DPUs to use for executors. An executor is the smallest unit of compute that a notebook session can request from Athena. The default is 1.</p>
    pub default_executor_dpu_size: ::std::option::Option<i32>,
    /// <p>Contains additional notebook engine <code>MAP
    /// <string, string></string,></code> parameter mappings in the form of key-value pairs. To specify an Athena notebook that the Jupyter server will download and serve, specify a value for the <code>StartSessionRequest$NotebookVersion</code> field, and then add a key named <code>NotebookId</code> to <code>AdditionalConfigs</code> that has the value of the Athena notebook ID.</p>
    pub additional_configs: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    /// <p>Specifies custom jar files and Spark properties for use cases like cluster encryption, table formats, and general Spark tuning.</p>
    pub spark_properties: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
}
impl EngineConfiguration {
    /// <p>The number of DPUs to use for the coordinator. A coordinator is a special executor that orchestrates processing work and manages other executors in a notebook session. The default is 1.</p>
    pub fn coordinator_dpu_size(&self) -> ::std::option::Option<i32> {
        self.coordinator_dpu_size
    }
    /// <p>The maximum number of DPUs that can run concurrently.</p>
    pub fn max_concurrent_dpus(&self) -> i32 {
        self.max_concurrent_dpus
    }
    /// <p>The default number of DPUs to use for executors. An executor is the smallest unit of compute that a notebook session can request from Athena. The default is 1.</p>
    pub fn default_executor_dpu_size(&self) -> ::std::option::Option<i32> {
        self.default_executor_dpu_size
    }
    /// <p>Contains additional notebook engine <code>MAP
    /// <string, string></string,></code> parameter mappings in the form of key-value pairs. To specify an Athena notebook that the Jupyter server will download and serve, specify a value for the <code>StartSessionRequest$NotebookVersion</code> field, and then add a key named <code>NotebookId</code> to <code>AdditionalConfigs</code> that has the value of the Athena notebook ID.</p>
    pub fn additional_configs(&self) -> ::std::option::Option<&::std::collections::HashMap<::std::string::String, ::std::string::String>> {
        self.additional_configs.as_ref()
    }
    /// <p>Specifies custom jar files and Spark properties for use cases like cluster encryption, table formats, and general Spark tuning.</p>
    pub fn spark_properties(&self) -> ::std::option::Option<&::std::collections::HashMap<::std::string::String, ::std::string::String>> {
        self.spark_properties.as_ref()
    }
}
impl EngineConfiguration {
    /// Creates a new builder-style object to manufacture [`EngineConfiguration`](crate::types::EngineConfiguration).
    pub fn builder() -> crate::types::builders::EngineConfigurationBuilder {
        crate::types::builders::EngineConfigurationBuilder::default()
    }
}

/// A builder for [`EngineConfiguration`](crate::types::EngineConfiguration).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct EngineConfigurationBuilder {
    pub(crate) coordinator_dpu_size: ::std::option::Option<i32>,
    pub(crate) max_concurrent_dpus: ::std::option::Option<i32>,
    pub(crate) default_executor_dpu_size: ::std::option::Option<i32>,
    pub(crate) additional_configs: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    pub(crate) spark_properties: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
}
impl EngineConfigurationBuilder {
    /// <p>The number of DPUs to use for the coordinator. A coordinator is a special executor that orchestrates processing work and manages other executors in a notebook session. The default is 1.</p>
    pub fn coordinator_dpu_size(mut self, input: i32) -> Self {
        self.coordinator_dpu_size = ::std::option::Option::Some(input);
        self
    }
    /// <p>The number of DPUs to use for the coordinator. A coordinator is a special executor that orchestrates processing work and manages other executors in a notebook session. The default is 1.</p>
    pub fn set_coordinator_dpu_size(mut self, input: ::std::option::Option<i32>) -> Self {
        self.coordinator_dpu_size = input;
        self
    }
    /// <p>The number of DPUs to use for the coordinator. A coordinator is a special executor that orchestrates processing work and manages other executors in a notebook session. The default is 1.</p>
    pub fn get_coordinator_dpu_size(&self) -> &::std::option::Option<i32> {
        &self.coordinator_dpu_size
    }
    /// <p>The maximum number of DPUs that can run concurrently.</p>
    /// This field is required.
    pub fn max_concurrent_dpus(mut self, input: i32) -> Self {
        self.max_concurrent_dpus = ::std::option::Option::Some(input);
        self
    }
    /// <p>The maximum number of DPUs that can run concurrently.</p>
    pub fn set_max_concurrent_dpus(mut self, input: ::std::option::Option<i32>) -> Self {
        self.max_concurrent_dpus = input;
        self
    }
    /// <p>The maximum number of DPUs that can run concurrently.</p>
    pub fn get_max_concurrent_dpus(&self) -> &::std::option::Option<i32> {
        &self.max_concurrent_dpus
    }
    /// <p>The default number of DPUs to use for executors. An executor is the smallest unit of compute that a notebook session can request from Athena. The default is 1.</p>
    pub fn default_executor_dpu_size(mut self, input: i32) -> Self {
        self.default_executor_dpu_size = ::std::option::Option::Some(input);
        self
    }
    /// <p>The default number of DPUs to use for executors. An executor is the smallest unit of compute that a notebook session can request from Athena. The default is 1.</p>
    pub fn set_default_executor_dpu_size(mut self, input: ::std::option::Option<i32>) -> Self {
        self.default_executor_dpu_size = input;
        self
    }
    /// <p>The default number of DPUs to use for executors. An executor is the smallest unit of compute that a notebook session can request from Athena. The default is 1.</p>
    pub fn get_default_executor_dpu_size(&self) -> &::std::option::Option<i32> {
        &self.default_executor_dpu_size
    }
    /// Adds a key-value pair to `additional_configs`.
    ///
    /// To override the contents of this collection use [`set_additional_configs`](Self::set_additional_configs).
    ///
    /// <p>Contains additional notebook engine <code>MAP
    /// <string, string></string,></code> parameter mappings in the form of key-value pairs. To specify an Athena notebook that the Jupyter server will download and serve, specify a value for the <code>StartSessionRequest$NotebookVersion</code> field, and then add a key named <code>NotebookId</code> to <code>AdditionalConfigs</code> that has the value of the Athena notebook ID.</p>
    pub fn additional_configs(
        mut self,
        k: impl ::std::convert::Into<::std::string::String>,
        v: impl ::std::convert::Into<::std::string::String>,
    ) -> Self {
        let mut hash_map = self.additional_configs.unwrap_or_default();
        hash_map.insert(k.into(), v.into());
        self.additional_configs = ::std::option::Option::Some(hash_map);
        self
    }
    /// <p>Contains additional notebook engine <code>MAP
    /// <string, string></string,></code> parameter mappings in the form of key-value pairs. To specify an Athena notebook that the Jupyter server will download and serve, specify a value for the <code>StartSessionRequest$NotebookVersion</code> field, and then add a key named <code>NotebookId</code> to <code>AdditionalConfigs</code> that has the value of the Athena notebook ID.</p>
    pub fn set_additional_configs(
        mut self,
        input: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    ) -> Self {
        self.additional_configs = input;
        self
    }
    /// <p>Contains additional notebook engine <code>MAP
    /// <string, string></string,></code> parameter mappings in the form of key-value pairs. To specify an Athena notebook that the Jupyter server will download and serve, specify a value for the <code>StartSessionRequest$NotebookVersion</code> field, and then add a key named <code>NotebookId</code> to <code>AdditionalConfigs</code> that has the value of the Athena notebook ID.</p>
    pub fn get_additional_configs(&self) -> &::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>> {
        &self.additional_configs
    }
    /// Adds a key-value pair to `spark_properties`.
    ///
    /// To override the contents of this collection use [`set_spark_properties`](Self::set_spark_properties).
    ///
    /// <p>Specifies custom jar files and Spark properties for use cases like cluster encryption, table formats, and general Spark tuning.</p>
    pub fn spark_properties(
        mut self,
        k: impl ::std::convert::Into<::std::string::String>,
        v: impl ::std::convert::Into<::std::string::String>,
    ) -> Self {
        let mut hash_map = self.spark_properties.unwrap_or_default();
        hash_map.insert(k.into(), v.into());
        self.spark_properties = ::std::option::Option::Some(hash_map);
        self
    }
    /// <p>Specifies custom jar files and Spark properties for use cases like cluster encryption, table formats, and general Spark tuning.</p>
    pub fn set_spark_properties(
        mut self,
        input: ::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>>,
    ) -> Self {
        self.spark_properties = input;
        self
    }
    /// <p>Specifies custom jar files and Spark properties for use cases like cluster encryption, table formats, and general Spark tuning.</p>
    pub fn get_spark_properties(&self) -> &::std::option::Option<::std::collections::HashMap<::std::string::String, ::std::string::String>> {
        &self.spark_properties
    }
    /// Consumes the builder and constructs a [`EngineConfiguration`](crate::types::EngineConfiguration).
    /// This method will fail if any of the following fields are not set:
    /// - [`max_concurrent_dpus`](crate::types::builders::EngineConfigurationBuilder::max_concurrent_dpus)
    pub fn build(self) -> ::std::result::Result<crate::types::EngineConfiguration, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::EngineConfiguration {
            coordinator_dpu_size: self.coordinator_dpu_size,
            max_concurrent_dpus: self.max_concurrent_dpus.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "max_concurrent_dpus",
                    "max_concurrent_dpus was not specified but it is required when building EngineConfiguration",
                )
            })?,
            default_executor_dpu_size: self.default_executor_dpu_size,
            additional_configs: self.additional_configs,
            spark_properties: self.spark_properties,
        })
    }
}
