// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
impl super::Client {
    /// Constructs a fluent builder for the [`AnalyzeDocument`](crate::client::fluent_builders::AnalyzeDocument) operation.
    ///
    /// - The fluent builder is configurable:
    ///   - [`document(Document)`](crate::client::fluent_builders::AnalyzeDocument::document) / [`set_document(Option<Document>)`](crate::client::fluent_builders::AnalyzeDocument::set_document): <p>The input document as base64-encoded bytes or an Amazon S3 object. If you use the AWS CLI to call Amazon Textract operations, you can't pass image bytes. The document must be an image in JPEG, PNG, PDF, or TIFF format.</p>  <p>If you're using an AWS SDK to call Amazon Textract, you might not need to base64-encode image bytes that are passed using the <code>Bytes</code> field. </p>
    ///   - [`feature_types(Vec<FeatureType>)`](crate::client::fluent_builders::AnalyzeDocument::feature_types) / [`set_feature_types(Option<Vec<FeatureType>>)`](crate::client::fluent_builders::AnalyzeDocument::set_feature_types): <p>A list of the types of analysis to perform. Add TABLES to the list to return information about the tables that are detected in the input document. Add FORMS to return detected form data. Add SIGNATURES to return the locations of detected signatures. To perform both forms and table analysis, add TABLES and FORMS to <code>FeatureTypes</code>. To detect signatures within form data and table data, add SIGNATURES to either TABLES or FORMS. All lines and words detected in the document are included in the response (including text that isn't related to the value of <code>FeatureTypes</code>). </p>
    ///   - [`human_loop_config(HumanLoopConfig)`](crate::client::fluent_builders::AnalyzeDocument::human_loop_config) / [`set_human_loop_config(Option<HumanLoopConfig>)`](crate::client::fluent_builders::AnalyzeDocument::set_human_loop_config): <p>Sets the configuration for the human in the loop workflow for analyzing documents.</p>
    ///   - [`queries_config(QueriesConfig)`](crate::client::fluent_builders::AnalyzeDocument::queries_config) / [`set_queries_config(Option<QueriesConfig>)`](crate::client::fluent_builders::AnalyzeDocument::set_queries_config): <p>Contains Queries and the alias for those Queries, as determined by the input. </p>
    /// - On success, responds with [`AnalyzeDocumentOutput`](crate::output::AnalyzeDocumentOutput) with field(s):
    ///   - [`document_metadata(Option<DocumentMetadata>)`](crate::output::AnalyzeDocumentOutput::document_metadata): <p>Metadata about the analyzed document. An example is the number of pages.</p>
    ///   - [`blocks(Option<Vec<Block>>)`](crate::output::AnalyzeDocumentOutput::blocks): <p>The items that are detected and analyzed by <code>AnalyzeDocument</code>.</p>
    ///   - [`human_loop_activation_output(Option<HumanLoopActivationOutput>)`](crate::output::AnalyzeDocumentOutput::human_loop_activation_output): <p>Shows the results of the human in the loop evaluation.</p>
    ///   - [`analyze_document_model_version(Option<String>)`](crate::output::AnalyzeDocumentOutput::analyze_document_model_version): <p>The version of the model used to analyze the document.</p>
    /// - On failure, responds with [`SdkError<AnalyzeDocumentError>`](crate::error::AnalyzeDocumentError)
    pub fn analyze_document(&self) -> crate::client::fluent_builders::AnalyzeDocument {
        crate::client::fluent_builders::AnalyzeDocument::new(self.handle.clone())
    }
}
