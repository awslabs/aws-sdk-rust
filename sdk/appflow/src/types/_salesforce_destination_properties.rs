// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p> The properties that are applied when Salesforce is being used as a destination. </p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct SalesforceDestinationProperties {
    /// <p> The object specified in the Salesforce flow destination. </p>
    pub object: ::std::string::String,
    /// <p> The name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete. </p>
    pub id_field_names: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    /// <p> The settings that determine how Amazon AppFlow handles an error when placing data in the Salesforce destination. For example, this setting would determine if the flow should fail after one insertion error, or continue and attempt to insert every record regardless of the initial failure. <code>ErrorHandlingConfig</code> is a part of the destination connector details. </p>
    pub error_handling_config: ::std::option::Option<crate::types::ErrorHandlingConfig>,
    /// <p> This specifies the type of write operation to be performed in Salesforce. When the value is <code>UPSERT</code>, then <code>idFieldNames</code> is required. </p>
    pub write_operation_type: ::std::option::Option<crate::types::WriteOperationType>,
    /// <p>Specifies which Salesforce API is used by Amazon AppFlow when your flow transfers data to Salesforce.</p>
    /// <dl>
    /// <dt>
    /// AUTOMATIC
    /// </dt>
    /// <dd>
    /// <p>The default. Amazon AppFlow selects which API to use based on the number of records that your flow transfers to Salesforce. If your flow transfers fewer than 1,000 records, Amazon AppFlow uses Salesforce REST API. If your flow transfers 1,000 records or more, Amazon AppFlow uses Salesforce Bulk API 2.0.</p>
    /// <p>Each of these Salesforce APIs structures data differently. If Amazon AppFlow selects the API automatically, be aware that, for recurring flows, the data output might vary from one flow run to the next. For example, if a flow runs daily, it might use REST API on one day to transfer 900 records, and it might use Bulk API 2.0 on the next day to transfer 1,100 records. For each of these flow runs, the respective Salesforce API formats the data differently. Some of the differences include how dates are formatted and null values are represented. Also, Bulk API 2.0 doesn't transfer Salesforce compound fields.</p>
    /// <p>By choosing this option, you optimize flow performance for both small and large data transfers, but the tradeoff is inconsistent formatting in the output.</p>
    /// </dd>
    /// <dt>
    /// BULKV2
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce Bulk API 2.0. This API runs asynchronous data transfers, and it's optimal for large sets of data. By choosing this option, you ensure that your flow writes consistent output, but you optimize performance only for large data transfers.</p>
    /// <p>Note that Bulk API 2.0 does not transfer Salesforce compound fields.</p>
    /// </dd>
    /// <dt>
    /// REST_SYNC
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce REST API. By choosing this option, you ensure that your flow writes consistent output, but you decrease performance for large data transfers that are better suited for Bulk API 2.0. In some cases, if your flow attempts to transfer a vary large set of data, it might fail with a timed out error.</p>
    /// </dd>
    /// </dl>
    pub data_transfer_api: ::std::option::Option<crate::types::SalesforceDataTransferApi>,
}
impl SalesforceDestinationProperties {
    /// <p> The object specified in the Salesforce flow destination. </p>
    pub fn object(&self) -> &str {
        use std::ops::Deref;
        self.object.deref()
    }
    /// <p> The name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete. </p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.id_field_names.is_none()`.
    pub fn id_field_names(&self) -> &[::std::string::String] {
        self.id_field_names.as_deref().unwrap_or_default()
    }
    /// <p> The settings that determine how Amazon AppFlow handles an error when placing data in the Salesforce destination. For example, this setting would determine if the flow should fail after one insertion error, or continue and attempt to insert every record regardless of the initial failure. <code>ErrorHandlingConfig</code> is a part of the destination connector details. </p>
    pub fn error_handling_config(&self) -> ::std::option::Option<&crate::types::ErrorHandlingConfig> {
        self.error_handling_config.as_ref()
    }
    /// <p> This specifies the type of write operation to be performed in Salesforce. When the value is <code>UPSERT</code>, then <code>idFieldNames</code> is required. </p>
    pub fn write_operation_type(&self) -> ::std::option::Option<&crate::types::WriteOperationType> {
        self.write_operation_type.as_ref()
    }
    /// <p>Specifies which Salesforce API is used by Amazon AppFlow when your flow transfers data to Salesforce.</p>
    /// <dl>
    /// <dt>
    /// AUTOMATIC
    /// </dt>
    /// <dd>
    /// <p>The default. Amazon AppFlow selects which API to use based on the number of records that your flow transfers to Salesforce. If your flow transfers fewer than 1,000 records, Amazon AppFlow uses Salesforce REST API. If your flow transfers 1,000 records or more, Amazon AppFlow uses Salesforce Bulk API 2.0.</p>
    /// <p>Each of these Salesforce APIs structures data differently. If Amazon AppFlow selects the API automatically, be aware that, for recurring flows, the data output might vary from one flow run to the next. For example, if a flow runs daily, it might use REST API on one day to transfer 900 records, and it might use Bulk API 2.0 on the next day to transfer 1,100 records. For each of these flow runs, the respective Salesforce API formats the data differently. Some of the differences include how dates are formatted and null values are represented. Also, Bulk API 2.0 doesn't transfer Salesforce compound fields.</p>
    /// <p>By choosing this option, you optimize flow performance for both small and large data transfers, but the tradeoff is inconsistent formatting in the output.</p>
    /// </dd>
    /// <dt>
    /// BULKV2
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce Bulk API 2.0. This API runs asynchronous data transfers, and it's optimal for large sets of data. By choosing this option, you ensure that your flow writes consistent output, but you optimize performance only for large data transfers.</p>
    /// <p>Note that Bulk API 2.0 does not transfer Salesforce compound fields.</p>
    /// </dd>
    /// <dt>
    /// REST_SYNC
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce REST API. By choosing this option, you ensure that your flow writes consistent output, but you decrease performance for large data transfers that are better suited for Bulk API 2.0. In some cases, if your flow attempts to transfer a vary large set of data, it might fail with a timed out error.</p>
    /// </dd>
    /// </dl>
    pub fn data_transfer_api(&self) -> ::std::option::Option<&crate::types::SalesforceDataTransferApi> {
        self.data_transfer_api.as_ref()
    }
}
impl SalesforceDestinationProperties {
    /// Creates a new builder-style object to manufacture [`SalesforceDestinationProperties`](crate::types::SalesforceDestinationProperties).
    pub fn builder() -> crate::types::builders::SalesforceDestinationPropertiesBuilder {
        crate::types::builders::SalesforceDestinationPropertiesBuilder::default()
    }
}

/// A builder for [`SalesforceDestinationProperties`](crate::types::SalesforceDestinationProperties).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct SalesforceDestinationPropertiesBuilder {
    pub(crate) object: ::std::option::Option<::std::string::String>,
    pub(crate) id_field_names: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    pub(crate) error_handling_config: ::std::option::Option<crate::types::ErrorHandlingConfig>,
    pub(crate) write_operation_type: ::std::option::Option<crate::types::WriteOperationType>,
    pub(crate) data_transfer_api: ::std::option::Option<crate::types::SalesforceDataTransferApi>,
}
impl SalesforceDestinationPropertiesBuilder {
    /// <p> The object specified in the Salesforce flow destination. </p>
    /// This field is required.
    pub fn object(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.object = ::std::option::Option::Some(input.into());
        self
    }
    /// <p> The object specified in the Salesforce flow destination. </p>
    pub fn set_object(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.object = input;
        self
    }
    /// <p> The object specified in the Salesforce flow destination. </p>
    pub fn get_object(&self) -> &::std::option::Option<::std::string::String> {
        &self.object
    }
    /// Appends an item to `id_field_names`.
    ///
    /// To override the contents of this collection use [`set_id_field_names`](Self::set_id_field_names).
    ///
    /// <p> The name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete. </p>
    pub fn id_field_names(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.id_field_names.unwrap_or_default();
        v.push(input.into());
        self.id_field_names = ::std::option::Option::Some(v);
        self
    }
    /// <p> The name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete. </p>
    pub fn set_id_field_names(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.id_field_names = input;
        self
    }
    /// <p> The name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete. </p>
    pub fn get_id_field_names(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.id_field_names
    }
    /// <p> The settings that determine how Amazon AppFlow handles an error when placing data in the Salesforce destination. For example, this setting would determine if the flow should fail after one insertion error, or continue and attempt to insert every record regardless of the initial failure. <code>ErrorHandlingConfig</code> is a part of the destination connector details. </p>
    pub fn error_handling_config(mut self, input: crate::types::ErrorHandlingConfig) -> Self {
        self.error_handling_config = ::std::option::Option::Some(input);
        self
    }
    /// <p> The settings that determine how Amazon AppFlow handles an error when placing data in the Salesforce destination. For example, this setting would determine if the flow should fail after one insertion error, or continue and attempt to insert every record regardless of the initial failure. <code>ErrorHandlingConfig</code> is a part of the destination connector details. </p>
    pub fn set_error_handling_config(mut self, input: ::std::option::Option<crate::types::ErrorHandlingConfig>) -> Self {
        self.error_handling_config = input;
        self
    }
    /// <p> The settings that determine how Amazon AppFlow handles an error when placing data in the Salesforce destination. For example, this setting would determine if the flow should fail after one insertion error, or continue and attempt to insert every record regardless of the initial failure. <code>ErrorHandlingConfig</code> is a part of the destination connector details. </p>
    pub fn get_error_handling_config(&self) -> &::std::option::Option<crate::types::ErrorHandlingConfig> {
        &self.error_handling_config
    }
    /// <p> This specifies the type of write operation to be performed in Salesforce. When the value is <code>UPSERT</code>, then <code>idFieldNames</code> is required. </p>
    pub fn write_operation_type(mut self, input: crate::types::WriteOperationType) -> Self {
        self.write_operation_type = ::std::option::Option::Some(input);
        self
    }
    /// <p> This specifies the type of write operation to be performed in Salesforce. When the value is <code>UPSERT</code>, then <code>idFieldNames</code> is required. </p>
    pub fn set_write_operation_type(mut self, input: ::std::option::Option<crate::types::WriteOperationType>) -> Self {
        self.write_operation_type = input;
        self
    }
    /// <p> This specifies the type of write operation to be performed in Salesforce. When the value is <code>UPSERT</code>, then <code>idFieldNames</code> is required. </p>
    pub fn get_write_operation_type(&self) -> &::std::option::Option<crate::types::WriteOperationType> {
        &self.write_operation_type
    }
    /// <p>Specifies which Salesforce API is used by Amazon AppFlow when your flow transfers data to Salesforce.</p>
    /// <dl>
    /// <dt>
    /// AUTOMATIC
    /// </dt>
    /// <dd>
    /// <p>The default. Amazon AppFlow selects which API to use based on the number of records that your flow transfers to Salesforce. If your flow transfers fewer than 1,000 records, Amazon AppFlow uses Salesforce REST API. If your flow transfers 1,000 records or more, Amazon AppFlow uses Salesforce Bulk API 2.0.</p>
    /// <p>Each of these Salesforce APIs structures data differently. If Amazon AppFlow selects the API automatically, be aware that, for recurring flows, the data output might vary from one flow run to the next. For example, if a flow runs daily, it might use REST API on one day to transfer 900 records, and it might use Bulk API 2.0 on the next day to transfer 1,100 records. For each of these flow runs, the respective Salesforce API formats the data differently. Some of the differences include how dates are formatted and null values are represented. Also, Bulk API 2.0 doesn't transfer Salesforce compound fields.</p>
    /// <p>By choosing this option, you optimize flow performance for both small and large data transfers, but the tradeoff is inconsistent formatting in the output.</p>
    /// </dd>
    /// <dt>
    /// BULKV2
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce Bulk API 2.0. This API runs asynchronous data transfers, and it's optimal for large sets of data. By choosing this option, you ensure that your flow writes consistent output, but you optimize performance only for large data transfers.</p>
    /// <p>Note that Bulk API 2.0 does not transfer Salesforce compound fields.</p>
    /// </dd>
    /// <dt>
    /// REST_SYNC
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce REST API. By choosing this option, you ensure that your flow writes consistent output, but you decrease performance for large data transfers that are better suited for Bulk API 2.0. In some cases, if your flow attempts to transfer a vary large set of data, it might fail with a timed out error.</p>
    /// </dd>
    /// </dl>
    pub fn data_transfer_api(mut self, input: crate::types::SalesforceDataTransferApi) -> Self {
        self.data_transfer_api = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies which Salesforce API is used by Amazon AppFlow when your flow transfers data to Salesforce.</p>
    /// <dl>
    /// <dt>
    /// AUTOMATIC
    /// </dt>
    /// <dd>
    /// <p>The default. Amazon AppFlow selects which API to use based on the number of records that your flow transfers to Salesforce. If your flow transfers fewer than 1,000 records, Amazon AppFlow uses Salesforce REST API. If your flow transfers 1,000 records or more, Amazon AppFlow uses Salesforce Bulk API 2.0.</p>
    /// <p>Each of these Salesforce APIs structures data differently. If Amazon AppFlow selects the API automatically, be aware that, for recurring flows, the data output might vary from one flow run to the next. For example, if a flow runs daily, it might use REST API on one day to transfer 900 records, and it might use Bulk API 2.0 on the next day to transfer 1,100 records. For each of these flow runs, the respective Salesforce API formats the data differently. Some of the differences include how dates are formatted and null values are represented. Also, Bulk API 2.0 doesn't transfer Salesforce compound fields.</p>
    /// <p>By choosing this option, you optimize flow performance for both small and large data transfers, but the tradeoff is inconsistent formatting in the output.</p>
    /// </dd>
    /// <dt>
    /// BULKV2
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce Bulk API 2.0. This API runs asynchronous data transfers, and it's optimal for large sets of data. By choosing this option, you ensure that your flow writes consistent output, but you optimize performance only for large data transfers.</p>
    /// <p>Note that Bulk API 2.0 does not transfer Salesforce compound fields.</p>
    /// </dd>
    /// <dt>
    /// REST_SYNC
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce REST API. By choosing this option, you ensure that your flow writes consistent output, but you decrease performance for large data transfers that are better suited for Bulk API 2.0. In some cases, if your flow attempts to transfer a vary large set of data, it might fail with a timed out error.</p>
    /// </dd>
    /// </dl>
    pub fn set_data_transfer_api(mut self, input: ::std::option::Option<crate::types::SalesforceDataTransferApi>) -> Self {
        self.data_transfer_api = input;
        self
    }
    /// <p>Specifies which Salesforce API is used by Amazon AppFlow when your flow transfers data to Salesforce.</p>
    /// <dl>
    /// <dt>
    /// AUTOMATIC
    /// </dt>
    /// <dd>
    /// <p>The default. Amazon AppFlow selects which API to use based on the number of records that your flow transfers to Salesforce. If your flow transfers fewer than 1,000 records, Amazon AppFlow uses Salesforce REST API. If your flow transfers 1,000 records or more, Amazon AppFlow uses Salesforce Bulk API 2.0.</p>
    /// <p>Each of these Salesforce APIs structures data differently. If Amazon AppFlow selects the API automatically, be aware that, for recurring flows, the data output might vary from one flow run to the next. For example, if a flow runs daily, it might use REST API on one day to transfer 900 records, and it might use Bulk API 2.0 on the next day to transfer 1,100 records. For each of these flow runs, the respective Salesforce API formats the data differently. Some of the differences include how dates are formatted and null values are represented. Also, Bulk API 2.0 doesn't transfer Salesforce compound fields.</p>
    /// <p>By choosing this option, you optimize flow performance for both small and large data transfers, but the tradeoff is inconsistent formatting in the output.</p>
    /// </dd>
    /// <dt>
    /// BULKV2
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce Bulk API 2.0. This API runs asynchronous data transfers, and it's optimal for large sets of data. By choosing this option, you ensure that your flow writes consistent output, but you optimize performance only for large data transfers.</p>
    /// <p>Note that Bulk API 2.0 does not transfer Salesforce compound fields.</p>
    /// </dd>
    /// <dt>
    /// REST_SYNC
    /// </dt>
    /// <dd>
    /// <p>Amazon AppFlow uses only Salesforce REST API. By choosing this option, you ensure that your flow writes consistent output, but you decrease performance for large data transfers that are better suited for Bulk API 2.0. In some cases, if your flow attempts to transfer a vary large set of data, it might fail with a timed out error.</p>
    /// </dd>
    /// </dl>
    pub fn get_data_transfer_api(&self) -> &::std::option::Option<crate::types::SalesforceDataTransferApi> {
        &self.data_transfer_api
    }
    /// Consumes the builder and constructs a [`SalesforceDestinationProperties`](crate::types::SalesforceDestinationProperties).
    /// This method will fail if any of the following fields are not set:
    /// - [`object`](crate::types::builders::SalesforceDestinationPropertiesBuilder::object)
    pub fn build(self) -> ::std::result::Result<crate::types::SalesforceDestinationProperties, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::SalesforceDestinationProperties {
            object: self.object.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "object",
                    "object was not specified but it is required when building SalesforceDestinationProperties",
                )
            })?,
            id_field_names: self.id_field_names,
            error_handling_config: self.error_handling_config,
            write_operation_type: self.write_operation_type,
            data_transfer_api: self.data_transfer_api,
        })
    }
}
