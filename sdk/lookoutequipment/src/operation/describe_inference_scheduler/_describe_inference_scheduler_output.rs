// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct DescribeInferenceSchedulerOutput {
    /// <p>The Amazon Resource Name (ARN) of the ML model of the inference scheduler being described. </p>
    #[doc(hidden)]
    pub model_arn: std::option::Option<std::string::String>,
    /// <p>The name of the ML model of the inference scheduler being described. </p>
    #[doc(hidden)]
    pub model_name: std::option::Option<std::string::String>,
    /// <p>The name of the inference scheduler being described. </p>
    #[doc(hidden)]
    pub inference_scheduler_name: std::option::Option<std::string::String>,
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being described. </p>
    #[doc(hidden)]
    pub inference_scheduler_arn: std::option::Option<std::string::String>,
    /// <p>Indicates the status of the inference scheduler. </p>
    #[doc(hidden)]
    pub status: std::option::Option<crate::types::InferenceSchedulerStatus>,
    /// <p> A period of time (in minutes) by which inference on the data is delayed after the data starts. For instance, if you select an offset delay time of five minutes, inference will not begin on the data until the first data measurement after the five minute mark. For example, if five minutes is selected, the inference scheduler will wake up at the configured frequency with the additional five minute delay time to check the customer S3 bucket. The customer can upload data at the same frequency and they don't need to stop and restart the scheduler when uploading new data.</p>
    #[doc(hidden)]
    pub data_delay_offset_in_minutes: std::option::Option<i64>,
    /// <p>Specifies how often data is uploaded to the source S3 bucket for the input data. This value is the length of time between data uploads. For instance, if you select 5 minutes, Amazon Lookout for Equipment will upload the real-time data to the source bucket once every 5 minutes. This frequency also determines how often Amazon Lookout for Equipment starts a scheduled inference on your data. In this example, it starts once every 5 minutes. </p>
    #[doc(hidden)]
    pub data_upload_frequency: std::option::Option<crate::types::DataUploadFrequency>,
    /// <p>Specifies the time at which the inference scheduler was created. </p>
    #[doc(hidden)]
    pub created_at: std::option::Option<aws_smithy_types::DateTime>,
    /// <p>Specifies the time at which the inference scheduler was last updated, if it was. </p>
    #[doc(hidden)]
    pub updated_at: std::option::Option<aws_smithy_types::DateTime>,
    /// <p> Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location. </p>
    #[doc(hidden)]
    pub data_input_configuration: std::option::Option<crate::types::InferenceInputConfiguration>,
    /// <p> Specifies information for the output results for the inference scheduler, including the output S3 location. </p>
    #[doc(hidden)]
    pub data_output_configuration: std::option::Option<crate::types::InferenceOutputConfiguration>,
    /// <p> The Amazon Resource Name (ARN) of a role with permission to access the data source for the inference scheduler being described. </p>
    #[doc(hidden)]
    pub role_arn: std::option::Option<std::string::String>,
    /// <p>Provides the identifier of the KMS key used to encrypt inference scheduler data by Amazon Lookout for Equipment. </p>
    #[doc(hidden)]
    pub server_side_kms_key_id: std::option::Option<std::string::String>,
    /// <p>Indicates whether the latest execution for the inference scheduler was Anomalous (anomalous events found) or Normal (no anomalous events found).</p>
    #[doc(hidden)]
    pub latest_inference_result: std::option::Option<crate::types::LatestInferenceResult>,
    _request_id: Option<String>,
}
impl DescribeInferenceSchedulerOutput {
    /// <p>The Amazon Resource Name (ARN) of the ML model of the inference scheduler being described. </p>
    pub fn model_arn(&self) -> std::option::Option<&str> {
        self.model_arn.as_deref()
    }
    /// <p>The name of the ML model of the inference scheduler being described. </p>
    pub fn model_name(&self) -> std::option::Option<&str> {
        self.model_name.as_deref()
    }
    /// <p>The name of the inference scheduler being described. </p>
    pub fn inference_scheduler_name(&self) -> std::option::Option<&str> {
        self.inference_scheduler_name.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being described. </p>
    pub fn inference_scheduler_arn(&self) -> std::option::Option<&str> {
        self.inference_scheduler_arn.as_deref()
    }
    /// <p>Indicates the status of the inference scheduler. </p>
    pub fn status(&self) -> std::option::Option<&crate::types::InferenceSchedulerStatus> {
        self.status.as_ref()
    }
    /// <p> A period of time (in minutes) by which inference on the data is delayed after the data starts. For instance, if you select an offset delay time of five minutes, inference will not begin on the data until the first data measurement after the five minute mark. For example, if five minutes is selected, the inference scheduler will wake up at the configured frequency with the additional five minute delay time to check the customer S3 bucket. The customer can upload data at the same frequency and they don't need to stop and restart the scheduler when uploading new data.</p>
    pub fn data_delay_offset_in_minutes(&self) -> std::option::Option<i64> {
        self.data_delay_offset_in_minutes
    }
    /// <p>Specifies how often data is uploaded to the source S3 bucket for the input data. This value is the length of time between data uploads. For instance, if you select 5 minutes, Amazon Lookout for Equipment will upload the real-time data to the source bucket once every 5 minutes. This frequency also determines how often Amazon Lookout for Equipment starts a scheduled inference on your data. In this example, it starts once every 5 minutes. </p>
    pub fn data_upload_frequency(&self) -> std::option::Option<&crate::types::DataUploadFrequency> {
        self.data_upload_frequency.as_ref()
    }
    /// <p>Specifies the time at which the inference scheduler was created. </p>
    pub fn created_at(&self) -> std::option::Option<&aws_smithy_types::DateTime> {
        self.created_at.as_ref()
    }
    /// <p>Specifies the time at which the inference scheduler was last updated, if it was. </p>
    pub fn updated_at(&self) -> std::option::Option<&aws_smithy_types::DateTime> {
        self.updated_at.as_ref()
    }
    /// <p> Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location. </p>
    pub fn data_input_configuration(
        &self,
    ) -> std::option::Option<&crate::types::InferenceInputConfiguration> {
        self.data_input_configuration.as_ref()
    }
    /// <p> Specifies information for the output results for the inference scheduler, including the output S3 location. </p>
    pub fn data_output_configuration(
        &self,
    ) -> std::option::Option<&crate::types::InferenceOutputConfiguration> {
        self.data_output_configuration.as_ref()
    }
    /// <p> The Amazon Resource Name (ARN) of a role with permission to access the data source for the inference scheduler being described. </p>
    pub fn role_arn(&self) -> std::option::Option<&str> {
        self.role_arn.as_deref()
    }
    /// <p>Provides the identifier of the KMS key used to encrypt inference scheduler data by Amazon Lookout for Equipment. </p>
    pub fn server_side_kms_key_id(&self) -> std::option::Option<&str> {
        self.server_side_kms_key_id.as_deref()
    }
    /// <p>Indicates whether the latest execution for the inference scheduler was Anomalous (anomalous events found) or Normal (no anomalous events found).</p>
    pub fn latest_inference_result(
        &self,
    ) -> std::option::Option<&crate::types::LatestInferenceResult> {
        self.latest_inference_result.as_ref()
    }
}
impl aws_http::request_id::RequestId for DescribeInferenceSchedulerOutput {
    fn request_id(&self) -> Option<&str> {
        self._request_id.as_deref()
    }
}
impl DescribeInferenceSchedulerOutput {
    /// Creates a new builder-style object to manufacture [`DescribeInferenceSchedulerOutput`](crate::operation::describe_inference_scheduler::DescribeInferenceSchedulerOutput).
    pub fn builder() -> crate::operation::describe_inference_scheduler::builders::DescribeInferenceSchedulerOutputBuilder{
        crate::operation::describe_inference_scheduler::builders::DescribeInferenceSchedulerOutputBuilder::default()
    }
}

/// A builder for [`DescribeInferenceSchedulerOutput`](crate::operation::describe_inference_scheduler::DescribeInferenceSchedulerOutput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct DescribeInferenceSchedulerOutputBuilder {
    pub(crate) model_arn: std::option::Option<std::string::String>,
    pub(crate) model_name: std::option::Option<std::string::String>,
    pub(crate) inference_scheduler_name: std::option::Option<std::string::String>,
    pub(crate) inference_scheduler_arn: std::option::Option<std::string::String>,
    pub(crate) status: std::option::Option<crate::types::InferenceSchedulerStatus>,
    pub(crate) data_delay_offset_in_minutes: std::option::Option<i64>,
    pub(crate) data_upload_frequency: std::option::Option<crate::types::DataUploadFrequency>,
    pub(crate) created_at: std::option::Option<aws_smithy_types::DateTime>,
    pub(crate) updated_at: std::option::Option<aws_smithy_types::DateTime>,
    pub(crate) data_input_configuration:
        std::option::Option<crate::types::InferenceInputConfiguration>,
    pub(crate) data_output_configuration:
        std::option::Option<crate::types::InferenceOutputConfiguration>,
    pub(crate) role_arn: std::option::Option<std::string::String>,
    pub(crate) server_side_kms_key_id: std::option::Option<std::string::String>,
    pub(crate) latest_inference_result: std::option::Option<crate::types::LatestInferenceResult>,
    _request_id: Option<String>,
}
impl DescribeInferenceSchedulerOutputBuilder {
    /// <p>The Amazon Resource Name (ARN) of the ML model of the inference scheduler being described. </p>
    pub fn model_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.model_arn = Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the ML model of the inference scheduler being described. </p>
    pub fn set_model_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.model_arn = input;
        self
    }
    /// <p>The name of the ML model of the inference scheduler being described. </p>
    pub fn model_name(mut self, input: impl Into<std::string::String>) -> Self {
        self.model_name = Some(input.into());
        self
    }
    /// <p>The name of the ML model of the inference scheduler being described. </p>
    pub fn set_model_name(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.model_name = input;
        self
    }
    /// <p>The name of the inference scheduler being described. </p>
    pub fn inference_scheduler_name(mut self, input: impl Into<std::string::String>) -> Self {
        self.inference_scheduler_name = Some(input.into());
        self
    }
    /// <p>The name of the inference scheduler being described. </p>
    pub fn set_inference_scheduler_name(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.inference_scheduler_name = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being described. </p>
    pub fn inference_scheduler_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.inference_scheduler_arn = Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being described. </p>
    pub fn set_inference_scheduler_arn(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.inference_scheduler_arn = input;
        self
    }
    /// <p>Indicates the status of the inference scheduler. </p>
    pub fn status(mut self, input: crate::types::InferenceSchedulerStatus) -> Self {
        self.status = Some(input);
        self
    }
    /// <p>Indicates the status of the inference scheduler. </p>
    pub fn set_status(
        mut self,
        input: std::option::Option<crate::types::InferenceSchedulerStatus>,
    ) -> Self {
        self.status = input;
        self
    }
    /// <p> A period of time (in minutes) by which inference on the data is delayed after the data starts. For instance, if you select an offset delay time of five minutes, inference will not begin on the data until the first data measurement after the five minute mark. For example, if five minutes is selected, the inference scheduler will wake up at the configured frequency with the additional five minute delay time to check the customer S3 bucket. The customer can upload data at the same frequency and they don't need to stop and restart the scheduler when uploading new data.</p>
    pub fn data_delay_offset_in_minutes(mut self, input: i64) -> Self {
        self.data_delay_offset_in_minutes = Some(input);
        self
    }
    /// <p> A period of time (in minutes) by which inference on the data is delayed after the data starts. For instance, if you select an offset delay time of five minutes, inference will not begin on the data until the first data measurement after the five minute mark. For example, if five minutes is selected, the inference scheduler will wake up at the configured frequency with the additional five minute delay time to check the customer S3 bucket. The customer can upload data at the same frequency and they don't need to stop and restart the scheduler when uploading new data.</p>
    pub fn set_data_delay_offset_in_minutes(mut self, input: std::option::Option<i64>) -> Self {
        self.data_delay_offset_in_minutes = input;
        self
    }
    /// <p>Specifies how often data is uploaded to the source S3 bucket for the input data. This value is the length of time between data uploads. For instance, if you select 5 minutes, Amazon Lookout for Equipment will upload the real-time data to the source bucket once every 5 minutes. This frequency also determines how often Amazon Lookout for Equipment starts a scheduled inference on your data. In this example, it starts once every 5 minutes. </p>
    pub fn data_upload_frequency(mut self, input: crate::types::DataUploadFrequency) -> Self {
        self.data_upload_frequency = Some(input);
        self
    }
    /// <p>Specifies how often data is uploaded to the source S3 bucket for the input data. This value is the length of time between data uploads. For instance, if you select 5 minutes, Amazon Lookout for Equipment will upload the real-time data to the source bucket once every 5 minutes. This frequency also determines how often Amazon Lookout for Equipment starts a scheduled inference on your data. In this example, it starts once every 5 minutes. </p>
    pub fn set_data_upload_frequency(
        mut self,
        input: std::option::Option<crate::types::DataUploadFrequency>,
    ) -> Self {
        self.data_upload_frequency = input;
        self
    }
    /// <p>Specifies the time at which the inference scheduler was created. </p>
    pub fn created_at(mut self, input: aws_smithy_types::DateTime) -> Self {
        self.created_at = Some(input);
        self
    }
    /// <p>Specifies the time at which the inference scheduler was created. </p>
    pub fn set_created_at(
        mut self,
        input: std::option::Option<aws_smithy_types::DateTime>,
    ) -> Self {
        self.created_at = input;
        self
    }
    /// <p>Specifies the time at which the inference scheduler was last updated, if it was. </p>
    pub fn updated_at(mut self, input: aws_smithy_types::DateTime) -> Self {
        self.updated_at = Some(input);
        self
    }
    /// <p>Specifies the time at which the inference scheduler was last updated, if it was. </p>
    pub fn set_updated_at(
        mut self,
        input: std::option::Option<aws_smithy_types::DateTime>,
    ) -> Self {
        self.updated_at = input;
        self
    }
    /// <p> Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location. </p>
    pub fn data_input_configuration(
        mut self,
        input: crate::types::InferenceInputConfiguration,
    ) -> Self {
        self.data_input_configuration = Some(input);
        self
    }
    /// <p> Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location. </p>
    pub fn set_data_input_configuration(
        mut self,
        input: std::option::Option<crate::types::InferenceInputConfiguration>,
    ) -> Self {
        self.data_input_configuration = input;
        self
    }
    /// <p> Specifies information for the output results for the inference scheduler, including the output S3 location. </p>
    pub fn data_output_configuration(
        mut self,
        input: crate::types::InferenceOutputConfiguration,
    ) -> Self {
        self.data_output_configuration = Some(input);
        self
    }
    /// <p> Specifies information for the output results for the inference scheduler, including the output S3 location. </p>
    pub fn set_data_output_configuration(
        mut self,
        input: std::option::Option<crate::types::InferenceOutputConfiguration>,
    ) -> Self {
        self.data_output_configuration = input;
        self
    }
    /// <p> The Amazon Resource Name (ARN) of a role with permission to access the data source for the inference scheduler being described. </p>
    pub fn role_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.role_arn = Some(input.into());
        self
    }
    /// <p> The Amazon Resource Name (ARN) of a role with permission to access the data source for the inference scheduler being described. </p>
    pub fn set_role_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.role_arn = input;
        self
    }
    /// <p>Provides the identifier of the KMS key used to encrypt inference scheduler data by Amazon Lookout for Equipment. </p>
    pub fn server_side_kms_key_id(mut self, input: impl Into<std::string::String>) -> Self {
        self.server_side_kms_key_id = Some(input.into());
        self
    }
    /// <p>Provides the identifier of the KMS key used to encrypt inference scheduler data by Amazon Lookout for Equipment. </p>
    pub fn set_server_side_kms_key_id(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.server_side_kms_key_id = input;
        self
    }
    /// <p>Indicates whether the latest execution for the inference scheduler was Anomalous (anomalous events found) or Normal (no anomalous events found).</p>
    pub fn latest_inference_result(mut self, input: crate::types::LatestInferenceResult) -> Self {
        self.latest_inference_result = Some(input);
        self
    }
    /// <p>Indicates whether the latest execution for the inference scheduler was Anomalous (anomalous events found) or Normal (no anomalous events found).</p>
    pub fn set_latest_inference_result(
        mut self,
        input: std::option::Option<crate::types::LatestInferenceResult>,
    ) -> Self {
        self.latest_inference_result = input;
        self
    }
    pub(crate) fn _request_id(mut self, request_id: impl Into<String>) -> Self {
        self._request_id = Some(request_id.into());
        self
    }

    pub(crate) fn _set_request_id(&mut self, request_id: Option<String>) -> &mut Self {
        self._request_id = request_id;
        self
    }
    /// Consumes the builder and constructs a [`DescribeInferenceSchedulerOutput`](crate::operation::describe_inference_scheduler::DescribeInferenceSchedulerOutput).
    pub fn build(
        self,
    ) -> crate::operation::describe_inference_scheduler::DescribeInferenceSchedulerOutput {
        crate::operation::describe_inference_scheduler::DescribeInferenceSchedulerOutput {
            model_arn: self.model_arn,
            model_name: self.model_name,
            inference_scheduler_name: self.inference_scheduler_name,
            inference_scheduler_arn: self.inference_scheduler_arn,
            status: self.status,
            data_delay_offset_in_minutes: self.data_delay_offset_in_minutes,
            data_upload_frequency: self.data_upload_frequency,
            created_at: self.created_at,
            updated_at: self.updated_at,
            data_input_configuration: self.data_input_configuration,
            data_output_configuration: self.data_output_configuration,
            role_arn: self.role_arn,
            server_side_kms_key_id: self.server_side_kms_key_id,
            latest_inference_result: self.latest_inference_result,
            _request_id: self._request_id,
        }
    }
}
