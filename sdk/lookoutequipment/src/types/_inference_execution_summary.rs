// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Contains information about the specific inference execution, including input and output data configuration, inference scheduling information, status, and so on.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct InferenceExecutionSummary {
    /// <p>The name of the machine learning model being used for the inference execution.</p>
    pub model_name: ::std::option::Option<::std::string::String>,
    /// <p>The Amazon Resource Name (ARN) of the machine learning model used for the inference execution.</p>
    pub model_arn: ::std::option::Option<::std::string::String>,
    /// <p>The name of the inference scheduler being used for the inference execution.</p>
    pub inference_scheduler_name: ::std::option::Option<::std::string::String>,
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being used for the inference execution.</p>
    pub inference_scheduler_arn: ::std::option::Option<::std::string::String>,
    /// <p>Indicates the start time at which the inference scheduler began the specific inference execution.</p>
    pub scheduled_start_time: ::std::option::Option<::aws_smithy_types::DateTime>,
    /// <p>Indicates the time reference in the dataset at which the inference execution began.</p>
    pub data_start_time: ::std::option::Option<::aws_smithy_types::DateTime>,
    /// <p>Indicates the time reference in the dataset at which the inference execution stopped.</p>
    pub data_end_time: ::std::option::Option<::aws_smithy_types::DateTime>,
    /// <p>Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location.</p>
    pub data_input_configuration: ::std::option::Option<crate::types::InferenceInputConfiguration>,
    /// <p>Specifies configuration information for the output results from for the inference execution, including the output Amazon S3 location.</p>
    pub data_output_configuration: ::std::option::Option<crate::types::InferenceOutputConfiguration>,
    /// <p>The S3 object that the inference execution results were uploaded to.</p>
    pub customer_result_object: ::std::option::Option<crate::types::S3Object>,
    /// <p>Indicates the status of the inference execution.</p>
    pub status: ::std::option::Option<crate::types::InferenceExecutionStatus>,
    /// <p>Specifies the reason for failure when an inference execution has failed.</p>
    pub failed_reason: ::std::option::Option<::std::string::String>,
    /// <p>The model version used for the inference execution.</p>
    pub model_version: ::std::option::Option<i64>,
    /// <p>The Amazon Resource Number (ARN) of the model version used for the inference execution.</p>
    pub model_version_arn: ::std::option::Option<::std::string::String>,
}
impl InferenceExecutionSummary {
    /// <p>The name of the machine learning model being used for the inference execution.</p>
    pub fn model_name(&self) -> ::std::option::Option<&str> {
        self.model_name.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) of the machine learning model used for the inference execution.</p>
    pub fn model_arn(&self) -> ::std::option::Option<&str> {
        self.model_arn.as_deref()
    }
    /// <p>The name of the inference scheduler being used for the inference execution.</p>
    pub fn inference_scheduler_name(&self) -> ::std::option::Option<&str> {
        self.inference_scheduler_name.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being used for the inference execution.</p>
    pub fn inference_scheduler_arn(&self) -> ::std::option::Option<&str> {
        self.inference_scheduler_arn.as_deref()
    }
    /// <p>Indicates the start time at which the inference scheduler began the specific inference execution.</p>
    pub fn scheduled_start_time(&self) -> ::std::option::Option<&::aws_smithy_types::DateTime> {
        self.scheduled_start_time.as_ref()
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution began.</p>
    pub fn data_start_time(&self) -> ::std::option::Option<&::aws_smithy_types::DateTime> {
        self.data_start_time.as_ref()
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution stopped.</p>
    pub fn data_end_time(&self) -> ::std::option::Option<&::aws_smithy_types::DateTime> {
        self.data_end_time.as_ref()
    }
    /// <p>Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location.</p>
    pub fn data_input_configuration(&self) -> ::std::option::Option<&crate::types::InferenceInputConfiguration> {
        self.data_input_configuration.as_ref()
    }
    /// <p>Specifies configuration information for the output results from for the inference execution, including the output Amazon S3 location.</p>
    pub fn data_output_configuration(&self) -> ::std::option::Option<&crate::types::InferenceOutputConfiguration> {
        self.data_output_configuration.as_ref()
    }
    /// <p>The S3 object that the inference execution results were uploaded to.</p>
    pub fn customer_result_object(&self) -> ::std::option::Option<&crate::types::S3Object> {
        self.customer_result_object.as_ref()
    }
    /// <p>Indicates the status of the inference execution.</p>
    pub fn status(&self) -> ::std::option::Option<&crate::types::InferenceExecutionStatus> {
        self.status.as_ref()
    }
    /// <p>Specifies the reason for failure when an inference execution has failed.</p>
    pub fn failed_reason(&self) -> ::std::option::Option<&str> {
        self.failed_reason.as_deref()
    }
    /// <p>The model version used for the inference execution.</p>
    pub fn model_version(&self) -> ::std::option::Option<i64> {
        self.model_version
    }
    /// <p>The Amazon Resource Number (ARN) of the model version used for the inference execution.</p>
    pub fn model_version_arn(&self) -> ::std::option::Option<&str> {
        self.model_version_arn.as_deref()
    }
}
impl InferenceExecutionSummary {
    /// Creates a new builder-style object to manufacture [`InferenceExecutionSummary`](crate::types::InferenceExecutionSummary).
    pub fn builder() -> crate::types::builders::InferenceExecutionSummaryBuilder {
        crate::types::builders::InferenceExecutionSummaryBuilder::default()
    }
}

/// A builder for [`InferenceExecutionSummary`](crate::types::InferenceExecutionSummary).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct InferenceExecutionSummaryBuilder {
    pub(crate) model_name: ::std::option::Option<::std::string::String>,
    pub(crate) model_arn: ::std::option::Option<::std::string::String>,
    pub(crate) inference_scheduler_name: ::std::option::Option<::std::string::String>,
    pub(crate) inference_scheduler_arn: ::std::option::Option<::std::string::String>,
    pub(crate) scheduled_start_time: ::std::option::Option<::aws_smithy_types::DateTime>,
    pub(crate) data_start_time: ::std::option::Option<::aws_smithy_types::DateTime>,
    pub(crate) data_end_time: ::std::option::Option<::aws_smithy_types::DateTime>,
    pub(crate) data_input_configuration: ::std::option::Option<crate::types::InferenceInputConfiguration>,
    pub(crate) data_output_configuration: ::std::option::Option<crate::types::InferenceOutputConfiguration>,
    pub(crate) customer_result_object: ::std::option::Option<crate::types::S3Object>,
    pub(crate) status: ::std::option::Option<crate::types::InferenceExecutionStatus>,
    pub(crate) failed_reason: ::std::option::Option<::std::string::String>,
    pub(crate) model_version: ::std::option::Option<i64>,
    pub(crate) model_version_arn: ::std::option::Option<::std::string::String>,
}
impl InferenceExecutionSummaryBuilder {
    /// <p>The name of the machine learning model being used for the inference execution.</p>
    pub fn model_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.model_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The name of the machine learning model being used for the inference execution.</p>
    pub fn set_model_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.model_name = input;
        self
    }
    /// <p>The name of the machine learning model being used for the inference execution.</p>
    pub fn get_model_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.model_name
    }
    /// <p>The Amazon Resource Name (ARN) of the machine learning model used for the inference execution.</p>
    pub fn model_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.model_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the machine learning model used for the inference execution.</p>
    pub fn set_model_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.model_arn = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the machine learning model used for the inference execution.</p>
    pub fn get_model_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.model_arn
    }
    /// <p>The name of the inference scheduler being used for the inference execution.</p>
    pub fn inference_scheduler_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inference_scheduler_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The name of the inference scheduler being used for the inference execution.</p>
    pub fn set_inference_scheduler_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inference_scheduler_name = input;
        self
    }
    /// <p>The name of the inference scheduler being used for the inference execution.</p>
    pub fn get_inference_scheduler_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.inference_scheduler_name
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being used for the inference execution.</p>
    pub fn inference_scheduler_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inference_scheduler_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being used for the inference execution.</p>
    pub fn set_inference_scheduler_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inference_scheduler_arn = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the inference scheduler being used for the inference execution.</p>
    pub fn get_inference_scheduler_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.inference_scheduler_arn
    }
    /// <p>Indicates the start time at which the inference scheduler began the specific inference execution.</p>
    pub fn scheduled_start_time(mut self, input: ::aws_smithy_types::DateTime) -> Self {
        self.scheduled_start_time = ::std::option::Option::Some(input);
        self
    }
    /// <p>Indicates the start time at which the inference scheduler began the specific inference execution.</p>
    pub fn set_scheduled_start_time(mut self, input: ::std::option::Option<::aws_smithy_types::DateTime>) -> Self {
        self.scheduled_start_time = input;
        self
    }
    /// <p>Indicates the start time at which the inference scheduler began the specific inference execution.</p>
    pub fn get_scheduled_start_time(&self) -> &::std::option::Option<::aws_smithy_types::DateTime> {
        &self.scheduled_start_time
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution began.</p>
    pub fn data_start_time(mut self, input: ::aws_smithy_types::DateTime) -> Self {
        self.data_start_time = ::std::option::Option::Some(input);
        self
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution began.</p>
    pub fn set_data_start_time(mut self, input: ::std::option::Option<::aws_smithy_types::DateTime>) -> Self {
        self.data_start_time = input;
        self
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution began.</p>
    pub fn get_data_start_time(&self) -> &::std::option::Option<::aws_smithy_types::DateTime> {
        &self.data_start_time
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution stopped.</p>
    pub fn data_end_time(mut self, input: ::aws_smithy_types::DateTime) -> Self {
        self.data_end_time = ::std::option::Option::Some(input);
        self
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution stopped.</p>
    pub fn set_data_end_time(mut self, input: ::std::option::Option<::aws_smithy_types::DateTime>) -> Self {
        self.data_end_time = input;
        self
    }
    /// <p>Indicates the time reference in the dataset at which the inference execution stopped.</p>
    pub fn get_data_end_time(&self) -> &::std::option::Option<::aws_smithy_types::DateTime> {
        &self.data_end_time
    }
    /// <p>Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location.</p>
    pub fn data_input_configuration(mut self, input: crate::types::InferenceInputConfiguration) -> Self {
        self.data_input_configuration = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location.</p>
    pub fn set_data_input_configuration(mut self, input: ::std::option::Option<crate::types::InferenceInputConfiguration>) -> Self {
        self.data_input_configuration = input;
        self
    }
    /// <p>Specifies configuration information for the input data for the inference scheduler, including delimiter, format, and dataset location.</p>
    pub fn get_data_input_configuration(&self) -> &::std::option::Option<crate::types::InferenceInputConfiguration> {
        &self.data_input_configuration
    }
    /// <p>Specifies configuration information for the output results from for the inference execution, including the output Amazon S3 location.</p>
    pub fn data_output_configuration(mut self, input: crate::types::InferenceOutputConfiguration) -> Self {
        self.data_output_configuration = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies configuration information for the output results from for the inference execution, including the output Amazon S3 location.</p>
    pub fn set_data_output_configuration(mut self, input: ::std::option::Option<crate::types::InferenceOutputConfiguration>) -> Self {
        self.data_output_configuration = input;
        self
    }
    /// <p>Specifies configuration information for the output results from for the inference execution, including the output Amazon S3 location.</p>
    pub fn get_data_output_configuration(&self) -> &::std::option::Option<crate::types::InferenceOutputConfiguration> {
        &self.data_output_configuration
    }
    /// <p>The S3 object that the inference execution results were uploaded to.</p>
    pub fn customer_result_object(mut self, input: crate::types::S3Object) -> Self {
        self.customer_result_object = ::std::option::Option::Some(input);
        self
    }
    /// <p>The S3 object that the inference execution results were uploaded to.</p>
    pub fn set_customer_result_object(mut self, input: ::std::option::Option<crate::types::S3Object>) -> Self {
        self.customer_result_object = input;
        self
    }
    /// <p>The S3 object that the inference execution results were uploaded to.</p>
    pub fn get_customer_result_object(&self) -> &::std::option::Option<crate::types::S3Object> {
        &self.customer_result_object
    }
    /// <p>Indicates the status of the inference execution.</p>
    pub fn status(mut self, input: crate::types::InferenceExecutionStatus) -> Self {
        self.status = ::std::option::Option::Some(input);
        self
    }
    /// <p>Indicates the status of the inference execution.</p>
    pub fn set_status(mut self, input: ::std::option::Option<crate::types::InferenceExecutionStatus>) -> Self {
        self.status = input;
        self
    }
    /// <p>Indicates the status of the inference execution.</p>
    pub fn get_status(&self) -> &::std::option::Option<crate::types::InferenceExecutionStatus> {
        &self.status
    }
    /// <p>Specifies the reason for failure when an inference execution has failed.</p>
    pub fn failed_reason(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.failed_reason = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>Specifies the reason for failure when an inference execution has failed.</p>
    pub fn set_failed_reason(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.failed_reason = input;
        self
    }
    /// <p>Specifies the reason for failure when an inference execution has failed.</p>
    pub fn get_failed_reason(&self) -> &::std::option::Option<::std::string::String> {
        &self.failed_reason
    }
    /// <p>The model version used for the inference execution.</p>
    pub fn model_version(mut self, input: i64) -> Self {
        self.model_version = ::std::option::Option::Some(input);
        self
    }
    /// <p>The model version used for the inference execution.</p>
    pub fn set_model_version(mut self, input: ::std::option::Option<i64>) -> Self {
        self.model_version = input;
        self
    }
    /// <p>The model version used for the inference execution.</p>
    pub fn get_model_version(&self) -> &::std::option::Option<i64> {
        &self.model_version
    }
    /// <p>The Amazon Resource Number (ARN) of the model version used for the inference execution.</p>
    pub fn model_version_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.model_version_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Resource Number (ARN) of the model version used for the inference execution.</p>
    pub fn set_model_version_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.model_version_arn = input;
        self
    }
    /// <p>The Amazon Resource Number (ARN) of the model version used for the inference execution.</p>
    pub fn get_model_version_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.model_version_arn
    }
    /// Consumes the builder and constructs a [`InferenceExecutionSummary`](crate::types::InferenceExecutionSummary).
    pub fn build(self) -> crate::types::InferenceExecutionSummary {
        crate::types::InferenceExecutionSummary {
            model_name: self.model_name,
            model_arn: self.model_arn,
            inference_scheduler_name: self.inference_scheduler_name,
            inference_scheduler_arn: self.inference_scheduler_arn,
            scheduled_start_time: self.scheduled_start_time,
            data_start_time: self.data_start_time,
            data_end_time: self.data_end_time,
            data_input_configuration: self.data_input_configuration,
            data_output_configuration: self.data_output_configuration,
            customer_result_object: self.customer_result_object,
            status: self.status,
            failed_reason: self.failed_reason,
            model_version: self.model_version,
            model_version_arn: self.model_version_arn,
        }
    }
}
