// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Optional deployment parameters that control how many tasks run during a deployment and the ordering of stopping and starting tasks.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct DeploymentConfiguration {
    /// <note>
    /// <p>The deployment circuit breaker can only be used for services using the rolling update (<code>ECS</code>) deployment type.</p>
    /// </note>
    /// <p>The <b>deployment circuit breaker</b> determines whether a service deployment will fail if the service can't reach a steady state. If you use the deployment circuit breaker, a service deployment will transition to a failed state and stop launching new tasks. If you use the rollback option, when a service deployment fails, the service is rolled back to the last deployment that completed successfully. For more information, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html">Rolling update</a> in the <i>Amazon Elastic Container Service Developer Guide</i></p>
    pub deployment_circuit_breaker: ::std::option::Option<crate::types::DeploymentCircuitBreaker>,
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>maximumPercent</code> parameter represents an upper limit on the number of your service's tasks that are allowed in the <code>RUNNING</code> or <code>PENDING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service is using the <code>REPLICA</code> service scheduler and has a <code>desiredCount</code> of four tasks and a <code>maximumPercent</code> value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default <code>maximumPercent</code> value for a service using the <code>REPLICA</code> service scheduler is 200%.</p>
    /// <p>The Amazon ECS scheduler uses this parameter to replace unhealthy tasks by starting replacement tasks first and then stopping the unhealthy tasks, as long as cluster resources for starting replacement tasks are available. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and tasks in the service use the EC2 launch type, the <b>maximum percent</b> value is set to the default value. The <b>maximum percent</b> value is used to define the upper limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>maximumPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If the service uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and the tasks in the service use the Fargate launch type, the maximum percent value is not used. The value is still returned when describing your service.</p>
    pub maximum_percent: ::std::option::Option<i32>,
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>minimumHealthyPercent</code> represents a lower limit on the number of your service's tasks that must remain in the <code>RUNNING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a <code>desiredCount</code> of four tasks and a <code>minimumHealthyPercent</code> of 50%, the service scheduler may stop two existing tasks to free up cluster capacity before starting two new tasks.</p>
    /// <p>If any tasks are unhealthy and if <code>maximumPercent</code> doesn't allow the Amazon ECS scheduler to start replacement tasks, the scheduler stops the unhealthy tasks one-by-one — using the <code>minimumHealthyPercent</code> as a constraint — to clear up capacity to launch replacement tasks. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>For services that <i>do not</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>A service is considered healthy if all essential containers within the tasks in the service pass their health checks.</p></li>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for 40 seconds after a task reaches a <code>RUNNING</code> state before the task is counted towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has one or more essential containers with a health check defined, the service scheduler will wait for the task to reach a healthy status before counting it towards the minimum healthy percent total. A task is considered healthy when all essential containers within the task have passed their health checks. The amount of time the service scheduler can wait for is determined by the container health check settings.</p></li>
    /// </ul>
    /// <p>For services that <i>do</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has an essential container with a health check defined, the service scheduler will wait for both the task to reach a healthy status and the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// </ul>
    /// <p>The default value for a replica service for <code>minimumHealthyPercent</code> is 100%. The default <code>minimumHealthyPercent</code> value for a service using the <code>DAEMON</code> service schedule is 0% for the CLI, the Amazon Web Services SDKs, and the APIs and 50% for the Amazon Web Services Management Console.</p>
    /// <p>The minimum number of healthy tasks during a deployment is the <code>desiredCount</code> multiplied by the <code>minimumHealthyPercent</code>/100, rounded up to the nearest integer value.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the EC2 launch type, the <b>minimum healthy percent</b> value is set to the default value. The <b>minimum healthy percent</b> value is used to define the lower limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>minimumHealthyPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the Fargate launch type, the minimum healthy percent value is not used, although it is returned when describing your service.</p>
    pub minimum_healthy_percent: ::std::option::Option<i32>,
    /// <p>Information about the CloudWatch alarms.</p>
    pub alarms: ::std::option::Option<crate::types::DeploymentAlarms>,
    /// <p>The deployment strategy for the service. Choose from these valid values:</p>
    /// <ul>
    /// <li>
    /// <p><code>ROLLING</code> - When you create a service which uses the rolling update (<code>ROLLING</code>) deployment strategy, the Amazon ECS service scheduler replaces the currently running tasks with new tasks. The number of tasks that Amazon ECS adds or removes from the service during a rolling update is controlled by the service deployment configuration.</p></li>
    /// <li>
    /// <p><code>BLUE_GREEN</code> - A blue/green deployment strategy (<code>BLUE_GREEN</code>) is a release methodology that reduces downtime and risk by running two identical production environments called blue and green. With Amazon ECS blue/green deployments, you can validate new service revisions before directing production traffic to them. This approach provides a safer way to deploy changes with the ability to quickly roll back if needed.</p></li>
    /// </ul>
    pub strategy: ::std::option::Option<crate::types::DeploymentStrategy>,
    /// <p>The time period when both blue and green service revisions are running simultaneously after the production traffic has shifted.</p>
    /// <p>You must provide this parameter when you use the <code>BLUE_GREEN</code> deployment strategy.</p>
    pub bake_time_in_minutes: ::std::option::Option<i32>,
    /// <p>An array of deployment lifecycle hook objects to run custom logic at specific stages of the deployment lifecycle.</p>
    pub lifecycle_hooks: ::std::option::Option<::std::vec::Vec<crate::types::DeploymentLifecycleHook>>,
    /// <p>Configuration for linear deployment strategy. Only valid when the deployment strategy is <code>LINEAR</code>. This configuration enables progressive traffic shifting in equal percentage increments with configurable bake times between each step.</p>
    pub linear_configuration: ::std::option::Option<crate::types::LinearConfiguration>,
    /// <p>Configuration for canary deployment strategy. Only valid when the deployment strategy is <code>CANARY</code>. This configuration enables shifting a fixed percentage of traffic for testing, followed by shifting the remaining traffic after a bake period.</p>
    pub canary_configuration: ::std::option::Option<crate::types::CanaryConfiguration>,
}
impl DeploymentConfiguration {
    /// <note>
    /// <p>The deployment circuit breaker can only be used for services using the rolling update (<code>ECS</code>) deployment type.</p>
    /// </note>
    /// <p>The <b>deployment circuit breaker</b> determines whether a service deployment will fail if the service can't reach a steady state. If you use the deployment circuit breaker, a service deployment will transition to a failed state and stop launching new tasks. If you use the rollback option, when a service deployment fails, the service is rolled back to the last deployment that completed successfully. For more information, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html">Rolling update</a> in the <i>Amazon Elastic Container Service Developer Guide</i></p>
    pub fn deployment_circuit_breaker(&self) -> ::std::option::Option<&crate::types::DeploymentCircuitBreaker> {
        self.deployment_circuit_breaker.as_ref()
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>maximumPercent</code> parameter represents an upper limit on the number of your service's tasks that are allowed in the <code>RUNNING</code> or <code>PENDING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service is using the <code>REPLICA</code> service scheduler and has a <code>desiredCount</code> of four tasks and a <code>maximumPercent</code> value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default <code>maximumPercent</code> value for a service using the <code>REPLICA</code> service scheduler is 200%.</p>
    /// <p>The Amazon ECS scheduler uses this parameter to replace unhealthy tasks by starting replacement tasks first and then stopping the unhealthy tasks, as long as cluster resources for starting replacement tasks are available. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and tasks in the service use the EC2 launch type, the <b>maximum percent</b> value is set to the default value. The <b>maximum percent</b> value is used to define the upper limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>maximumPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If the service uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and the tasks in the service use the Fargate launch type, the maximum percent value is not used. The value is still returned when describing your service.</p>
    pub fn maximum_percent(&self) -> ::std::option::Option<i32> {
        self.maximum_percent
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>minimumHealthyPercent</code> represents a lower limit on the number of your service's tasks that must remain in the <code>RUNNING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a <code>desiredCount</code> of four tasks and a <code>minimumHealthyPercent</code> of 50%, the service scheduler may stop two existing tasks to free up cluster capacity before starting two new tasks.</p>
    /// <p>If any tasks are unhealthy and if <code>maximumPercent</code> doesn't allow the Amazon ECS scheduler to start replacement tasks, the scheduler stops the unhealthy tasks one-by-one — using the <code>minimumHealthyPercent</code> as a constraint — to clear up capacity to launch replacement tasks. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>For services that <i>do not</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>A service is considered healthy if all essential containers within the tasks in the service pass their health checks.</p></li>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for 40 seconds after a task reaches a <code>RUNNING</code> state before the task is counted towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has one or more essential containers with a health check defined, the service scheduler will wait for the task to reach a healthy status before counting it towards the minimum healthy percent total. A task is considered healthy when all essential containers within the task have passed their health checks. The amount of time the service scheduler can wait for is determined by the container health check settings.</p></li>
    /// </ul>
    /// <p>For services that <i>do</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has an essential container with a health check defined, the service scheduler will wait for both the task to reach a healthy status and the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// </ul>
    /// <p>The default value for a replica service for <code>minimumHealthyPercent</code> is 100%. The default <code>minimumHealthyPercent</code> value for a service using the <code>DAEMON</code> service schedule is 0% for the CLI, the Amazon Web Services SDKs, and the APIs and 50% for the Amazon Web Services Management Console.</p>
    /// <p>The minimum number of healthy tasks during a deployment is the <code>desiredCount</code> multiplied by the <code>minimumHealthyPercent</code>/100, rounded up to the nearest integer value.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the EC2 launch type, the <b>minimum healthy percent</b> value is set to the default value. The <b>minimum healthy percent</b> value is used to define the lower limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>minimumHealthyPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the Fargate launch type, the minimum healthy percent value is not used, although it is returned when describing your service.</p>
    pub fn minimum_healthy_percent(&self) -> ::std::option::Option<i32> {
        self.minimum_healthy_percent
    }
    /// <p>Information about the CloudWatch alarms.</p>
    pub fn alarms(&self) -> ::std::option::Option<&crate::types::DeploymentAlarms> {
        self.alarms.as_ref()
    }
    /// <p>The deployment strategy for the service. Choose from these valid values:</p>
    /// <ul>
    /// <li>
    /// <p><code>ROLLING</code> - When you create a service which uses the rolling update (<code>ROLLING</code>) deployment strategy, the Amazon ECS service scheduler replaces the currently running tasks with new tasks. The number of tasks that Amazon ECS adds or removes from the service during a rolling update is controlled by the service deployment configuration.</p></li>
    /// <li>
    /// <p><code>BLUE_GREEN</code> - A blue/green deployment strategy (<code>BLUE_GREEN</code>) is a release methodology that reduces downtime and risk by running two identical production environments called blue and green. With Amazon ECS blue/green deployments, you can validate new service revisions before directing production traffic to them. This approach provides a safer way to deploy changes with the ability to quickly roll back if needed.</p></li>
    /// </ul>
    pub fn strategy(&self) -> ::std::option::Option<&crate::types::DeploymentStrategy> {
        self.strategy.as_ref()
    }
    /// <p>The time period when both blue and green service revisions are running simultaneously after the production traffic has shifted.</p>
    /// <p>You must provide this parameter when you use the <code>BLUE_GREEN</code> deployment strategy.</p>
    pub fn bake_time_in_minutes(&self) -> ::std::option::Option<i32> {
        self.bake_time_in_minutes
    }
    /// <p>An array of deployment lifecycle hook objects to run custom logic at specific stages of the deployment lifecycle.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.lifecycle_hooks.is_none()`.
    pub fn lifecycle_hooks(&self) -> &[crate::types::DeploymentLifecycleHook] {
        self.lifecycle_hooks.as_deref().unwrap_or_default()
    }
    /// <p>Configuration for linear deployment strategy. Only valid when the deployment strategy is <code>LINEAR</code>. This configuration enables progressive traffic shifting in equal percentage increments with configurable bake times between each step.</p>
    pub fn linear_configuration(&self) -> ::std::option::Option<&crate::types::LinearConfiguration> {
        self.linear_configuration.as_ref()
    }
    /// <p>Configuration for canary deployment strategy. Only valid when the deployment strategy is <code>CANARY</code>. This configuration enables shifting a fixed percentage of traffic for testing, followed by shifting the remaining traffic after a bake period.</p>
    pub fn canary_configuration(&self) -> ::std::option::Option<&crate::types::CanaryConfiguration> {
        self.canary_configuration.as_ref()
    }
}
impl DeploymentConfiguration {
    /// Creates a new builder-style object to manufacture [`DeploymentConfiguration`](crate::types::DeploymentConfiguration).
    pub fn builder() -> crate::types::builders::DeploymentConfigurationBuilder {
        crate::types::builders::DeploymentConfigurationBuilder::default()
    }
}

/// A builder for [`DeploymentConfiguration`](crate::types::DeploymentConfiguration).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct DeploymentConfigurationBuilder {
    pub(crate) deployment_circuit_breaker: ::std::option::Option<crate::types::DeploymentCircuitBreaker>,
    pub(crate) maximum_percent: ::std::option::Option<i32>,
    pub(crate) minimum_healthy_percent: ::std::option::Option<i32>,
    pub(crate) alarms: ::std::option::Option<crate::types::DeploymentAlarms>,
    pub(crate) strategy: ::std::option::Option<crate::types::DeploymentStrategy>,
    pub(crate) bake_time_in_minutes: ::std::option::Option<i32>,
    pub(crate) lifecycle_hooks: ::std::option::Option<::std::vec::Vec<crate::types::DeploymentLifecycleHook>>,
    pub(crate) linear_configuration: ::std::option::Option<crate::types::LinearConfiguration>,
    pub(crate) canary_configuration: ::std::option::Option<crate::types::CanaryConfiguration>,
}
impl DeploymentConfigurationBuilder {
    /// <note>
    /// <p>The deployment circuit breaker can only be used for services using the rolling update (<code>ECS</code>) deployment type.</p>
    /// </note>
    /// <p>The <b>deployment circuit breaker</b> determines whether a service deployment will fail if the service can't reach a steady state. If you use the deployment circuit breaker, a service deployment will transition to a failed state and stop launching new tasks. If you use the rollback option, when a service deployment fails, the service is rolled back to the last deployment that completed successfully. For more information, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html">Rolling update</a> in the <i>Amazon Elastic Container Service Developer Guide</i></p>
    pub fn deployment_circuit_breaker(mut self, input: crate::types::DeploymentCircuitBreaker) -> Self {
        self.deployment_circuit_breaker = ::std::option::Option::Some(input);
        self
    }
    /// <note>
    /// <p>The deployment circuit breaker can only be used for services using the rolling update (<code>ECS</code>) deployment type.</p>
    /// </note>
    /// <p>The <b>deployment circuit breaker</b> determines whether a service deployment will fail if the service can't reach a steady state. If you use the deployment circuit breaker, a service deployment will transition to a failed state and stop launching new tasks. If you use the rollback option, when a service deployment fails, the service is rolled back to the last deployment that completed successfully. For more information, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html">Rolling update</a> in the <i>Amazon Elastic Container Service Developer Guide</i></p>
    pub fn set_deployment_circuit_breaker(mut self, input: ::std::option::Option<crate::types::DeploymentCircuitBreaker>) -> Self {
        self.deployment_circuit_breaker = input;
        self
    }
    /// <note>
    /// <p>The deployment circuit breaker can only be used for services using the rolling update (<code>ECS</code>) deployment type.</p>
    /// </note>
    /// <p>The <b>deployment circuit breaker</b> determines whether a service deployment will fail if the service can't reach a steady state. If you use the deployment circuit breaker, a service deployment will transition to a failed state and stop launching new tasks. If you use the rollback option, when a service deployment fails, the service is rolled back to the last deployment that completed successfully. For more information, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html">Rolling update</a> in the <i>Amazon Elastic Container Service Developer Guide</i></p>
    pub fn get_deployment_circuit_breaker(&self) -> &::std::option::Option<crate::types::DeploymentCircuitBreaker> {
        &self.deployment_circuit_breaker
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>maximumPercent</code> parameter represents an upper limit on the number of your service's tasks that are allowed in the <code>RUNNING</code> or <code>PENDING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service is using the <code>REPLICA</code> service scheduler and has a <code>desiredCount</code> of four tasks and a <code>maximumPercent</code> value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default <code>maximumPercent</code> value for a service using the <code>REPLICA</code> service scheduler is 200%.</p>
    /// <p>The Amazon ECS scheduler uses this parameter to replace unhealthy tasks by starting replacement tasks first and then stopping the unhealthy tasks, as long as cluster resources for starting replacement tasks are available. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and tasks in the service use the EC2 launch type, the <b>maximum percent</b> value is set to the default value. The <b>maximum percent</b> value is used to define the upper limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>maximumPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If the service uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and the tasks in the service use the Fargate launch type, the maximum percent value is not used. The value is still returned when describing your service.</p>
    pub fn maximum_percent(mut self, input: i32) -> Self {
        self.maximum_percent = ::std::option::Option::Some(input);
        self
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>maximumPercent</code> parameter represents an upper limit on the number of your service's tasks that are allowed in the <code>RUNNING</code> or <code>PENDING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service is using the <code>REPLICA</code> service scheduler and has a <code>desiredCount</code> of four tasks and a <code>maximumPercent</code> value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default <code>maximumPercent</code> value for a service using the <code>REPLICA</code> service scheduler is 200%.</p>
    /// <p>The Amazon ECS scheduler uses this parameter to replace unhealthy tasks by starting replacement tasks first and then stopping the unhealthy tasks, as long as cluster resources for starting replacement tasks are available. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and tasks in the service use the EC2 launch type, the <b>maximum percent</b> value is set to the default value. The <b>maximum percent</b> value is used to define the upper limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>maximumPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If the service uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and the tasks in the service use the Fargate launch type, the maximum percent value is not used. The value is still returned when describing your service.</p>
    pub fn set_maximum_percent(mut self, input: ::std::option::Option<i32>) -> Self {
        self.maximum_percent = input;
        self
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>maximumPercent</code> parameter represents an upper limit on the number of your service's tasks that are allowed in the <code>RUNNING</code> or <code>PENDING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service is using the <code>REPLICA</code> service scheduler and has a <code>desiredCount</code> of four tasks and a <code>maximumPercent</code> value of 200%, the scheduler may start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default <code>maximumPercent</code> value for a service using the <code>REPLICA</code> service scheduler is 200%.</p>
    /// <p>The Amazon ECS scheduler uses this parameter to replace unhealthy tasks by starting replacement tasks first and then stopping the unhealthy tasks, as long as cluster resources for starting replacement tasks are available. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and tasks in the service use the EC2 launch type, the <b>maximum percent</b> value is set to the default value. The <b>maximum percent</b> value is used to define the upper limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>maximumPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If the service uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types, and the tasks in the service use the Fargate launch type, the maximum percent value is not used. The value is still returned when describing your service.</p>
    pub fn get_maximum_percent(&self) -> &::std::option::Option<i32> {
        &self.maximum_percent
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>minimumHealthyPercent</code> represents a lower limit on the number of your service's tasks that must remain in the <code>RUNNING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a <code>desiredCount</code> of four tasks and a <code>minimumHealthyPercent</code> of 50%, the service scheduler may stop two existing tasks to free up cluster capacity before starting two new tasks.</p>
    /// <p>If any tasks are unhealthy and if <code>maximumPercent</code> doesn't allow the Amazon ECS scheduler to start replacement tasks, the scheduler stops the unhealthy tasks one-by-one — using the <code>minimumHealthyPercent</code> as a constraint — to clear up capacity to launch replacement tasks. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>For services that <i>do not</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>A service is considered healthy if all essential containers within the tasks in the service pass their health checks.</p></li>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for 40 seconds after a task reaches a <code>RUNNING</code> state before the task is counted towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has one or more essential containers with a health check defined, the service scheduler will wait for the task to reach a healthy status before counting it towards the minimum healthy percent total. A task is considered healthy when all essential containers within the task have passed their health checks. The amount of time the service scheduler can wait for is determined by the container health check settings.</p></li>
    /// </ul>
    /// <p>For services that <i>do</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has an essential container with a health check defined, the service scheduler will wait for both the task to reach a healthy status and the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// </ul>
    /// <p>The default value for a replica service for <code>minimumHealthyPercent</code> is 100%. The default <code>minimumHealthyPercent</code> value for a service using the <code>DAEMON</code> service schedule is 0% for the CLI, the Amazon Web Services SDKs, and the APIs and 50% for the Amazon Web Services Management Console.</p>
    /// <p>The minimum number of healthy tasks during a deployment is the <code>desiredCount</code> multiplied by the <code>minimumHealthyPercent</code>/100, rounded up to the nearest integer value.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the EC2 launch type, the <b>minimum healthy percent</b> value is set to the default value. The <b>minimum healthy percent</b> value is used to define the lower limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>minimumHealthyPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the Fargate launch type, the minimum healthy percent value is not used, although it is returned when describing your service.</p>
    pub fn minimum_healthy_percent(mut self, input: i32) -> Self {
        self.minimum_healthy_percent = ::std::option::Option::Some(input);
        self
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>minimumHealthyPercent</code> represents a lower limit on the number of your service's tasks that must remain in the <code>RUNNING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a <code>desiredCount</code> of four tasks and a <code>minimumHealthyPercent</code> of 50%, the service scheduler may stop two existing tasks to free up cluster capacity before starting two new tasks.</p>
    /// <p>If any tasks are unhealthy and if <code>maximumPercent</code> doesn't allow the Amazon ECS scheduler to start replacement tasks, the scheduler stops the unhealthy tasks one-by-one — using the <code>minimumHealthyPercent</code> as a constraint — to clear up capacity to launch replacement tasks. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>For services that <i>do not</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>A service is considered healthy if all essential containers within the tasks in the service pass their health checks.</p></li>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for 40 seconds after a task reaches a <code>RUNNING</code> state before the task is counted towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has one or more essential containers with a health check defined, the service scheduler will wait for the task to reach a healthy status before counting it towards the minimum healthy percent total. A task is considered healthy when all essential containers within the task have passed their health checks. The amount of time the service scheduler can wait for is determined by the container health check settings.</p></li>
    /// </ul>
    /// <p>For services that <i>do</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has an essential container with a health check defined, the service scheduler will wait for both the task to reach a healthy status and the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// </ul>
    /// <p>The default value for a replica service for <code>minimumHealthyPercent</code> is 100%. The default <code>minimumHealthyPercent</code> value for a service using the <code>DAEMON</code> service schedule is 0% for the CLI, the Amazon Web Services SDKs, and the APIs and 50% for the Amazon Web Services Management Console.</p>
    /// <p>The minimum number of healthy tasks during a deployment is the <code>desiredCount</code> multiplied by the <code>minimumHealthyPercent</code>/100, rounded up to the nearest integer value.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the EC2 launch type, the <b>minimum healthy percent</b> value is set to the default value. The <b>minimum healthy percent</b> value is used to define the lower limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>minimumHealthyPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the Fargate launch type, the minimum healthy percent value is not used, although it is returned when describing your service.</p>
    pub fn set_minimum_healthy_percent(mut self, input: ::std::option::Option<i32>) -> Self {
        self.minimum_healthy_percent = input;
        self
    }
    /// <p>If a service is using the rolling update (<code>ECS</code>) deployment type, the <code>minimumHealthyPercent</code> represents a lower limit on the number of your service's tasks that must remain in the <code>RUNNING</code> state during a deployment, as a percentage of the <code>desiredCount</code> (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a <code>desiredCount</code> of four tasks and a <code>minimumHealthyPercent</code> of 50%, the service scheduler may stop two existing tasks to free up cluster capacity before starting two new tasks.</p>
    /// <p>If any tasks are unhealthy and if <code>maximumPercent</code> doesn't allow the Amazon ECS scheduler to start replacement tasks, the scheduler stops the unhealthy tasks one-by-one — using the <code>minimumHealthyPercent</code> as a constraint — to clear up capacity to launch replacement tasks. For more information about how the scheduler replaces unhealthy tasks, see <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">Amazon ECS services</a>.</p>
    /// <p>For services that <i>do not</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>A service is considered healthy if all essential containers within the tasks in the service pass their health checks.</p></li>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for 40 seconds after a task reaches a <code>RUNNING</code> state before the task is counted towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has one or more essential containers with a health check defined, the service scheduler will wait for the task to reach a healthy status before counting it towards the minimum healthy percent total. A task is considered healthy when all essential containers within the task have passed their health checks. The amount of time the service scheduler can wait for is determined by the container health check settings.</p></li>
    /// </ul>
    /// <p>For services that <i>do</i> use a load balancer, the following should be noted:</p>
    /// <ul>
    /// <li>
    /// <p>If a task has no essential containers with a health check defined, the service scheduler will wait for the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// <li>
    /// <p>If a task has an essential container with a health check defined, the service scheduler will wait for both the task to reach a healthy status and the load balancer target group health check to return a healthy status before counting the task towards the minimum healthy percent total.</p></li>
    /// </ul>
    /// <p>The default value for a replica service for <code>minimumHealthyPercent</code> is 100%. The default <code>minimumHealthyPercent</code> value for a service using the <code>DAEMON</code> service schedule is 0% for the CLI, the Amazon Web Services SDKs, and the APIs and 50% for the Amazon Web Services Management Console.</p>
    /// <p>The minimum number of healthy tasks during a deployment is the <code>desiredCount</code> multiplied by the <code>minimumHealthyPercent</code>/100, rounded up to the nearest integer value.</p>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the EC2 launch type, the <b>minimum healthy percent</b> value is set to the default value. The <b>minimum healthy percent</b> value is used to define the lower limit on the number of the tasks in the service that remain in the <code>RUNNING</code> state while the container instances are in the <code>DRAINING</code> state.</p><note>
    /// <p>You can't specify a custom <code>minimumHealthyPercent</code> value for a service that uses either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and has tasks that use the EC2 launch type.</p>
    /// </note>
    /// <p>If a service is using either the blue/green (<code>CODE_DEPLOY</code>) or <code>EXTERNAL</code> deployment types and is running tasks that use the Fargate launch type, the minimum healthy percent value is not used, although it is returned when describing your service.</p>
    pub fn get_minimum_healthy_percent(&self) -> &::std::option::Option<i32> {
        &self.minimum_healthy_percent
    }
    /// <p>Information about the CloudWatch alarms.</p>
    pub fn alarms(mut self, input: crate::types::DeploymentAlarms) -> Self {
        self.alarms = ::std::option::Option::Some(input);
        self
    }
    /// <p>Information about the CloudWatch alarms.</p>
    pub fn set_alarms(mut self, input: ::std::option::Option<crate::types::DeploymentAlarms>) -> Self {
        self.alarms = input;
        self
    }
    /// <p>Information about the CloudWatch alarms.</p>
    pub fn get_alarms(&self) -> &::std::option::Option<crate::types::DeploymentAlarms> {
        &self.alarms
    }
    /// <p>The deployment strategy for the service. Choose from these valid values:</p>
    /// <ul>
    /// <li>
    /// <p><code>ROLLING</code> - When you create a service which uses the rolling update (<code>ROLLING</code>) deployment strategy, the Amazon ECS service scheduler replaces the currently running tasks with new tasks. The number of tasks that Amazon ECS adds or removes from the service during a rolling update is controlled by the service deployment configuration.</p></li>
    /// <li>
    /// <p><code>BLUE_GREEN</code> - A blue/green deployment strategy (<code>BLUE_GREEN</code>) is a release methodology that reduces downtime and risk by running two identical production environments called blue and green. With Amazon ECS blue/green deployments, you can validate new service revisions before directing production traffic to them. This approach provides a safer way to deploy changes with the ability to quickly roll back if needed.</p></li>
    /// </ul>
    pub fn strategy(mut self, input: crate::types::DeploymentStrategy) -> Self {
        self.strategy = ::std::option::Option::Some(input);
        self
    }
    /// <p>The deployment strategy for the service. Choose from these valid values:</p>
    /// <ul>
    /// <li>
    /// <p><code>ROLLING</code> - When you create a service which uses the rolling update (<code>ROLLING</code>) deployment strategy, the Amazon ECS service scheduler replaces the currently running tasks with new tasks. The number of tasks that Amazon ECS adds or removes from the service during a rolling update is controlled by the service deployment configuration.</p></li>
    /// <li>
    /// <p><code>BLUE_GREEN</code> - A blue/green deployment strategy (<code>BLUE_GREEN</code>) is a release methodology that reduces downtime and risk by running two identical production environments called blue and green. With Amazon ECS blue/green deployments, you can validate new service revisions before directing production traffic to them. This approach provides a safer way to deploy changes with the ability to quickly roll back if needed.</p></li>
    /// </ul>
    pub fn set_strategy(mut self, input: ::std::option::Option<crate::types::DeploymentStrategy>) -> Self {
        self.strategy = input;
        self
    }
    /// <p>The deployment strategy for the service. Choose from these valid values:</p>
    /// <ul>
    /// <li>
    /// <p><code>ROLLING</code> - When you create a service which uses the rolling update (<code>ROLLING</code>) deployment strategy, the Amazon ECS service scheduler replaces the currently running tasks with new tasks. The number of tasks that Amazon ECS adds or removes from the service during a rolling update is controlled by the service deployment configuration.</p></li>
    /// <li>
    /// <p><code>BLUE_GREEN</code> - A blue/green deployment strategy (<code>BLUE_GREEN</code>) is a release methodology that reduces downtime and risk by running two identical production environments called blue and green. With Amazon ECS blue/green deployments, you can validate new service revisions before directing production traffic to them. This approach provides a safer way to deploy changes with the ability to quickly roll back if needed.</p></li>
    /// </ul>
    pub fn get_strategy(&self) -> &::std::option::Option<crate::types::DeploymentStrategy> {
        &self.strategy
    }
    /// <p>The time period when both blue and green service revisions are running simultaneously after the production traffic has shifted.</p>
    /// <p>You must provide this parameter when you use the <code>BLUE_GREEN</code> deployment strategy.</p>
    pub fn bake_time_in_minutes(mut self, input: i32) -> Self {
        self.bake_time_in_minutes = ::std::option::Option::Some(input);
        self
    }
    /// <p>The time period when both blue and green service revisions are running simultaneously after the production traffic has shifted.</p>
    /// <p>You must provide this parameter when you use the <code>BLUE_GREEN</code> deployment strategy.</p>
    pub fn set_bake_time_in_minutes(mut self, input: ::std::option::Option<i32>) -> Self {
        self.bake_time_in_minutes = input;
        self
    }
    /// <p>The time period when both blue and green service revisions are running simultaneously after the production traffic has shifted.</p>
    /// <p>You must provide this parameter when you use the <code>BLUE_GREEN</code> deployment strategy.</p>
    pub fn get_bake_time_in_minutes(&self) -> &::std::option::Option<i32> {
        &self.bake_time_in_minutes
    }
    /// Appends an item to `lifecycle_hooks`.
    ///
    /// To override the contents of this collection use [`set_lifecycle_hooks`](Self::set_lifecycle_hooks).
    ///
    /// <p>An array of deployment lifecycle hook objects to run custom logic at specific stages of the deployment lifecycle.</p>
    pub fn lifecycle_hooks(mut self, input: crate::types::DeploymentLifecycleHook) -> Self {
        let mut v = self.lifecycle_hooks.unwrap_or_default();
        v.push(input);
        self.lifecycle_hooks = ::std::option::Option::Some(v);
        self
    }
    /// <p>An array of deployment lifecycle hook objects to run custom logic at specific stages of the deployment lifecycle.</p>
    pub fn set_lifecycle_hooks(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::DeploymentLifecycleHook>>) -> Self {
        self.lifecycle_hooks = input;
        self
    }
    /// <p>An array of deployment lifecycle hook objects to run custom logic at specific stages of the deployment lifecycle.</p>
    pub fn get_lifecycle_hooks(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::DeploymentLifecycleHook>> {
        &self.lifecycle_hooks
    }
    /// <p>Configuration for linear deployment strategy. Only valid when the deployment strategy is <code>LINEAR</code>. This configuration enables progressive traffic shifting in equal percentage increments with configurable bake times between each step.</p>
    pub fn linear_configuration(mut self, input: crate::types::LinearConfiguration) -> Self {
        self.linear_configuration = ::std::option::Option::Some(input);
        self
    }
    /// <p>Configuration for linear deployment strategy. Only valid when the deployment strategy is <code>LINEAR</code>. This configuration enables progressive traffic shifting in equal percentage increments with configurable bake times between each step.</p>
    pub fn set_linear_configuration(mut self, input: ::std::option::Option<crate::types::LinearConfiguration>) -> Self {
        self.linear_configuration = input;
        self
    }
    /// <p>Configuration for linear deployment strategy. Only valid when the deployment strategy is <code>LINEAR</code>. This configuration enables progressive traffic shifting in equal percentage increments with configurable bake times between each step.</p>
    pub fn get_linear_configuration(&self) -> &::std::option::Option<crate::types::LinearConfiguration> {
        &self.linear_configuration
    }
    /// <p>Configuration for canary deployment strategy. Only valid when the deployment strategy is <code>CANARY</code>. This configuration enables shifting a fixed percentage of traffic for testing, followed by shifting the remaining traffic after a bake period.</p>
    pub fn canary_configuration(mut self, input: crate::types::CanaryConfiguration) -> Self {
        self.canary_configuration = ::std::option::Option::Some(input);
        self
    }
    /// <p>Configuration for canary deployment strategy. Only valid when the deployment strategy is <code>CANARY</code>. This configuration enables shifting a fixed percentage of traffic for testing, followed by shifting the remaining traffic after a bake period.</p>
    pub fn set_canary_configuration(mut self, input: ::std::option::Option<crate::types::CanaryConfiguration>) -> Self {
        self.canary_configuration = input;
        self
    }
    /// <p>Configuration for canary deployment strategy. Only valid when the deployment strategy is <code>CANARY</code>. This configuration enables shifting a fixed percentage of traffic for testing, followed by shifting the remaining traffic after a bake period.</p>
    pub fn get_canary_configuration(&self) -> &::std::option::Option<crate::types::CanaryConfiguration> {
        &self.canary_configuration
    }
    /// Consumes the builder and constructs a [`DeploymentConfiguration`](crate::types::DeploymentConfiguration).
    pub fn build(self) -> crate::types::DeploymentConfiguration {
        crate::types::DeploymentConfiguration {
            deployment_circuit_breaker: self.deployment_circuit_breaker,
            maximum_percent: self.maximum_percent,
            minimum_healthy_percent: self.minimum_healthy_percent,
            alarms: self.alarms,
            strategy: self.strategy,
            bake_time_in_minutes: self.bake_time_in_minutes,
            lifecycle_hooks: self.lifecycle_hooks,
            linear_configuration: self.linear_configuration,
            canary_configuration: self.canary_configuration,
        }
    }
}
