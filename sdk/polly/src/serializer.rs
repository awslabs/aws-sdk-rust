// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq)]
pub struct PutLexiconInputBody<'a> {
    /// <p>Content of the PLS lexicon as string data.</p>
    pub content: &'a std::option::Option<std::string::String>,
}
impl<'a> std::fmt::Debug for PutLexiconInputBody<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("PutLexiconInputBody");
        formatter.field("content", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq)]
pub struct StartSpeechSynthesisTaskInputBody<'a> {
    /// <p>Specifies the engine (<code>standard</code> or <code>neural</code>) for Amazon Polly to
    /// use when processing input text for speech synthesis. Using a voice that is not supported for
    /// the engine selected will result in an error.</p>
    pub engine: &'a std::option::Option<crate::model::Engine>,
    /// <p>Optional language code for the Speech Synthesis request. This is only necessary if using a
    /// bilingual voice, such as Aditi, which can be used for either Indian English (en-IN) or Hindi
    /// (hi-IN). </p>
    /// <p>If a bilingual voice is used and no language code is specified, Amazon Polly will use the
    /// default language of the bilingual voice. The default language for any voice is the one
    /// returned by the <a href="https://docs.aws.amazon.com/polly/latest/dg/API_DescribeVoices.html">DescribeVoices</a> operation for the <code>LanguageCode</code> parameter. For example,
    /// if no language code is specified, Aditi will use Indian English rather than Hindi.</p>
    pub language_code: &'a std::option::Option<crate::model::LanguageCode>,
    /// <p>List of one or more pronunciation lexicon names you want the service to apply during
    /// synthesis. Lexicons are applied only if the language of the lexicon is the same as the
    /// language of the voice. </p>
    pub lexicon_names: &'a std::option::Option<std::vec::Vec<std::string::String>>,
    /// <p>The format in which the returned output will be encoded. For audio stream, this will be
    /// mp3, ogg_vorbis, or pcm. For speech marks, this will be json. </p>
    pub output_format: &'a std::option::Option<crate::model::OutputFormat>,
    /// <p>Amazon S3 bucket name to which the output file will be saved.</p>
    pub output_s3_bucket_name: &'a std::option::Option<std::string::String>,
    /// <p>The Amazon S3 key prefix for the output speech file.</p>
    pub output_s3_key_prefix: &'a std::option::Option<std::string::String>,
    /// <p>The audio frequency specified in Hz.</p>
    /// <p>The valid values for mp3 and ogg_vorbis are "8000", "16000", "22050", and "24000". The
    /// default value for standard voices is "22050". The default value for neural voices is
    /// "24000".</p>
    /// <p>Valid values for pcm are "8000" and "16000" The default value is "16000". </p>
    pub sample_rate: &'a std::option::Option<std::string::String>,
    /// <p>ARN for the SNS topic optionally used for providing status notification for a speech
    /// synthesis task.</p>
    pub sns_topic_arn: &'a std::option::Option<std::string::String>,
    /// <p>The type of speech marks returned for the input text.</p>
    pub speech_mark_types: &'a std::option::Option<std::vec::Vec<crate::model::SpeechMarkType>>,
    /// <p>The input text to synthesize. If you specify ssml as the TextType, follow the SSML format
    /// for the input text. </p>
    pub text: &'a std::option::Option<std::string::String>,
    /// <p>Specifies whether the input text is plain text or SSML. The default value is plain text.
    /// </p>
    pub text_type: &'a std::option::Option<crate::model::TextType>,
    /// <p>Voice ID to use for the synthesis. </p>
    pub voice_id: &'a std::option::Option<crate::model::VoiceId>,
}
impl<'a> std::fmt::Debug for StartSpeechSynthesisTaskInputBody<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("StartSpeechSynthesisTaskInputBody");
        formatter.field("engine", &self.engine);
        formatter.field("language_code", &self.language_code);
        formatter.field("lexicon_names", &self.lexicon_names);
        formatter.field("output_format", &self.output_format);
        formatter.field("output_s3_bucket_name", &self.output_s3_bucket_name);
        formatter.field("output_s3_key_prefix", &self.output_s3_key_prefix);
        formatter.field("sample_rate", &self.sample_rate);
        formatter.field("sns_topic_arn", &self.sns_topic_arn);
        formatter.field("speech_mark_types", &self.speech_mark_types);
        formatter.field("text", &self.text);
        formatter.field("text_type", &self.text_type);
        formatter.field("voice_id", &self.voice_id);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq)]
pub struct SynthesizeSpeechInputBody<'a> {
    /// <p>Specifies the engine (<code>standard</code> or <code>neural</code>) for Amazon Polly to
    /// use when processing input text for speech synthesis. For information on Amazon Polly voices and which voices are available in standard-only, NTTS-only, and
    /// both standard and NTTS formats, see <a href="https://docs.aws.amazon.com/polly/latest/dg/voicelist.html">Available Voices</a>.</p>
    /// <p>
    /// <b>NTTS-only voices</b>
    /// </p>
    /// <p>When using NTTS-only voices such as Kevin (en-US), this parameter is required and must be
    /// set to <code>neural</code>. If the engine is not specified, or is set to <code>standard</code>,
    /// this will result in an error. </p>
    /// <p>Type: String</p>
    /// <p>Valid Values: <code>standard</code>  |  <code>neural</code>
    /// </p>
    /// <p>Required: Yes</p>
    /// <p>
    /// <b>Standard voices</b>
    /// </p>
    /// <p>For standard voices, this is not required; the engine parameter defaults to
    /// <code>standard</code>. If the engine is not specified, or is set to <code>standard</code> and
    /// an NTTS-only voice is selected, this will result in an error. </p>
    pub engine: &'a std::option::Option<crate::model::Engine>,
    /// <p>Optional language code for the Synthesize Speech request. This is only necessary if using
    /// a bilingual voice, such as Aditi, which can be used for either Indian English (en-IN) or Hindi
    /// (hi-IN). </p>
    /// <p>If a bilingual voice is used and no language code is specified, Amazon Polly will use the
    /// default language of the bilingual voice. The default language for any voice is the one
    /// returned by the <a href="https://docs.aws.amazon.com/polly/latest/dg/API_DescribeVoices.html">DescribeVoices</a> operation for the <code>LanguageCode</code> parameter. For example,
    /// if no language code is specified, Aditi will use Indian English rather than Hindi.</p>
    pub language_code: &'a std::option::Option<crate::model::LanguageCode>,
    /// <p>List of one or more pronunciation lexicon names you want the service to apply during
    /// synthesis. Lexicons are applied only if the language of the lexicon is the same as the
    /// language of the voice. For information about storing lexicons, see <a href="https://docs.aws.amazon.com/polly/latest/dg/API_PutLexicon.html">PutLexicon</a>.</p>
    pub lexicon_names: &'a std::option::Option<std::vec::Vec<std::string::String>>,
    /// <p> The format in which the returned output will be encoded. For audio stream, this will
    /// be mp3, ogg_vorbis, or pcm. For speech marks, this will be json. </p>
    /// <p>When pcm is used, the content returned is audio/pcm in a signed 16-bit, 1 channel
    /// (mono), little-endian format. </p>
    pub output_format: &'a std::option::Option<crate::model::OutputFormat>,
    /// <p>The audio frequency specified in Hz.</p>
    /// <p>The valid values for mp3 and ogg_vorbis are "8000", "16000", "22050", and "24000". The
    /// default value for standard voices is "22050". The default value for neural voices is
    /// "24000".</p>
    /// <p>Valid values for pcm are "8000" and "16000" The default value is "16000". </p>
    pub sample_rate: &'a std::option::Option<std::string::String>,
    /// <p>The type of speech marks returned for the input text.</p>
    pub speech_mark_types: &'a std::option::Option<std::vec::Vec<crate::model::SpeechMarkType>>,
    /// <p> Input text to synthesize. If you specify <code>ssml</code> as the
    /// <code>TextType</code>, follow the SSML format for the input text. </p>
    pub text: &'a std::option::Option<std::string::String>,
    /// <p> Specifies whether the input text is plain text or SSML. The default value is plain
    /// text. For more information, see <a href="https://docs.aws.amazon.com/polly/latest/dg/ssml.html">Using SSML</a>.</p>
    pub text_type: &'a std::option::Option<crate::model::TextType>,
    /// <p> Voice ID to use for the synthesis. You can get a list of available voice IDs by
    /// calling the <a href="https://docs.aws.amazon.com/polly/latest/dg/API_DescribeVoices.html">DescribeVoices</a> operation. </p>
    pub voice_id: &'a std::option::Option<crate::model::VoiceId>,
}
impl<'a> std::fmt::Debug for SynthesizeSpeechInputBody<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("SynthesizeSpeechInputBody");
        formatter.field("engine", &self.engine);
        formatter.field("language_code", &self.language_code);
        formatter.field("lexicon_names", &self.lexicon_names);
        formatter.field("output_format", &self.output_format);
        formatter.field("sample_rate", &self.sample_rate);
        formatter.field("speech_mark_types", &self.speech_mark_types);
        formatter.field("text", &self.text);
        formatter.field("text_type", &self.text_type);
        formatter.field("voice_id", &self.voice_id);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::default::Default, serde::Deserialize, std::clone::Clone, std::cmp::PartialEq)]
pub struct DescribeVoicesOutputBody {
    /// <p>A list of voices with their properties.</p>
    #[serde(rename = "Voices")]
    #[serde(default)]
    pub voices: std::option::Option<std::vec::Vec<crate::model::Voice>>,
    /// <p>The pagination token to use in the next request to continue the listing of voices.
    /// <code>NextToken</code> is returned only if the response is truncated.</p>
    #[serde(rename = "NextToken")]
    #[serde(default)]
    pub next_token: std::option::Option<std::string::String>,
}
impl std::fmt::Debug for DescribeVoicesOutputBody {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("DescribeVoicesOutputBody");
        formatter.field("voices", &self.voices);
        formatter.field("next_token", &self.next_token);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::default::Default, serde::Deserialize, std::clone::Clone, std::cmp::PartialEq)]
pub struct GetLexiconOutputBody {
    /// <p>Lexicon object that provides name and the string content of the lexicon. </p>
    #[serde(rename = "Lexicon")]
    #[serde(default)]
    pub lexicon: std::option::Option<crate::model::Lexicon>,
    /// <p>Metadata of the lexicon, including phonetic alphabetic used, language code, lexicon
    /// ARN, number of lexemes defined in the lexicon, and size of lexicon in bytes.</p>
    #[serde(rename = "LexiconAttributes")]
    #[serde(default)]
    pub lexicon_attributes: std::option::Option<crate::model::LexiconAttributes>,
}
impl std::fmt::Debug for GetLexiconOutputBody {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("GetLexiconOutputBody");
        formatter.field("lexicon", &self.lexicon);
        formatter.field("lexicon_attributes", &self.lexicon_attributes);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::default::Default, serde::Deserialize, std::clone::Clone, std::cmp::PartialEq)]
pub struct GetSpeechSynthesisTaskOutputBody {
    /// <p>SynthesisTask object that provides information from the requested task, including output
    /// format, creation time, task status, and so on.</p>
    #[serde(rename = "SynthesisTask")]
    #[serde(default)]
    pub synthesis_task: std::option::Option<crate::model::SynthesisTask>,
}
impl std::fmt::Debug for GetSpeechSynthesisTaskOutputBody {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("GetSpeechSynthesisTaskOutputBody");
        formatter.field("synthesis_task", &self.synthesis_task);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::default::Default, serde::Deserialize, std::clone::Clone, std::cmp::PartialEq)]
pub struct ListLexiconsOutputBody {
    /// <p>A list of lexicon names and attributes.</p>
    #[serde(rename = "Lexicons")]
    #[serde(default)]
    pub lexicons: std::option::Option<std::vec::Vec<crate::model::LexiconDescription>>,
    /// <p>The pagination token to use in the next request to continue the listing of lexicons.
    /// <code>NextToken</code> is returned only if the response is truncated.</p>
    #[serde(rename = "NextToken")]
    #[serde(default)]
    pub next_token: std::option::Option<std::string::String>,
}
impl std::fmt::Debug for ListLexiconsOutputBody {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("ListLexiconsOutputBody");
        formatter.field("lexicons", &self.lexicons);
        formatter.field("next_token", &self.next_token);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::default::Default, serde::Deserialize, std::clone::Clone, std::cmp::PartialEq)]
pub struct ListSpeechSynthesisTasksOutputBody {
    /// <p>An opaque pagination token returned from the previous List operation in this request. If
    /// present, this indicates where to continue the listing.</p>
    #[serde(rename = "NextToken")]
    #[serde(default)]
    pub next_token: std::option::Option<std::string::String>,
    /// <p>List of SynthesisTask objects that provides information from the specified task in the
    /// list request, including output format, creation time, task status, and so on.</p>
    #[serde(rename = "SynthesisTasks")]
    #[serde(default)]
    pub synthesis_tasks: std::option::Option<std::vec::Vec<crate::model::SynthesisTask>>,
}
impl std::fmt::Debug for ListSpeechSynthesisTasksOutputBody {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("ListSpeechSynthesisTasksOutputBody");
        formatter.field("next_token", &self.next_token);
        formatter.field("synthesis_tasks", &self.synthesis_tasks);
        formatter.finish()
    }
}

#[non_exhaustive]
#[derive(std::default::Default, serde::Deserialize, std::clone::Clone, std::cmp::PartialEq)]
pub struct StartSpeechSynthesisTaskOutputBody {
    /// <p>SynthesisTask object that provides information and attributes about a newly submitted
    /// speech synthesis task.</p>
    #[serde(rename = "SynthesisTask")]
    #[serde(default)]
    pub synthesis_task: std::option::Option<crate::model::SynthesisTask>,
}
impl std::fmt::Debug for StartSpeechSynthesisTaskOutputBody {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("StartSpeechSynthesisTaskOutputBody");
        formatter.field("synthesis_task", &self.synthesis_task);
        formatter.finish()
    }
}
