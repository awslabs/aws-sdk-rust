// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
pub use crate::operation::create_model_invocation_job::_create_model_invocation_job_input::CreateModelInvocationJobInputBuilder;

pub use crate::operation::create_model_invocation_job::_create_model_invocation_job_output::CreateModelInvocationJobOutputBuilder;

impl crate::operation::create_model_invocation_job::builders::CreateModelInvocationJobInputBuilder {
    /// Sends a request with this input using the given client.
    pub async fn send_with(
        self,
        client: &crate::Client,
    ) -> ::std::result::Result<
        crate::operation::create_model_invocation_job::CreateModelInvocationJobOutput,
        ::aws_smithy_runtime_api::client::result::SdkError<
            crate::operation::create_model_invocation_job::CreateModelInvocationJobError,
            ::aws_smithy_runtime_api::client::orchestrator::HttpResponse,
        >,
    > {
        let mut fluent_builder = client.create_model_invocation_job();
        fluent_builder.inner = self;
        fluent_builder.send().await
    }
}
/// Fluent builder constructing a request to `CreateModelInvocationJob`.
///
/// <p>Creates a batch inference job to invoke a model on multiple prompts. Format your data according to <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-data">Format your inference data</a> and upload it to an Amazon S3 bucket. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html">Process multiple prompts with batch inference</a>.</p>
/// <p>The response returns a <code>jobArn</code> that you can use to stop or get details about the job.</p>
#[derive(::std::clone::Clone, ::std::fmt::Debug)]
pub struct CreateModelInvocationJobFluentBuilder {
    handle: ::std::sync::Arc<crate::client::Handle>,
    inner: crate::operation::create_model_invocation_job::builders::CreateModelInvocationJobInputBuilder,
    config_override: ::std::option::Option<crate::config::Builder>,
}
impl
    crate::client::customize::internal::CustomizableSend<
        crate::operation::create_model_invocation_job::CreateModelInvocationJobOutput,
        crate::operation::create_model_invocation_job::CreateModelInvocationJobError,
    > for CreateModelInvocationJobFluentBuilder
{
    fn send(
        self,
        config_override: crate::config::Builder,
    ) -> crate::client::customize::internal::BoxFuture<
        crate::client::customize::internal::SendResult<
            crate::operation::create_model_invocation_job::CreateModelInvocationJobOutput,
            crate::operation::create_model_invocation_job::CreateModelInvocationJobError,
        >,
    > {
        ::std::boxed::Box::pin(async move { self.config_override(config_override).send().await })
    }
}
impl CreateModelInvocationJobFluentBuilder {
    /// Creates a new `CreateModelInvocationJobFluentBuilder`.
    pub(crate) fn new(handle: ::std::sync::Arc<crate::client::Handle>) -> Self {
        Self {
            handle,
            inner: ::std::default::Default::default(),
            config_override: ::std::option::Option::None,
        }
    }
    /// Access the CreateModelInvocationJob as a reference.
    pub fn as_input(&self) -> &crate::operation::create_model_invocation_job::builders::CreateModelInvocationJobInputBuilder {
        &self.inner
    }
    /// Sends the request and returns the response.
    ///
    /// If an error occurs, an `SdkError` will be returned with additional details that
    /// can be matched against.
    ///
    /// By default, any retryable failures will be retried twice. Retry behavior
    /// is configurable with the [RetryConfig](aws_smithy_types::retry::RetryConfig), which can be
    /// set when configuring the client.
    pub async fn send(
        self,
    ) -> ::std::result::Result<
        crate::operation::create_model_invocation_job::CreateModelInvocationJobOutput,
        ::aws_smithy_runtime_api::client::result::SdkError<
            crate::operation::create_model_invocation_job::CreateModelInvocationJobError,
            ::aws_smithy_runtime_api::client::orchestrator::HttpResponse,
        >,
    > {
        let input = self
            .inner
            .build()
            .map_err(::aws_smithy_runtime_api::client::result::SdkError::construction_failure)?;
        let runtime_plugins = crate::operation::create_model_invocation_job::CreateModelInvocationJob::operation_runtime_plugins(
            self.handle.runtime_plugins.clone(),
            &self.handle.conf,
            self.config_override,
        );
        crate::operation::create_model_invocation_job::CreateModelInvocationJob::orchestrate(&runtime_plugins, input).await
    }

    /// Consumes this builder, creating a customizable operation that can be modified before being sent.
    pub fn customize(
        self,
    ) -> crate::client::customize::CustomizableOperation<
        crate::operation::create_model_invocation_job::CreateModelInvocationJobOutput,
        crate::operation::create_model_invocation_job::CreateModelInvocationJobError,
        Self,
    > {
        crate::client::customize::CustomizableOperation::new(self)
    }
    pub(crate) fn config_override(mut self, config_override: impl ::std::convert::Into<crate::config::Builder>) -> Self {
        self.set_config_override(::std::option::Option::Some(config_override.into()));
        self
    }

    pub(crate) fn set_config_override(&mut self, config_override: ::std::option::Option<crate::config::Builder>) -> &mut Self {
        self.config_override = config_override;
        self
    }
    /// <p>A name to give the batch inference job.</p>
    pub fn job_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.job_name(input.into());
        self
    }
    /// <p>A name to give the batch inference job.</p>
    pub fn set_job_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_job_name(input);
        self
    }
    /// <p>A name to give the batch inference job.</p>
    pub fn get_job_name(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_job_name()
    }
    /// <p>The Amazon Resource Name (ARN) of the service role with permissions to carry out and manage batch inference. You can use the console to create a default service role or follow the steps at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html">Create a service role for batch inference</a>.</p>
    pub fn role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.role_arn(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the service role with permissions to carry out and manage batch inference. You can use the console to create a default service role or follow the steps at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html">Create a service role for batch inference</a>.</p>
    pub fn set_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_role_arn(input);
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the service role with permissions to carry out and manage batch inference. You can use the console to create a default service role or follow the steps at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html">Create a service role for batch inference</a>.</p>
    pub fn get_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_role_arn()
    }
    /// <p>A unique, case-sensitive identifier to ensure that the API request completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error. For more information, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Run_Instance_Idempotency.html">Ensuring idempotency</a>.</p>
    pub fn client_request_token(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.client_request_token(input.into());
        self
    }
    /// <p>A unique, case-sensitive identifier to ensure that the API request completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error. For more information, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Run_Instance_Idempotency.html">Ensuring idempotency</a>.</p>
    pub fn set_client_request_token(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_client_request_token(input);
        self
    }
    /// <p>A unique, case-sensitive identifier to ensure that the API request completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error. For more information, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Run_Instance_Idempotency.html">Ensuring idempotency</a>.</p>
    pub fn get_client_request_token(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_client_request_token()
    }
    /// <p>The unique identifier of the foundation model to use for the batch inference job.</p>
    pub fn model_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.model_id(input.into());
        self
    }
    /// <p>The unique identifier of the foundation model to use for the batch inference job.</p>
    pub fn set_model_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_model_id(input);
        self
    }
    /// <p>The unique identifier of the foundation model to use for the batch inference job.</p>
    pub fn get_model_id(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_model_id()
    }
    /// <p>Details about the location of the input to the batch inference job.</p>
    pub fn input_data_config(mut self, input: crate::types::ModelInvocationJobInputDataConfig) -> Self {
        self.inner = self.inner.input_data_config(input);
        self
    }
    /// <p>Details about the location of the input to the batch inference job.</p>
    pub fn set_input_data_config(mut self, input: ::std::option::Option<crate::types::ModelInvocationJobInputDataConfig>) -> Self {
        self.inner = self.inner.set_input_data_config(input);
        self
    }
    /// <p>Details about the location of the input to the batch inference job.</p>
    pub fn get_input_data_config(&self) -> &::std::option::Option<crate::types::ModelInvocationJobInputDataConfig> {
        self.inner.get_input_data_config()
    }
    /// <p>Details about the location of the output of the batch inference job.</p>
    pub fn output_data_config(mut self, input: crate::types::ModelInvocationJobOutputDataConfig) -> Self {
        self.inner = self.inner.output_data_config(input);
        self
    }
    /// <p>Details about the location of the output of the batch inference job.</p>
    pub fn set_output_data_config(mut self, input: ::std::option::Option<crate::types::ModelInvocationJobOutputDataConfig>) -> Self {
        self.inner = self.inner.set_output_data_config(input);
        self
    }
    /// <p>Details about the location of the output of the batch inference job.</p>
    pub fn get_output_data_config(&self) -> &::std::option::Option<crate::types::ModelInvocationJobOutputDataConfig> {
        self.inner.get_output_data_config()
    }
    /// <p>The configuration of the Virtual Private Cloud (VPC) for the data in the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-vpc">Protect batch inference jobs using a VPC</a>.</p>
    pub fn vpc_config(mut self, input: crate::types::VpcConfig) -> Self {
        self.inner = self.inner.vpc_config(input);
        self
    }
    /// <p>The configuration of the Virtual Private Cloud (VPC) for the data in the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-vpc">Protect batch inference jobs using a VPC</a>.</p>
    pub fn set_vpc_config(mut self, input: ::std::option::Option<crate::types::VpcConfig>) -> Self {
        self.inner = self.inner.set_vpc_config(input);
        self
    }
    /// <p>The configuration of the Virtual Private Cloud (VPC) for the data in the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-vpc">Protect batch inference jobs using a VPC</a>.</p>
    pub fn get_vpc_config(&self) -> &::std::option::Option<crate::types::VpcConfig> {
        self.inner.get_vpc_config()
    }
    /// <p>The number of hours after which to force the batch inference job to time out.</p>
    pub fn timeout_duration_in_hours(mut self, input: i32) -> Self {
        self.inner = self.inner.timeout_duration_in_hours(input);
        self
    }
    /// <p>The number of hours after which to force the batch inference job to time out.</p>
    pub fn set_timeout_duration_in_hours(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_timeout_duration_in_hours(input);
        self
    }
    /// <p>The number of hours after which to force the batch inference job to time out.</p>
    pub fn get_timeout_duration_in_hours(&self) -> &::std::option::Option<i32> {
        self.inner.get_timeout_duration_in_hours()
    }
    ///
    /// Appends an item to `tags`.
    ///
    /// To override the contents of this collection use [`set_tags`](Self::set_tags).
    ///
    /// <p>Any tags to associate with the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/tagging.html">Tagging Amazon Bedrock resources</a>.</p>
    pub fn tags(mut self, input: crate::types::Tag) -> Self {
        self.inner = self.inner.tags(input);
        self
    }
    /// <p>Any tags to associate with the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/tagging.html">Tagging Amazon Bedrock resources</a>.</p>
    pub fn set_tags(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::Tag>>) -> Self {
        self.inner = self.inner.set_tags(input);
        self
    }
    /// <p>Any tags to associate with the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/tagging.html">Tagging Amazon Bedrock resources</a>.</p>
    pub fn get_tags(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::Tag>> {
        self.inner.get_tags()
    }
    /// <p>The invocation endpoint for ModelInvocationJob</p>
    pub fn model_invocation_type(mut self, input: crate::types::ModelInvocationType) -> Self {
        self.inner = self.inner.model_invocation_type(input);
        self
    }
    /// <p>The invocation endpoint for ModelInvocationJob</p>
    pub fn set_model_invocation_type(mut self, input: ::std::option::Option<crate::types::ModelInvocationType>) -> Self {
        self.inner = self.inner.set_model_invocation_type(input);
        self
    }
    /// <p>The invocation endpoint for ModelInvocationJob</p>
    pub fn get_model_invocation_type(&self) -> &::std::option::Option<crate::types::ModelInvocationType> {
        self.inner.get_model_invocation_type()
    }
}
