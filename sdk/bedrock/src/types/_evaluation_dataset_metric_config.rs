// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Defines the prompt datasets, built-in metric names and custom metric names, and the task type.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct EvaluationDatasetMetricConfig {
    /// <p>The the type of task you want to evaluate for your evaluation job. This applies only to model evaluation jobs and is ignored for knowledge base evaluation jobs.</p>
    pub task_type: crate::types::EvaluationTaskType,
    /// <p>Specifies the prompt dataset.</p>
    pub dataset: ::std::option::Option<crate::types::EvaluationDataset>,
    /// <p>The names of the metrics you want to use for your evaluation job.</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval only, valid values are "<code>Builtin.ContextRelevance</code>", "<code>Builtin.ContextConverage</code>".</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval with response generation, valid values are "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness</code>", "<code>Builtin.Helpfulness</code>", "<code>Builtin.LogicalCoherence</code>", "<code>Builtin.Faithfulness</code>", "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", "<code>Builtin.Refusal</code>".</p>
    /// <p>For automated model evaluation jobs, valid values are "<code>Builtin.Accuracy</code>", "<code>Builtin.Robustness</code>", and "<code>Builtin.Toxicity</code>". In model evaluation jobs that use a LLM as judge you can specify "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness"</code>, "<code>Builtin.Faithfulness"</code>, "<code>Builtin.Helpfulness</code>", "<code>Builtin.Coherence</code>", "<code>Builtin.Relevance</code>", "<code>Builtin.FollowingInstructions</code>", "<code>Builtin.ProfessionalStyleAndTone</code>", You can also specify the following responsible AI related metrics only for model evaluation job that use a LLM as judge "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", and "<code>Builtin.Refusal</code>".</p>
    /// <p>For human-based model evaluation jobs, the list of strings must match the <code>name</code> parameter specified in <code>HumanEvaluationCustomMetric</code>.</p>
    pub metric_names: ::std::vec::Vec<::std::string::String>,
}
impl EvaluationDatasetMetricConfig {
    /// <p>The the type of task you want to evaluate for your evaluation job. This applies only to model evaluation jobs and is ignored for knowledge base evaluation jobs.</p>
    pub fn task_type(&self) -> &crate::types::EvaluationTaskType {
        &self.task_type
    }
    /// <p>Specifies the prompt dataset.</p>
    pub fn dataset(&self) -> ::std::option::Option<&crate::types::EvaluationDataset> {
        self.dataset.as_ref()
    }
    /// <p>The names of the metrics you want to use for your evaluation job.</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval only, valid values are "<code>Builtin.ContextRelevance</code>", "<code>Builtin.ContextConverage</code>".</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval with response generation, valid values are "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness</code>", "<code>Builtin.Helpfulness</code>", "<code>Builtin.LogicalCoherence</code>", "<code>Builtin.Faithfulness</code>", "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", "<code>Builtin.Refusal</code>".</p>
    /// <p>For automated model evaluation jobs, valid values are "<code>Builtin.Accuracy</code>", "<code>Builtin.Robustness</code>", and "<code>Builtin.Toxicity</code>". In model evaluation jobs that use a LLM as judge you can specify "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness"</code>, "<code>Builtin.Faithfulness"</code>, "<code>Builtin.Helpfulness</code>", "<code>Builtin.Coherence</code>", "<code>Builtin.Relevance</code>", "<code>Builtin.FollowingInstructions</code>", "<code>Builtin.ProfessionalStyleAndTone</code>", You can also specify the following responsible AI related metrics only for model evaluation job that use a LLM as judge "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", and "<code>Builtin.Refusal</code>".</p>
    /// <p>For human-based model evaluation jobs, the list of strings must match the <code>name</code> parameter specified in <code>HumanEvaluationCustomMetric</code>.</p>
    pub fn metric_names(&self) -> &[::std::string::String] {
        use std::ops::Deref;
        self.metric_names.deref()
    }
}
impl EvaluationDatasetMetricConfig {
    /// Creates a new builder-style object to manufacture [`EvaluationDatasetMetricConfig`](crate::types::EvaluationDatasetMetricConfig).
    pub fn builder() -> crate::types::builders::EvaluationDatasetMetricConfigBuilder {
        crate::types::builders::EvaluationDatasetMetricConfigBuilder::default()
    }
}

/// A builder for [`EvaluationDatasetMetricConfig`](crate::types::EvaluationDatasetMetricConfig).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct EvaluationDatasetMetricConfigBuilder {
    pub(crate) task_type: ::std::option::Option<crate::types::EvaluationTaskType>,
    pub(crate) dataset: ::std::option::Option<crate::types::EvaluationDataset>,
    pub(crate) metric_names: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
}
impl EvaluationDatasetMetricConfigBuilder {
    /// <p>The the type of task you want to evaluate for your evaluation job. This applies only to model evaluation jobs and is ignored for knowledge base evaluation jobs.</p>
    /// This field is required.
    pub fn task_type(mut self, input: crate::types::EvaluationTaskType) -> Self {
        self.task_type = ::std::option::Option::Some(input);
        self
    }
    /// <p>The the type of task you want to evaluate for your evaluation job. This applies only to model evaluation jobs and is ignored for knowledge base evaluation jobs.</p>
    pub fn set_task_type(mut self, input: ::std::option::Option<crate::types::EvaluationTaskType>) -> Self {
        self.task_type = input;
        self
    }
    /// <p>The the type of task you want to evaluate for your evaluation job. This applies only to model evaluation jobs and is ignored for knowledge base evaluation jobs.</p>
    pub fn get_task_type(&self) -> &::std::option::Option<crate::types::EvaluationTaskType> {
        &self.task_type
    }
    /// <p>Specifies the prompt dataset.</p>
    /// This field is required.
    pub fn dataset(mut self, input: crate::types::EvaluationDataset) -> Self {
        self.dataset = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies the prompt dataset.</p>
    pub fn set_dataset(mut self, input: ::std::option::Option<crate::types::EvaluationDataset>) -> Self {
        self.dataset = input;
        self
    }
    /// <p>Specifies the prompt dataset.</p>
    pub fn get_dataset(&self) -> &::std::option::Option<crate::types::EvaluationDataset> {
        &self.dataset
    }
    /// Appends an item to `metric_names`.
    ///
    /// To override the contents of this collection use [`set_metric_names`](Self::set_metric_names).
    ///
    /// <p>The names of the metrics you want to use for your evaluation job.</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval only, valid values are "<code>Builtin.ContextRelevance</code>", "<code>Builtin.ContextConverage</code>".</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval with response generation, valid values are "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness</code>", "<code>Builtin.Helpfulness</code>", "<code>Builtin.LogicalCoherence</code>", "<code>Builtin.Faithfulness</code>", "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", "<code>Builtin.Refusal</code>".</p>
    /// <p>For automated model evaluation jobs, valid values are "<code>Builtin.Accuracy</code>", "<code>Builtin.Robustness</code>", and "<code>Builtin.Toxicity</code>". In model evaluation jobs that use a LLM as judge you can specify "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness"</code>, "<code>Builtin.Faithfulness"</code>, "<code>Builtin.Helpfulness</code>", "<code>Builtin.Coherence</code>", "<code>Builtin.Relevance</code>", "<code>Builtin.FollowingInstructions</code>", "<code>Builtin.ProfessionalStyleAndTone</code>", You can also specify the following responsible AI related metrics only for model evaluation job that use a LLM as judge "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", and "<code>Builtin.Refusal</code>".</p>
    /// <p>For human-based model evaluation jobs, the list of strings must match the <code>name</code> parameter specified in <code>HumanEvaluationCustomMetric</code>.</p>
    pub fn metric_names(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.metric_names.unwrap_or_default();
        v.push(input.into());
        self.metric_names = ::std::option::Option::Some(v);
        self
    }
    /// <p>The names of the metrics you want to use for your evaluation job.</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval only, valid values are "<code>Builtin.ContextRelevance</code>", "<code>Builtin.ContextConverage</code>".</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval with response generation, valid values are "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness</code>", "<code>Builtin.Helpfulness</code>", "<code>Builtin.LogicalCoherence</code>", "<code>Builtin.Faithfulness</code>", "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", "<code>Builtin.Refusal</code>".</p>
    /// <p>For automated model evaluation jobs, valid values are "<code>Builtin.Accuracy</code>", "<code>Builtin.Robustness</code>", and "<code>Builtin.Toxicity</code>". In model evaluation jobs that use a LLM as judge you can specify "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness"</code>, "<code>Builtin.Faithfulness"</code>, "<code>Builtin.Helpfulness</code>", "<code>Builtin.Coherence</code>", "<code>Builtin.Relevance</code>", "<code>Builtin.FollowingInstructions</code>", "<code>Builtin.ProfessionalStyleAndTone</code>", You can also specify the following responsible AI related metrics only for model evaluation job that use a LLM as judge "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", and "<code>Builtin.Refusal</code>".</p>
    /// <p>For human-based model evaluation jobs, the list of strings must match the <code>name</code> parameter specified in <code>HumanEvaluationCustomMetric</code>.</p>
    pub fn set_metric_names(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.metric_names = input;
        self
    }
    /// <p>The names of the metrics you want to use for your evaluation job.</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval only, valid values are "<code>Builtin.ContextRelevance</code>", "<code>Builtin.ContextConverage</code>".</p>
    /// <p>For knowledge base evaluation jobs that evaluate retrieval with response generation, valid values are "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness</code>", "<code>Builtin.Helpfulness</code>", "<code>Builtin.LogicalCoherence</code>", "<code>Builtin.Faithfulness</code>", "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", "<code>Builtin.Refusal</code>".</p>
    /// <p>For automated model evaluation jobs, valid values are "<code>Builtin.Accuracy</code>", "<code>Builtin.Robustness</code>", and "<code>Builtin.Toxicity</code>". In model evaluation jobs that use a LLM as judge you can specify "<code>Builtin.Correctness</code>", "<code>Builtin.Completeness"</code>, "<code>Builtin.Faithfulness"</code>, "<code>Builtin.Helpfulness</code>", "<code>Builtin.Coherence</code>", "<code>Builtin.Relevance</code>", "<code>Builtin.FollowingInstructions</code>", "<code>Builtin.ProfessionalStyleAndTone</code>", You can also specify the following responsible AI related metrics only for model evaluation job that use a LLM as judge "<code>Builtin.Harmfulness</code>", "<code>Builtin.Stereotyping</code>", and "<code>Builtin.Refusal</code>".</p>
    /// <p>For human-based model evaluation jobs, the list of strings must match the <code>name</code> parameter specified in <code>HumanEvaluationCustomMetric</code>.</p>
    pub fn get_metric_names(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.metric_names
    }
    /// Consumes the builder and constructs a [`EvaluationDatasetMetricConfig`](crate::types::EvaluationDatasetMetricConfig).
    /// This method will fail if any of the following fields are not set:
    /// - [`task_type`](crate::types::builders::EvaluationDatasetMetricConfigBuilder::task_type)
    /// - [`metric_names`](crate::types::builders::EvaluationDatasetMetricConfigBuilder::metric_names)
    pub fn build(self) -> ::std::result::Result<crate::types::EvaluationDatasetMetricConfig, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::EvaluationDatasetMetricConfig {
            task_type: self.task_type.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "task_type",
                    "task_type was not specified but it is required when building EvaluationDatasetMetricConfig",
                )
            })?,
            dataset: self.dataset,
            metric_names: self.metric_names.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "metric_names",
                    "metric_names was not specified but it is required when building EvaluationDatasetMetricConfig",
                )
            })?,
        })
    }
}
