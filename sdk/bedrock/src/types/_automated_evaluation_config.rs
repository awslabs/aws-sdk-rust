// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>The configuration details of an automated evaluation job. The <code>EvaluationDatasetMetricConfig</code> object is used to specify the prompt datasets, task type, and metric names.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct AutomatedEvaluationConfig {
    /// <p>Configuration details of the prompt datasets and metrics you want to use for your evaluation job.</p>
    pub dataset_metric_configs: ::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>,
    /// <p>Contains the evaluator model configuration details. <code>EvaluatorModelConfig</code> is required for evaluation jobs that use a knowledge base or in model evaluation job that use a model as judge. This model computes all evaluation related metrics.</p>
    pub evaluator_model_config: ::std::option::Option<crate::types::EvaluatorModelConfig>,
    /// <p>Defines the configuration of custom metrics to be used in an evaluation job.</p>
    pub custom_metric_config: ::std::option::Option<crate::types::AutomatedEvaluationCustomMetricConfig>,
}
impl AutomatedEvaluationConfig {
    /// <p>Configuration details of the prompt datasets and metrics you want to use for your evaluation job.</p>
    pub fn dataset_metric_configs(&self) -> &[crate::types::EvaluationDatasetMetricConfig] {
        use std::ops::Deref;
        self.dataset_metric_configs.deref()
    }
    /// <p>Contains the evaluator model configuration details. <code>EvaluatorModelConfig</code> is required for evaluation jobs that use a knowledge base or in model evaluation job that use a model as judge. This model computes all evaluation related metrics.</p>
    pub fn evaluator_model_config(&self) -> ::std::option::Option<&crate::types::EvaluatorModelConfig> {
        self.evaluator_model_config.as_ref()
    }
    /// <p>Defines the configuration of custom metrics to be used in an evaluation job.</p>
    pub fn custom_metric_config(&self) -> ::std::option::Option<&crate::types::AutomatedEvaluationCustomMetricConfig> {
        self.custom_metric_config.as_ref()
    }
}
impl AutomatedEvaluationConfig {
    /// Creates a new builder-style object to manufacture [`AutomatedEvaluationConfig`](crate::types::AutomatedEvaluationConfig).
    pub fn builder() -> crate::types::builders::AutomatedEvaluationConfigBuilder {
        crate::types::builders::AutomatedEvaluationConfigBuilder::default()
    }
}

/// A builder for [`AutomatedEvaluationConfig`](crate::types::AutomatedEvaluationConfig).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct AutomatedEvaluationConfigBuilder {
    pub(crate) dataset_metric_configs: ::std::option::Option<::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>>,
    pub(crate) evaluator_model_config: ::std::option::Option<crate::types::EvaluatorModelConfig>,
    pub(crate) custom_metric_config: ::std::option::Option<crate::types::AutomatedEvaluationCustomMetricConfig>,
}
impl AutomatedEvaluationConfigBuilder {
    /// Appends an item to `dataset_metric_configs`.
    ///
    /// To override the contents of this collection use [`set_dataset_metric_configs`](Self::set_dataset_metric_configs).
    ///
    /// <p>Configuration details of the prompt datasets and metrics you want to use for your evaluation job.</p>
    pub fn dataset_metric_configs(mut self, input: crate::types::EvaluationDatasetMetricConfig) -> Self {
        let mut v = self.dataset_metric_configs.unwrap_or_default();
        v.push(input);
        self.dataset_metric_configs = ::std::option::Option::Some(v);
        self
    }
    /// <p>Configuration details of the prompt datasets and metrics you want to use for your evaluation job.</p>
    pub fn set_dataset_metric_configs(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>>) -> Self {
        self.dataset_metric_configs = input;
        self
    }
    /// <p>Configuration details of the prompt datasets and metrics you want to use for your evaluation job.</p>
    pub fn get_dataset_metric_configs(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::EvaluationDatasetMetricConfig>> {
        &self.dataset_metric_configs
    }
    /// <p>Contains the evaluator model configuration details. <code>EvaluatorModelConfig</code> is required for evaluation jobs that use a knowledge base or in model evaluation job that use a model as judge. This model computes all evaluation related metrics.</p>
    pub fn evaluator_model_config(mut self, input: crate::types::EvaluatorModelConfig) -> Self {
        self.evaluator_model_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Contains the evaluator model configuration details. <code>EvaluatorModelConfig</code> is required for evaluation jobs that use a knowledge base or in model evaluation job that use a model as judge. This model computes all evaluation related metrics.</p>
    pub fn set_evaluator_model_config(mut self, input: ::std::option::Option<crate::types::EvaluatorModelConfig>) -> Self {
        self.evaluator_model_config = input;
        self
    }
    /// <p>Contains the evaluator model configuration details. <code>EvaluatorModelConfig</code> is required for evaluation jobs that use a knowledge base or in model evaluation job that use a model as judge. This model computes all evaluation related metrics.</p>
    pub fn get_evaluator_model_config(&self) -> &::std::option::Option<crate::types::EvaluatorModelConfig> {
        &self.evaluator_model_config
    }
    /// <p>Defines the configuration of custom metrics to be used in an evaluation job.</p>
    pub fn custom_metric_config(mut self, input: crate::types::AutomatedEvaluationCustomMetricConfig) -> Self {
        self.custom_metric_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Defines the configuration of custom metrics to be used in an evaluation job.</p>
    pub fn set_custom_metric_config(mut self, input: ::std::option::Option<crate::types::AutomatedEvaluationCustomMetricConfig>) -> Self {
        self.custom_metric_config = input;
        self
    }
    /// <p>Defines the configuration of custom metrics to be used in an evaluation job.</p>
    pub fn get_custom_metric_config(&self) -> &::std::option::Option<crate::types::AutomatedEvaluationCustomMetricConfig> {
        &self.custom_metric_config
    }
    /// Consumes the builder and constructs a [`AutomatedEvaluationConfig`](crate::types::AutomatedEvaluationConfig).
    /// This method will fail if any of the following fields are not set:
    /// - [`dataset_metric_configs`](crate::types::builders::AutomatedEvaluationConfigBuilder::dataset_metric_configs)
    pub fn build(self) -> ::std::result::Result<crate::types::AutomatedEvaluationConfig, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::AutomatedEvaluationConfig {
            dataset_metric_configs: self.dataset_metric_configs.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "dataset_metric_configs",
                    "dataset_metric_configs was not specified but it is required when building AutomatedEvaluationConfig",
                )
            })?,
            evaluator_model_config: self.evaluator_model_config,
            custom_metric_config: self.custom_metric_config,
        })
    }
}
