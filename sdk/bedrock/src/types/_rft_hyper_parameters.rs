// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Hyperparameters for controlling the reinforcement fine-tuning training process, including learning settings and evaluation intervals.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct RftHyperParameters {
    /// <p>Number of training epochs to run during reinforcement fine-tuning. Higher values may improve performance but increase training time.</p>
    pub epoch_count: ::std::option::Option<i32>,
    /// <p>Number of training samples processed in each batch during reinforcement fine-tuning (RFT) training. Larger batches may improve training stability.</p>
    pub batch_size: ::std::option::Option<i32>,
    /// <p>Learning rate for the reinforcement fine-tuning. Controls how quickly the model adapts to reward signals.</p>
    pub learning_rate: ::std::option::Option<f32>,
    /// <p>Maximum length of input prompts during RFT training, measured in tokens. Longer prompts allow more context but increase memory usage and training-time.</p>
    pub max_prompt_length: ::std::option::Option<i32>,
    /// <p>Number of response samples generated per prompt during RFT training. More samples provide better reward signal estimation.</p>
    pub training_sample_per_prompt: ::std::option::Option<i32>,
    /// <p>Maximum number of tokens the model can generate in response to each prompt during RFT training.</p>
    pub inference_max_tokens: ::std::option::Option<i32>,
    /// <p>Level of reasoning effort applied during RFT training. Higher values may improve response quality but increase training time.</p>
    pub reasoning_effort: ::std::option::Option<crate::types::ReasoningEffort>,
    /// <p>Interval between evaluation runs during RFT training, measured in training steps. More frequent evaluation provides better monitoring.</p>
    pub eval_interval: ::std::option::Option<i32>,
}
impl RftHyperParameters {
    /// <p>Number of training epochs to run during reinforcement fine-tuning. Higher values may improve performance but increase training time.</p>
    pub fn epoch_count(&self) -> ::std::option::Option<i32> {
        self.epoch_count
    }
    /// <p>Number of training samples processed in each batch during reinforcement fine-tuning (RFT) training. Larger batches may improve training stability.</p>
    pub fn batch_size(&self) -> ::std::option::Option<i32> {
        self.batch_size
    }
    /// <p>Learning rate for the reinforcement fine-tuning. Controls how quickly the model adapts to reward signals.</p>
    pub fn learning_rate(&self) -> ::std::option::Option<f32> {
        self.learning_rate
    }
    /// <p>Maximum length of input prompts during RFT training, measured in tokens. Longer prompts allow more context but increase memory usage and training-time.</p>
    pub fn max_prompt_length(&self) -> ::std::option::Option<i32> {
        self.max_prompt_length
    }
    /// <p>Number of response samples generated per prompt during RFT training. More samples provide better reward signal estimation.</p>
    pub fn training_sample_per_prompt(&self) -> ::std::option::Option<i32> {
        self.training_sample_per_prompt
    }
    /// <p>Maximum number of tokens the model can generate in response to each prompt during RFT training.</p>
    pub fn inference_max_tokens(&self) -> ::std::option::Option<i32> {
        self.inference_max_tokens
    }
    /// <p>Level of reasoning effort applied during RFT training. Higher values may improve response quality but increase training time.</p>
    pub fn reasoning_effort(&self) -> ::std::option::Option<&crate::types::ReasoningEffort> {
        self.reasoning_effort.as_ref()
    }
    /// <p>Interval between evaluation runs during RFT training, measured in training steps. More frequent evaluation provides better monitoring.</p>
    pub fn eval_interval(&self) -> ::std::option::Option<i32> {
        self.eval_interval
    }
}
impl RftHyperParameters {
    /// Creates a new builder-style object to manufacture [`RftHyperParameters`](crate::types::RftHyperParameters).
    pub fn builder() -> crate::types::builders::RftHyperParametersBuilder {
        crate::types::builders::RftHyperParametersBuilder::default()
    }
}

/// A builder for [`RftHyperParameters`](crate::types::RftHyperParameters).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct RftHyperParametersBuilder {
    pub(crate) epoch_count: ::std::option::Option<i32>,
    pub(crate) batch_size: ::std::option::Option<i32>,
    pub(crate) learning_rate: ::std::option::Option<f32>,
    pub(crate) max_prompt_length: ::std::option::Option<i32>,
    pub(crate) training_sample_per_prompt: ::std::option::Option<i32>,
    pub(crate) inference_max_tokens: ::std::option::Option<i32>,
    pub(crate) reasoning_effort: ::std::option::Option<crate::types::ReasoningEffort>,
    pub(crate) eval_interval: ::std::option::Option<i32>,
}
impl RftHyperParametersBuilder {
    /// <p>Number of training epochs to run during reinforcement fine-tuning. Higher values may improve performance but increase training time.</p>
    pub fn epoch_count(mut self, input: i32) -> Self {
        self.epoch_count = ::std::option::Option::Some(input);
        self
    }
    /// <p>Number of training epochs to run during reinforcement fine-tuning. Higher values may improve performance but increase training time.</p>
    pub fn set_epoch_count(mut self, input: ::std::option::Option<i32>) -> Self {
        self.epoch_count = input;
        self
    }
    /// <p>Number of training epochs to run during reinforcement fine-tuning. Higher values may improve performance but increase training time.</p>
    pub fn get_epoch_count(&self) -> &::std::option::Option<i32> {
        &self.epoch_count
    }
    /// <p>Number of training samples processed in each batch during reinforcement fine-tuning (RFT) training. Larger batches may improve training stability.</p>
    pub fn batch_size(mut self, input: i32) -> Self {
        self.batch_size = ::std::option::Option::Some(input);
        self
    }
    /// <p>Number of training samples processed in each batch during reinforcement fine-tuning (RFT) training. Larger batches may improve training stability.</p>
    pub fn set_batch_size(mut self, input: ::std::option::Option<i32>) -> Self {
        self.batch_size = input;
        self
    }
    /// <p>Number of training samples processed in each batch during reinforcement fine-tuning (RFT) training. Larger batches may improve training stability.</p>
    pub fn get_batch_size(&self) -> &::std::option::Option<i32> {
        &self.batch_size
    }
    /// <p>Learning rate for the reinforcement fine-tuning. Controls how quickly the model adapts to reward signals.</p>
    pub fn learning_rate(mut self, input: f32) -> Self {
        self.learning_rate = ::std::option::Option::Some(input);
        self
    }
    /// <p>Learning rate for the reinforcement fine-tuning. Controls how quickly the model adapts to reward signals.</p>
    pub fn set_learning_rate(mut self, input: ::std::option::Option<f32>) -> Self {
        self.learning_rate = input;
        self
    }
    /// <p>Learning rate for the reinforcement fine-tuning. Controls how quickly the model adapts to reward signals.</p>
    pub fn get_learning_rate(&self) -> &::std::option::Option<f32> {
        &self.learning_rate
    }
    /// <p>Maximum length of input prompts during RFT training, measured in tokens. Longer prompts allow more context but increase memory usage and training-time.</p>
    pub fn max_prompt_length(mut self, input: i32) -> Self {
        self.max_prompt_length = ::std::option::Option::Some(input);
        self
    }
    /// <p>Maximum length of input prompts during RFT training, measured in tokens. Longer prompts allow more context but increase memory usage and training-time.</p>
    pub fn set_max_prompt_length(mut self, input: ::std::option::Option<i32>) -> Self {
        self.max_prompt_length = input;
        self
    }
    /// <p>Maximum length of input prompts during RFT training, measured in tokens. Longer prompts allow more context but increase memory usage and training-time.</p>
    pub fn get_max_prompt_length(&self) -> &::std::option::Option<i32> {
        &self.max_prompt_length
    }
    /// <p>Number of response samples generated per prompt during RFT training. More samples provide better reward signal estimation.</p>
    pub fn training_sample_per_prompt(mut self, input: i32) -> Self {
        self.training_sample_per_prompt = ::std::option::Option::Some(input);
        self
    }
    /// <p>Number of response samples generated per prompt during RFT training. More samples provide better reward signal estimation.</p>
    pub fn set_training_sample_per_prompt(mut self, input: ::std::option::Option<i32>) -> Self {
        self.training_sample_per_prompt = input;
        self
    }
    /// <p>Number of response samples generated per prompt during RFT training. More samples provide better reward signal estimation.</p>
    pub fn get_training_sample_per_prompt(&self) -> &::std::option::Option<i32> {
        &self.training_sample_per_prompt
    }
    /// <p>Maximum number of tokens the model can generate in response to each prompt during RFT training.</p>
    pub fn inference_max_tokens(mut self, input: i32) -> Self {
        self.inference_max_tokens = ::std::option::Option::Some(input);
        self
    }
    /// <p>Maximum number of tokens the model can generate in response to each prompt during RFT training.</p>
    pub fn set_inference_max_tokens(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inference_max_tokens = input;
        self
    }
    /// <p>Maximum number of tokens the model can generate in response to each prompt during RFT training.</p>
    pub fn get_inference_max_tokens(&self) -> &::std::option::Option<i32> {
        &self.inference_max_tokens
    }
    /// <p>Level of reasoning effort applied during RFT training. Higher values may improve response quality but increase training time.</p>
    pub fn reasoning_effort(mut self, input: crate::types::ReasoningEffort) -> Self {
        self.reasoning_effort = ::std::option::Option::Some(input);
        self
    }
    /// <p>Level of reasoning effort applied during RFT training. Higher values may improve response quality but increase training time.</p>
    pub fn set_reasoning_effort(mut self, input: ::std::option::Option<crate::types::ReasoningEffort>) -> Self {
        self.reasoning_effort = input;
        self
    }
    /// <p>Level of reasoning effort applied during RFT training. Higher values may improve response quality but increase training time.</p>
    pub fn get_reasoning_effort(&self) -> &::std::option::Option<crate::types::ReasoningEffort> {
        &self.reasoning_effort
    }
    /// <p>Interval between evaluation runs during RFT training, measured in training steps. More frequent evaluation provides better monitoring.</p>
    pub fn eval_interval(mut self, input: i32) -> Self {
        self.eval_interval = ::std::option::Option::Some(input);
        self
    }
    /// <p>Interval between evaluation runs during RFT training, measured in training steps. More frequent evaluation provides better monitoring.</p>
    pub fn set_eval_interval(mut self, input: ::std::option::Option<i32>) -> Self {
        self.eval_interval = input;
        self
    }
    /// <p>Interval between evaluation runs during RFT training, measured in training steps. More frequent evaluation provides better monitoring.</p>
    pub fn get_eval_interval(&self) -> &::std::option::Option<i32> {
        &self.eval_interval
    }
    /// Consumes the builder and constructs a [`RftHyperParameters`](crate::types::RftHyperParameters).
    pub fn build(self) -> crate::types::RftHyperParameters {
        crate::types::RftHyperParameters {
            epoch_count: self.epoch_count,
            batch_size: self.batch_size,
            learning_rate: self.learning_rate,
            max_prompt_length: self.max_prompt_length,
            training_sample_per_prompt: self.training_sample_per_prompt,
            inference_max_tokens: self.inference_max_tokens,
            reasoning_effort: self.reasoning_effort,
            eval_interval: self.eval_interval,
        }
    }
}
