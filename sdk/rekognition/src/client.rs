// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[derive(std::fmt::Debug)]
pub(crate) struct Handle<C = aws_hyper::DynConnector> {
    client: aws_hyper::Client<C>,
    conf: crate::Config,
}

#[derive(Clone, std::fmt::Debug)]
pub struct Client<C = aws_hyper::DynConnector> {
    handle: std::sync::Arc<Handle<C>>,
}
impl<C> Client<C> {
    pub fn from_conf_conn(conf: crate::Config, conn: C) -> Self {
        let client = aws_hyper::Client::new(conn);
        Self {
            handle: std::sync::Arc::new(Handle { client, conf }),
        }
    }

    pub fn conf(&self) -> &crate::Config {
        &self.handle.conf
    }
}
impl Client {
    #[cfg(any(feature = "rustls", feature = "native-tls"))]
    pub fn from_env() -> Self {
        Self::from_conf(crate::Config::builder().build())
    }

    #[cfg(any(feature = "rustls", feature = "native-tls"))]
    pub fn from_conf(conf: crate::Config) -> Self {
        let client = aws_hyper::Client::https();
        Self {
            handle: std::sync::Arc::new(Handle { client, conf }),
        }
    }
}
impl<C> Client<C>
where
    C: aws_hyper::SmithyConnector,
{
    pub fn compare_faces(&self) -> fluent_builders::CompareFaces<C> {
        fluent_builders::CompareFaces::new(self.handle.clone())
    }
    pub fn create_collection(&self) -> fluent_builders::CreateCollection<C> {
        fluent_builders::CreateCollection::new(self.handle.clone())
    }
    pub fn create_project(&self) -> fluent_builders::CreateProject<C> {
        fluent_builders::CreateProject::new(self.handle.clone())
    }
    pub fn create_project_version(&self) -> fluent_builders::CreateProjectVersion<C> {
        fluent_builders::CreateProjectVersion::new(self.handle.clone())
    }
    pub fn create_stream_processor(&self) -> fluent_builders::CreateStreamProcessor<C> {
        fluent_builders::CreateStreamProcessor::new(self.handle.clone())
    }
    pub fn delete_collection(&self) -> fluent_builders::DeleteCollection<C> {
        fluent_builders::DeleteCollection::new(self.handle.clone())
    }
    pub fn delete_faces(&self) -> fluent_builders::DeleteFaces<C> {
        fluent_builders::DeleteFaces::new(self.handle.clone())
    }
    pub fn delete_project(&self) -> fluent_builders::DeleteProject<C> {
        fluent_builders::DeleteProject::new(self.handle.clone())
    }
    pub fn delete_project_version(&self) -> fluent_builders::DeleteProjectVersion<C> {
        fluent_builders::DeleteProjectVersion::new(self.handle.clone())
    }
    pub fn delete_stream_processor(&self) -> fluent_builders::DeleteStreamProcessor<C> {
        fluent_builders::DeleteStreamProcessor::new(self.handle.clone())
    }
    pub fn describe_collection(&self) -> fluent_builders::DescribeCollection<C> {
        fluent_builders::DescribeCollection::new(self.handle.clone())
    }
    pub fn describe_projects(&self) -> fluent_builders::DescribeProjects<C> {
        fluent_builders::DescribeProjects::new(self.handle.clone())
    }
    pub fn describe_project_versions(&self) -> fluent_builders::DescribeProjectVersions<C> {
        fluent_builders::DescribeProjectVersions::new(self.handle.clone())
    }
    pub fn describe_stream_processor(&self) -> fluent_builders::DescribeStreamProcessor<C> {
        fluent_builders::DescribeStreamProcessor::new(self.handle.clone())
    }
    pub fn detect_custom_labels(&self) -> fluent_builders::DetectCustomLabels<C> {
        fluent_builders::DetectCustomLabels::new(self.handle.clone())
    }
    pub fn detect_faces(&self) -> fluent_builders::DetectFaces<C> {
        fluent_builders::DetectFaces::new(self.handle.clone())
    }
    pub fn detect_labels(&self) -> fluent_builders::DetectLabels<C> {
        fluent_builders::DetectLabels::new(self.handle.clone())
    }
    pub fn detect_moderation_labels(&self) -> fluent_builders::DetectModerationLabels<C> {
        fluent_builders::DetectModerationLabels::new(self.handle.clone())
    }
    pub fn detect_protective_equipment(&self) -> fluent_builders::DetectProtectiveEquipment<C> {
        fluent_builders::DetectProtectiveEquipment::new(self.handle.clone())
    }
    pub fn detect_text(&self) -> fluent_builders::DetectText<C> {
        fluent_builders::DetectText::new(self.handle.clone())
    }
    pub fn get_celebrity_info(&self) -> fluent_builders::GetCelebrityInfo<C> {
        fluent_builders::GetCelebrityInfo::new(self.handle.clone())
    }
    pub fn get_celebrity_recognition(&self) -> fluent_builders::GetCelebrityRecognition<C> {
        fluent_builders::GetCelebrityRecognition::new(self.handle.clone())
    }
    pub fn get_content_moderation(&self) -> fluent_builders::GetContentModeration<C> {
        fluent_builders::GetContentModeration::new(self.handle.clone())
    }
    pub fn get_face_detection(&self) -> fluent_builders::GetFaceDetection<C> {
        fluent_builders::GetFaceDetection::new(self.handle.clone())
    }
    pub fn get_face_search(&self) -> fluent_builders::GetFaceSearch<C> {
        fluent_builders::GetFaceSearch::new(self.handle.clone())
    }
    pub fn get_label_detection(&self) -> fluent_builders::GetLabelDetection<C> {
        fluent_builders::GetLabelDetection::new(self.handle.clone())
    }
    pub fn get_person_tracking(&self) -> fluent_builders::GetPersonTracking<C> {
        fluent_builders::GetPersonTracking::new(self.handle.clone())
    }
    pub fn get_segment_detection(&self) -> fluent_builders::GetSegmentDetection<C> {
        fluent_builders::GetSegmentDetection::new(self.handle.clone())
    }
    pub fn get_text_detection(&self) -> fluent_builders::GetTextDetection<C> {
        fluent_builders::GetTextDetection::new(self.handle.clone())
    }
    pub fn index_faces(&self) -> fluent_builders::IndexFaces<C> {
        fluent_builders::IndexFaces::new(self.handle.clone())
    }
    pub fn list_collections(&self) -> fluent_builders::ListCollections<C> {
        fluent_builders::ListCollections::new(self.handle.clone())
    }
    pub fn list_faces(&self) -> fluent_builders::ListFaces<C> {
        fluent_builders::ListFaces::new(self.handle.clone())
    }
    pub fn list_stream_processors(&self) -> fluent_builders::ListStreamProcessors<C> {
        fluent_builders::ListStreamProcessors::new(self.handle.clone())
    }
    pub fn list_tags_for_resource(&self) -> fluent_builders::ListTagsForResource<C> {
        fluent_builders::ListTagsForResource::new(self.handle.clone())
    }
    pub fn recognize_celebrities(&self) -> fluent_builders::RecognizeCelebrities<C> {
        fluent_builders::RecognizeCelebrities::new(self.handle.clone())
    }
    pub fn search_faces(&self) -> fluent_builders::SearchFaces<C> {
        fluent_builders::SearchFaces::new(self.handle.clone())
    }
    pub fn search_faces_by_image(&self) -> fluent_builders::SearchFacesByImage<C> {
        fluent_builders::SearchFacesByImage::new(self.handle.clone())
    }
    pub fn start_celebrity_recognition(&self) -> fluent_builders::StartCelebrityRecognition<C> {
        fluent_builders::StartCelebrityRecognition::new(self.handle.clone())
    }
    pub fn start_content_moderation(&self) -> fluent_builders::StartContentModeration<C> {
        fluent_builders::StartContentModeration::new(self.handle.clone())
    }
    pub fn start_face_detection(&self) -> fluent_builders::StartFaceDetection<C> {
        fluent_builders::StartFaceDetection::new(self.handle.clone())
    }
    pub fn start_face_search(&self) -> fluent_builders::StartFaceSearch<C> {
        fluent_builders::StartFaceSearch::new(self.handle.clone())
    }
    pub fn start_label_detection(&self) -> fluent_builders::StartLabelDetection<C> {
        fluent_builders::StartLabelDetection::new(self.handle.clone())
    }
    pub fn start_person_tracking(&self) -> fluent_builders::StartPersonTracking<C> {
        fluent_builders::StartPersonTracking::new(self.handle.clone())
    }
    pub fn start_project_version(&self) -> fluent_builders::StartProjectVersion<C> {
        fluent_builders::StartProjectVersion::new(self.handle.clone())
    }
    pub fn start_segment_detection(&self) -> fluent_builders::StartSegmentDetection<C> {
        fluent_builders::StartSegmentDetection::new(self.handle.clone())
    }
    pub fn start_stream_processor(&self) -> fluent_builders::StartStreamProcessor<C> {
        fluent_builders::StartStreamProcessor::new(self.handle.clone())
    }
    pub fn start_text_detection(&self) -> fluent_builders::StartTextDetection<C> {
        fluent_builders::StartTextDetection::new(self.handle.clone())
    }
    pub fn stop_project_version(&self) -> fluent_builders::StopProjectVersion<C> {
        fluent_builders::StopProjectVersion::new(self.handle.clone())
    }
    pub fn stop_stream_processor(&self) -> fluent_builders::StopStreamProcessor<C> {
        fluent_builders::StopStreamProcessor::new(self.handle.clone())
    }
    pub fn tag_resource(&self) -> fluent_builders::TagResource<C> {
        fluent_builders::TagResource::new(self.handle.clone())
    }
    pub fn untag_resource(&self) -> fluent_builders::UntagResource<C> {
        fluent_builders::UntagResource::new(self.handle.clone())
    }
}
pub mod fluent_builders {
    #[derive(std::fmt::Debug)]
    pub struct CompareFaces<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::compare_faces_input::Builder,
    }
    impl<C> CompareFaces<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::CompareFacesOutput,
            smithy_http::result::SdkError<crate::error::CompareFacesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The input image as base64-encoded bytes or an S3 object.
        /// If you use the AWS CLI to call Amazon Rekognition operations,
        /// passing base64-encoded image bytes is not supported. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn source_image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.source_image(input);
            self
        }
        pub fn set_source_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_source_image(input);
            self
        }
        /// <p>The target image as base64-encoded bytes or an S3 object. If you use the AWS CLI to
        /// call Amazon Rekognition operations, passing base64-encoded image bytes is not supported.
        /// </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn target_image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.target_image(input);
            self
        }
        pub fn set_target_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_target_image(input);
            self
        }
        /// <p>The minimum level of confidence in the face matches that a match must meet to be
        /// included in the <code>FaceMatches</code> array.</p>
        pub fn similarity_threshold(mut self, input: f32) -> Self {
            self.inner = self.inner.similarity_threshold(input);
            self
        }
        pub fn set_similarity_threshold(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_similarity_threshold(input);
            self
        }
        /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces.
        /// Filtered faces aren't compared. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar.
        /// If you specify <code>LOW</code>,
        /// <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that
        /// donâ€™t meet the chosen quality bar.
        /// The quality bar is based on a variety of common use cases. Low-quality
        /// detections can occur for a number of reasons. Some examples are an object that's misidentified
        /// as a face, a face that's too blurry, or a face with a
        /// pose that's too extreme to use. If you specify <code>NONE</code>, no
        /// filtering is performed. The default value is <code>NONE</code>.
        /// </p>
        /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
        pub fn quality_filter(mut self, input: crate::model::QualityFilter) -> Self {
            self.inner = self.inner.quality_filter(input);
            self
        }
        pub fn set_quality_filter(
            mut self,
            input: std::option::Option<crate::model::QualityFilter>,
        ) -> Self {
            self.inner = self.inner.set_quality_filter(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct CreateCollection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::create_collection_input::Builder,
    }
    impl<C> CreateCollection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::CreateCollectionOutput,
            smithy_http::result::SdkError<crate::error::CreateCollectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>ID for the collection that you are creating.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>
        /// A set of tags (key-value pairs) that you want to attach to the collection.
        /// </p>
        pub fn tags(
            mut self,
            k: impl Into<std::string::String>,
            v: impl Into<std::string::String>,
        ) -> Self {
            self.inner = self.inner.tags(k, v);
            self
        }
        pub fn set_tags(
            mut self,
            input: std::option::Option<
                std::collections::HashMap<std::string::String, std::string::String>,
            >,
        ) -> Self {
            self.inner = self.inner.set_tags(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct CreateProject<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::create_project_input::Builder,
    }
    impl<C> CreateProject<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::CreateProjectOutput,
            smithy_http::result::SdkError<crate::error::CreateProjectError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The name of the project to create.</p>
        pub fn project_name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_name(input);
            self
        }
        pub fn set_project_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_project_name(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct CreateProjectVersion<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::create_project_version_input::Builder,
    }
    impl<C> CreateProjectVersion<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::CreateProjectVersionOutput,
            smithy_http::result::SdkError<crate::error::CreateProjectVersionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The ARN of the Amazon Rekognition Custom Labels project that
        /// manages the model that you want to train.</p>
        pub fn project_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_arn(input);
            self
        }
        pub fn set_project_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_project_arn(input);
            self
        }
        /// <p>A name for the version of the model. This value must be unique.</p>
        pub fn version_name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.version_name(input);
            self
        }
        pub fn set_version_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_version_name(input);
            self
        }
        /// <p>The Amazon S3 location to store the results of training.</p>
        pub fn output_config(mut self, input: crate::model::OutputConfig) -> Self {
            self.inner = self.inner.output_config(input);
            self
        }
        pub fn set_output_config(
            mut self,
            input: std::option::Option<crate::model::OutputConfig>,
        ) -> Self {
            self.inner = self.inner.set_output_config(input);
            self
        }
        /// <p>The dataset to use for training. </p>
        pub fn training_data(mut self, input: crate::model::TrainingData) -> Self {
            self.inner = self.inner.training_data(input);
            self
        }
        pub fn set_training_data(
            mut self,
            input: std::option::Option<crate::model::TrainingData>,
        ) -> Self {
            self.inner = self.inner.set_training_data(input);
            self
        }
        /// <p>The dataset to use for testing.</p>
        pub fn testing_data(mut self, input: crate::model::TestingData) -> Self {
            self.inner = self.inner.testing_data(input);
            self
        }
        pub fn set_testing_data(
            mut self,
            input: std::option::Option<crate::model::TestingData>,
        ) -> Self {
            self.inner = self.inner.set_testing_data(input);
            self
        }
        /// <p>
        /// A set of tags (key-value pairs) that you want to attach to the model.
        /// </p>
        pub fn tags(
            mut self,
            k: impl Into<std::string::String>,
            v: impl Into<std::string::String>,
        ) -> Self {
            self.inner = self.inner.tags(k, v);
            self
        }
        pub fn set_tags(
            mut self,
            input: std::option::Option<
                std::collections::HashMap<std::string::String, std::string::String>,
            >,
        ) -> Self {
            self.inner = self.inner.set_tags(input);
            self
        }
        /// <p>The identifier for your AWS Key Management Service (AWS KMS) customer master key (CMK).
        /// You can supply the Amazon Resource Name (ARN) of your CMK, the ID of your CMK,
        /// or an alias for your CMK.
        /// The key is used to encrypt training and test images copied into the service for model training. Your
        /// source images are unaffected. The key is also used to encrypt training results and manifest files written
        /// to the output Amazon S3 bucket (<code>OutputConfig</code>).</p>
        /// <p>If you don't specify a value for <code>KmsKeyId</code>, images copied into the service are encrypted
        /// using a key that AWS owns and manages.</p>
        pub fn kms_key_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.kms_key_id(input);
            self
        }
        pub fn set_kms_key_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_kms_key_id(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct CreateStreamProcessor<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::create_stream_processor_input::Builder,
    }
    impl<C> CreateStreamProcessor<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::CreateStreamProcessorOutput,
            smithy_http::result::SdkError<crate::error::CreateStreamProcessorError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Kinesis video stream stream that provides the source streaming video. If you are using the AWS CLI, the parameter name is <code>StreamProcessorInput</code>.</p>
        pub fn input(mut self, input: crate::model::StreamProcessorInput) -> Self {
            self.inner = self.inner.input(input);
            self
        }
        pub fn set_input(
            mut self,
            input: std::option::Option<crate::model::StreamProcessorInput>,
        ) -> Self {
            self.inner = self.inner.set_input(input);
            self
        }
        /// <p>Kinesis data stream stream to which Amazon Rekognition Video puts the analysis results. If you are using the AWS CLI, the parameter name is <code>StreamProcessorOutput</code>.</p>
        pub fn output(mut self, input: crate::model::StreamProcessorOutput) -> Self {
            self.inner = self.inner.output(input);
            self
        }
        pub fn set_output(
            mut self,
            input: std::option::Option<crate::model::StreamProcessorOutput>,
        ) -> Self {
            self.inner = self.inner.set_output(input);
            self
        }
        /// <p>An identifier you assign to the stream processor. You can use <code>Name</code> to
        /// manage the stream processor. For example, you can get the current status of the stream processor by calling <a>DescribeStreamProcessor</a>.
        /// <code>Name</code> is idempotent.
        /// </p>
        pub fn name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.name(input);
            self
        }
        pub fn set_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_name(input);
            self
        }
        /// <p>Face recognition input parameters to be used by the stream processor. Includes the collection to use for face recognition and the face
        /// attributes to detect.</p>
        pub fn settings(mut self, input: crate::model::StreamProcessorSettings) -> Self {
            self.inner = self.inner.settings(input);
            self
        }
        pub fn set_settings(
            mut self,
            input: std::option::Option<crate::model::StreamProcessorSettings>,
        ) -> Self {
            self.inner = self.inner.set_settings(input);
            self
        }
        /// <p>ARN of the IAM role that allows access to the stream processor.</p>
        pub fn role_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.role_arn(input);
            self
        }
        pub fn set_role_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_role_arn(input);
            self
        }
        /// <p>
        /// A set of tags (key-value pairs) that you want to attach to the stream processor.
        /// </p>
        pub fn tags(
            mut self,
            k: impl Into<std::string::String>,
            v: impl Into<std::string::String>,
        ) -> Self {
            self.inner = self.inner.tags(k, v);
            self
        }
        pub fn set_tags(
            mut self,
            input: std::option::Option<
                std::collections::HashMap<std::string::String, std::string::String>,
            >,
        ) -> Self {
            self.inner = self.inner.set_tags(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DeleteCollection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::delete_collection_input::Builder,
    }
    impl<C> DeleteCollection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DeleteCollectionOutput,
            smithy_http::result::SdkError<crate::error::DeleteCollectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>ID of the collection to delete.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DeleteFaces<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::delete_faces_input::Builder,
    }
    impl<C> DeleteFaces<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DeleteFacesOutput,
            smithy_http::result::SdkError<crate::error::DeleteFacesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Collection from which to remove the specific faces.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>An array of face IDs to delete.</p>
        pub fn face_ids(mut self, inp: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.face_ids(inp);
            self
        }
        pub fn set_face_ids(
            mut self,
            input: std::option::Option<std::vec::Vec<std::string::String>>,
        ) -> Self {
            self.inner = self.inner.set_face_ids(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DeleteProject<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::delete_project_input::Builder,
    }
    impl<C> DeleteProject<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DeleteProjectOutput,
            smithy_http::result::SdkError<crate::error::DeleteProjectError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The Amazon Resource Name (ARN) of the project that you want to delete.</p>
        pub fn project_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_arn(input);
            self
        }
        pub fn set_project_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_project_arn(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DeleteProjectVersion<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::delete_project_version_input::Builder,
    }
    impl<C> DeleteProjectVersion<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DeleteProjectVersionOutput,
            smithy_http::result::SdkError<crate::error::DeleteProjectVersionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The Amazon Resource Name (ARN) of the model version that you want to delete.</p>
        pub fn project_version_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_version_arn(input);
            self
        }
        pub fn set_project_version_arn(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_project_version_arn(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DeleteStreamProcessor<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::delete_stream_processor_input::Builder,
    }
    impl<C> DeleteStreamProcessor<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DeleteStreamProcessorOutput,
            smithy_http::result::SdkError<crate::error::DeleteStreamProcessorError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The name of the stream processor you want to delete.</p>
        pub fn name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.name(input);
            self
        }
        pub fn set_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_name(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DescribeCollection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::describe_collection_input::Builder,
    }
    impl<C> DescribeCollection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DescribeCollectionOutput,
            smithy_http::result::SdkError<crate::error::DescribeCollectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The ID of the collection to describe.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DescribeProjects<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::describe_projects_input::Builder,
    }
    impl<C> DescribeProjects<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DescribeProjectsOutput,
            smithy_http::result::SdkError<crate::error::DescribeProjectsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>If the previous response was incomplete (because there is more
        /// results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. You can use this pagination
        /// token to retrieve the next set of results. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>The maximum number of results to return per paginated call. The largest value you can specify is 100.
        /// If you specify a value greater than 100, a ValidationException
        /// error occurs. The default value is 100. </p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DescribeProjectVersions<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::describe_project_versions_input::Builder,
    }
    impl<C> DescribeProjectVersions<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DescribeProjectVersionsOutput,
            smithy_http::result::SdkError<crate::error::DescribeProjectVersionsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The Amazon Resource Name (ARN) of the project that contains the models you want to describe.</p>
        pub fn project_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_arn(input);
            self
        }
        pub fn set_project_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_project_arn(input);
            self
        }
        /// <p>A list of model version names that you want to describe. You can add up to 10 model version names
        /// to the list. If you don't specify a value, all model descriptions are returned.  A version name is part of a
        /// model (ProjectVersion) ARN. For example, <code>my-model.2020-01-21T09.10.15</code> is the version name in the following ARN.
        /// <code>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/<i>my-model.2020-01-21T09.10.15</i>/1234567890123</code>.</p>
        pub fn version_names(mut self, inp: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.version_names(inp);
            self
        }
        pub fn set_version_names(
            mut self,
            input: std::option::Option<std::vec::Vec<std::string::String>>,
        ) -> Self {
            self.inner = self.inner.set_version_names(input);
            self
        }
        /// <p>If the previous response was incomplete (because there is more
        /// results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.
        /// You can use this pagination token to retrieve the next set of results. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>The maximum number of results to return per paginated call.
        /// The largest value you can specify is 100. If you specify a value greater than 100, a ValidationException
        /// error occurs. The default value is 100. </p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DescribeStreamProcessor<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::describe_stream_processor_input::Builder,
    }
    impl<C> DescribeStreamProcessor<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DescribeStreamProcessorOutput,
            smithy_http::result::SdkError<crate::error::DescribeStreamProcessorError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Name of the stream processor for which you want information.</p>
        pub fn name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.name(input);
            self
        }
        pub fn set_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_name(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DetectCustomLabels<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::detect_custom_labels_input::Builder,
    }
    impl<C> DetectCustomLabels<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DetectCustomLabelsOutput,
            smithy_http::result::SdkError<crate::error::DetectCustomLabelsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The ARN of the model version that you want to use.</p>
        pub fn project_version_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_version_arn(input);
            self
        }
        pub fn set_project_version_arn(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_project_version_arn(input);
            self
        }
        /// <p>Provides the input image either as bytes or an S3 object.</p>
        /// <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code>
        /// property. For example, you would use the <code>Bytes</code> property to pass an image loaded
        /// from a local file system. Image bytes passed by using the <code>Bytes</code> property must be
        /// base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to
        /// call Amazon Rekognition API operations. </p>
        /// <p>For more information, see Analyzing an Image Loaded from a Local File System
        /// in the Amazon Rekognition Developer Guide.</p>
        /// <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the
        /// <code>S3Object</code> property. Images stored in an S3 bucket do not need to be
        /// base64-encoded.</p>
        /// <p>The region for the S3 bucket containing the S3 object must match the region you use for
        /// Amazon Rekognition operations.</p>
        /// <p>If you use the
        /// AWS
        /// CLI to call Amazon Rekognition operations, passing image bytes using the Bytes
        /// property is not supported. You must first upload the image to an Amazon S3 bucket and then
        /// call the operation using the S3Object property.</p>
        /// <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3
        /// object. For more information, see Resource Based Policies in the Amazon Rekognition Developer Guide.
        /// </p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>Maximum number of results you want the service to return in the response.
        /// The service returns the specified number of highest confidence labels ranked from highest confidence
        /// to lowest.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>Specifies the minimum confidence level for the labels to return.
        /// Amazon Rekognition doesn't return any labels with a confidence lower than this specified value. If you specify a
        /// value of 0, all labels are return, regardless of the default thresholds that the model version applies.</p>
        pub fn min_confidence(mut self, input: f32) -> Self {
            self.inner = self.inner.min_confidence(input);
            self
        }
        pub fn set_min_confidence(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_min_confidence(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DetectFaces<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::detect_faces_input::Builder,
    }
    impl<C> DetectFaces<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DetectFacesOutput,
            smithy_http::result::SdkError<crate::error::DetectFacesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call
        /// Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>An array of facial attributes you want to be returned. This can be the default list of
        /// attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if
        /// you specify <code>["DEFAULT"]</code>, the API returns the following subset of facial
        /// attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>,
        /// <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>["ALL"]</code>, all
        /// facial attributes are returned, but the operation takes longer to complete.</p>
        /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND
        /// operator to determine which attributes to return (in this case, all attributes). </p>
        pub fn attributes(mut self, inp: impl Into<crate::model::Attribute>) -> Self {
            self.inner = self.inner.attributes(inp);
            self
        }
        pub fn set_attributes(
            mut self,
            input: std::option::Option<std::vec::Vec<crate::model::Attribute>>,
        ) -> Self {
            self.inner = self.inner.set_attributes(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DetectLabels<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::detect_labels_input::Builder,
    }
    impl<C> DetectLabels<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DetectLabelsOutput,
            smithy_http::result::SdkError<crate::error::DetectLabelsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call
        /// Amazon Rekognition operations, passing image bytes is not supported. Images stored in an S3 Bucket do
        /// not need to be base64-encoded.</p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>Maximum number of labels you want the service to return in the response. The service
        /// returns the specified number of highest confidence labels. </p>
        pub fn max_labels(mut self, input: i32) -> Self {
            self.inner = self.inner.max_labels(input);
            self
        }
        pub fn set_max_labels(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_labels(input);
            self
        }
        /// <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't
        /// return any labels with confidence lower than this specified value.</p>
        /// <p>If <code>MinConfidence</code> is not specified, the operation returns labels with a
        /// confidence values greater than or equal to 55 percent.</p>
        pub fn min_confidence(mut self, input: f32) -> Self {
            self.inner = self.inner.min_confidence(input);
            self
        }
        pub fn set_min_confidence(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_min_confidence(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DetectModerationLabels<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::detect_moderation_labels_input::Builder,
    }
    impl<C> DetectModerationLabels<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DetectModerationLabelsOutput,
            smithy_http::result::SdkError<crate::error::DetectModerationLabelsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The input image as base64-encoded bytes or an S3 object.
        /// If you use the AWS CLI to call Amazon Rekognition operations,
        /// passing base64-encoded image bytes is not supported. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't
        /// return any labels with a confidence level lower than this specified value.</p>
        /// <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with
        /// confidence values greater than or equal to 50 percent.</p>
        pub fn min_confidence(mut self, input: f32) -> Self {
            self.inner = self.inner.min_confidence(input);
            self
        }
        pub fn set_min_confidence(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_min_confidence(input);
            self
        }
        /// <p>Sets up the configuration for human evaluation, including the FlowDefinition
        /// the image will be sent to.</p>
        pub fn human_loop_config(mut self, input: crate::model::HumanLoopConfig) -> Self {
            self.inner = self.inner.human_loop_config(input);
            self
        }
        pub fn set_human_loop_config(
            mut self,
            input: std::option::Option<crate::model::HumanLoopConfig>,
        ) -> Self {
            self.inner = self.inner.set_human_loop_config(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DetectProtectiveEquipment<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::detect_protective_equipment_input::Builder,
    }
    impl<C> DetectProtectiveEquipment<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DetectProtectiveEquipmentOutput,
            smithy_http::result::SdkError<crate::error::DetectProtectiveEquipmentError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The image in which you want to detect PPE on detected persons. The image can be passed as image bytes or you can
        /// reference an image stored in an Amazon S3 bucket. </p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>An array of PPE types that you want to summarize.</p>
        pub fn summarization_attributes(
            mut self,
            input: crate::model::ProtectiveEquipmentSummarizationAttributes,
        ) -> Self {
            self.inner = self.inner.summarization_attributes(input);
            self
        }
        pub fn set_summarization_attributes(
            mut self,
            input: std::option::Option<crate::model::ProtectiveEquipmentSummarizationAttributes>,
        ) -> Self {
            self.inner = self.inner.set_summarization_attributes(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct DetectText<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::detect_text_input::Builder,
    }
    impl<C> DetectText<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::DetectTextOutput,
            smithy_http::result::SdkError<crate::error::DetectTextError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The input image as base64-encoded bytes or an Amazon S3 object. If you use the AWS CLI
        /// to call Amazon Rekognition operations, you can't pass image bytes. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>Optional parameters that let you set the criteria that the text must meet to be included in your response.</p>
        pub fn filters(mut self, input: crate::model::DetectTextFilters) -> Self {
            self.inner = self.inner.filters(input);
            self
        }
        pub fn set_filters(
            mut self,
            input: std::option::Option<crate::model::DetectTextFilters>,
        ) -> Self {
            self.inner = self.inner.set_filters(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetCelebrityInfo<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_celebrity_info_input::Builder,
    }
    impl<C> GetCelebrityInfo<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetCelebrityInfoOutput,
            smithy_http::result::SdkError<crate::error::GetCelebrityInfoError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The ID for the celebrity. You get the celebrity ID from a call to the <a>RecognizeCelebrities</a> operation,
        /// which recognizes celebrities in an image. </p>
        pub fn id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.id(input);
            self
        }
        pub fn set_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_id(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetCelebrityRecognition<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_celebrity_recognition_input::Builder,
    }
    impl<C> GetCelebrityRecognition<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetCelebrityRecognitionOutput,
            smithy_http::result::SdkError<crate::error::GetCelebrityRecognitionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Job identifier for the required celebrity recognition analysis. You can get the job identifer from
        /// a call to <code>StartCelebrityRecognition</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.
        /// If you specify a value greater than 1000, a maximum of 1000 results is returned.
        /// The default value is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there is more recognized celebrities to retrieve), Amazon Rekognition Video returns a pagination
        /// token in the response. You can use this pagination token to retrieve the next set of celebrities. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Sort to use for celebrities returned in <code>Celebrities</code> field. Specify <code>ID</code> to sort by the celebrity identifier,
        /// specify <code>TIMESTAMP</code> to sort by the time the celebrity was recognized.</p>
        pub fn sort_by(mut self, input: crate::model::CelebrityRecognitionSortBy) -> Self {
            self.inner = self.inner.sort_by(input);
            self
        }
        pub fn set_sort_by(
            mut self,
            input: std::option::Option<crate::model::CelebrityRecognitionSortBy>,
        ) -> Self {
            self.inner = self.inner.set_sort_by(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetContentModeration<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_content_moderation_input::Builder,
    }
    impl<C> GetContentModeration<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetContentModerationOutput,
            smithy_http::result::SdkError<crate::error::GetContentModerationError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The identifier for the unsafe content job. Use <code>JobId</code> to identify the job in
        /// a subsequent call to <code>GetContentModeration</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.
        /// If you specify a value greater than 1000, a maximum of 1000 results is returned.
        /// The default value is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there is more data to retrieve), Amazon Rekognition
        /// returns a pagination token in the response. You can use this pagination token
        /// to retrieve the next set of unsafe content labels.</p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Sort to use for elements in the <code>ModerationLabelDetections</code> array.
        /// Use <code>TIMESTAMP</code> to sort array elements by the time labels are detected.
        /// Use <code>NAME</code> to alphabetically group elements for a label together.
        /// Within each label group, the array element are sorted by detection confidence.
        /// The default sort is by <code>TIMESTAMP</code>.</p>
        pub fn sort_by(mut self, input: crate::model::ContentModerationSortBy) -> Self {
            self.inner = self.inner.sort_by(input);
            self
        }
        pub fn set_sort_by(
            mut self,
            input: std::option::Option<crate::model::ContentModerationSortBy>,
        ) -> Self {
            self.inner = self.inner.set_sort_by(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetFaceDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_face_detection_input::Builder,
    }
    impl<C> GetFaceDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetFaceDetectionOutput,
            smithy_http::result::SdkError<crate::error::GetFaceDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Unique identifier for the face detection job. The <code>JobId</code> is returned from <code>StartFaceDetection</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.
        /// If you specify a value greater than 1000, a maximum of 1000 results is returned.
        /// The default value is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there are more faces to retrieve), Amazon Rekognition Video returns a pagination
        /// token in the response. You can use this pagination token to retrieve the next set of faces.</p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetFaceSearch<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_face_search_input::Builder,
    }
    impl<C> GetFaceSearch<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetFaceSearchOutput,
            smithy_http::result::SdkError<crate::error::GetFaceSearchError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The job identifer for the search request. You get the job identifier from an initial call to <code>StartFaceSearch</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.
        /// If you specify a value greater than 1000, a maximum of 1000 results is returned.
        /// The default value is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there is more search results to retrieve), Amazon Rekognition Video returns a pagination
        /// token in the response. You can use this pagination token to retrieve the next set of search results. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Sort to use for grouping faces in the response. Use <code>TIMESTAMP</code> to group faces by the time
        /// that they are recognized. Use <code>INDEX</code> to sort by recognized faces. </p>
        pub fn sort_by(mut self, input: crate::model::FaceSearchSortBy) -> Self {
            self.inner = self.inner.sort_by(input);
            self
        }
        pub fn set_sort_by(
            mut self,
            input: std::option::Option<crate::model::FaceSearchSortBy>,
        ) -> Self {
            self.inner = self.inner.set_sort_by(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetLabelDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_label_detection_input::Builder,
    }
    impl<C> GetLabelDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetLabelDetectionOutput,
            smithy_http::result::SdkError<crate::error::GetLabelDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Job identifier for the label detection operation for which you want results returned. You get the job identifer from
        /// an initial call to <code>StartlabelDetection</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.
        /// If you specify a value greater than 1000, a maximum of 1000 results is returned.
        /// The default value is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination
        /// token in the response. You can use this pagination token to retrieve the next set of labels. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Sort to use for elements in the <code>Labels</code> array.
        /// Use <code>TIMESTAMP</code> to sort array elements by the time labels are detected.
        /// Use <code>NAME</code> to alphabetically group elements for a label together.
        /// Within each label group, the array element are sorted by detection confidence.
        /// The default sort is by <code>TIMESTAMP</code>.</p>
        pub fn sort_by(mut self, input: crate::model::LabelDetectionSortBy) -> Self {
            self.inner = self.inner.sort_by(input);
            self
        }
        pub fn set_sort_by(
            mut self,
            input: std::option::Option<crate::model::LabelDetectionSortBy>,
        ) -> Self {
            self.inner = self.inner.set_sort_by(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetPersonTracking<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_person_tracking_input::Builder,
    }
    impl<C> GetPersonTracking<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetPersonTrackingOutput,
            smithy_http::result::SdkError<crate::error::GetPersonTrackingError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The identifier for a job that tracks persons in a video. You get the <code>JobId</code> from a call to <code>StartPersonTracking</code>.
        /// </p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.
        /// If you specify a value greater than 1000, a maximum of 1000 results is returned.
        /// The default value is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there are more persons to retrieve), Amazon Rekognition Video returns a pagination
        /// token in the response. You can use this pagination token to retrieve the next set of persons. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Sort to use for elements in the <code>Persons</code> array. Use <code>TIMESTAMP</code> to sort array elements
        /// by the time persons are detected. Use <code>INDEX</code> to sort by the tracked persons.
        /// If you sort by <code>INDEX</code>, the array elements for each person are sorted by detection confidence.
        /// The default sort is by <code>TIMESTAMP</code>.</p>
        pub fn sort_by(mut self, input: crate::model::PersonTrackingSortBy) -> Self {
            self.inner = self.inner.sort_by(input);
            self
        }
        pub fn set_sort_by(
            mut self,
            input: std::option::Option<crate::model::PersonTrackingSortBy>,
        ) -> Self {
            self.inner = self.inner.set_sort_by(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetSegmentDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_segment_detection_input::Builder,
    }
    impl<C> GetSegmentDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetSegmentDetectionOutput,
            smithy_http::result::SdkError<crate::error::GetSegmentDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Job identifier for the text detection operation for which you want results returned.
        /// You get the job identifer from an initial call to <code>StartSegmentDetection</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent
        /// request to retrieve the next set of text.</p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct GetTextDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::get_text_detection_input::Builder,
    }
    impl<C> GetTextDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::GetTextDetectionOutput,
            smithy_http::result::SdkError<crate::error::GetTextDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Job identifier for the text detection operation for which you want results returned.
        /// You get the job identifer from an initial call to <code>StartTextDetection</code>.</p>
        pub fn job_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_id(input);
            self
        }
        pub fn set_job_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_id(input);
            self
        }
        /// <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
        /// <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns
        /// a pagination token in the response. You can use this pagination token to retrieve the next set of text.</p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct IndexFaces<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::index_faces_input::Builder,
    }
    impl<C> IndexFaces<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::IndexFacesOutput,
            smithy_http::result::SdkError<crate::error::IndexFacesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The ID of an existing collection to which you want to add the faces that are detected
        /// in the input images.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call
        /// Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>The ID you want to assign to all the faces detected in the image.</p>
        pub fn external_image_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.external_image_id(input);
            self
        }
        pub fn set_external_image_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_external_image_id(input);
            self
        }
        /// <p>An array of facial attributes that you want to be returned. This can be the default
        /// list of attributes or all attributes. If you don't specify a value for <code>Attributes</code>
        /// or if you specify <code>["DEFAULT"]</code>, the API returns the following subset of facial
        /// attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>,
        /// <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>["ALL"]</code>, all
        /// facial attributes are returned, but the operation takes longer to complete.</p>
        /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND
        /// operator to determine which attributes to return (in this case, all attributes). </p>
        pub fn detection_attributes(mut self, inp: impl Into<crate::model::Attribute>) -> Self {
            self.inner = self.inner.detection_attributes(inp);
            self
        }
        pub fn set_detection_attributes(
            mut self,
            input: std::option::Option<std::vec::Vec<crate::model::Attribute>>,
        ) -> Self {
            self.inner = self.inner.set_detection_attributes(input);
            self
        }
        /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater
        /// than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an
        /// image, even if you specify a larger value for <code>MaxFaces</code>.</p>
        /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the
        /// faces with the lowest quality are filtered out first. If there are still more faces than the
        /// value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out
        /// (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information
        /// about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>
        /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face
        /// bounding box size to the smallest size, in descending order.</p>
        /// <p>
        /// <code>MaxFaces</code> can be used with a collection associated with any version of
        /// the face model.</p>
        pub fn max_faces(mut self, input: i32) -> Self {
            self.inner = self.inner.max_faces(input);
            self
        }
        pub fn set_max_faces(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_faces(input);
            self
        }
        /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces.
        /// Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar.
        /// If you specify <code>LOW</code>,
        /// <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that
        /// donâ€™t meet the chosen quality bar.  The default value is <code>AUTO</code>.
        /// The quality bar is based on a variety of common use cases. Low-quality
        /// detections can occur for a number of reasons. Some examples are an object that's misidentified
        /// as a face, a face that's too blurry, or a face with a
        /// pose that's too extreme to use. If you specify <code>NONE</code>, no
        /// filtering is performed.
        /// </p>
        /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
        pub fn quality_filter(mut self, input: crate::model::QualityFilter) -> Self {
            self.inner = self.inner.quality_filter(input);
            self
        }
        pub fn set_quality_filter(
            mut self,
            input: std::option::Option<crate::model::QualityFilter>,
        ) -> Self {
            self.inner = self.inner.set_quality_filter(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct ListCollections<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::list_collections_input::Builder,
    }
    impl<C> ListCollections<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::ListCollectionsOutput,
            smithy_http::result::SdkError<crate::error::ListCollectionsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Pagination token from the previous response.</p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Maximum number of collection IDs to return. </p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct ListFaces<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::list_faces_input::Builder,
    }
    impl<C> ListFaces<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::ListFacesOutput,
            smithy_http::result::SdkError<crate::error::ListFacesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>ID of the collection from which to list the faces.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>If the previous response was incomplete (because there is more data to retrieve),
        /// Amazon Rekognition returns a pagination token in the response. You can use this pagination token to
        /// retrieve the next set of faces.</p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Maximum number of faces to return.</p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct ListStreamProcessors<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::list_stream_processors_input::Builder,
    }
    impl<C> ListStreamProcessors<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::ListStreamProcessorsOutput,
            smithy_http::result::SdkError<crate::error::ListStreamProcessorsError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>If the previous response was incomplete (because there are more stream processors to retrieve), Amazon Rekognition Video
        /// returns a pagination token in the response. You can use this pagination token to retrieve the next set of stream processors. </p>
        pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.next_token(input);
            self
        }
        pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_next_token(input);
            self
        }
        /// <p>Maximum number of stream processors you want Amazon Rekognition Video to return in the response. The default is 1000. </p>
        pub fn max_results(mut self, input: i32) -> Self {
            self.inner = self.inner.max_results(input);
            self
        }
        pub fn set_max_results(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_results(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct ListTagsForResource<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::list_tags_for_resource_input::Builder,
    }
    impl<C> ListTagsForResource<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::ListTagsForResourceOutput,
            smithy_http::result::SdkError<crate::error::ListTagsForResourceError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>
        /// Amazon Resource Name (ARN) of the model, collection, or stream processor that contains the tags that you want a list of.
        /// </p>
        pub fn resource_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.resource_arn(input);
            self
        }
        pub fn set_resource_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_resource_arn(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct RecognizeCelebrities<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::recognize_celebrities_input::Builder,
    }
    impl<C> RecognizeCelebrities<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::RecognizeCelebritiesOutput,
            smithy_http::result::SdkError<crate::error::RecognizeCelebritiesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call
        /// Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct SearchFaces<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::search_faces_input::Builder,
    }
    impl<C> SearchFaces<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::SearchFacesOutput,
            smithy_http::result::SdkError<crate::error::SearchFacesError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>ID of the collection the face belongs to.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>ID of a face to find matches for in the collection.</p>
        pub fn face_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.face_id(input);
            self
        }
        pub fn set_face_id(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_face_id(input);
            self
        }
        /// <p>Maximum number of faces to return. The operation returns the maximum number of faces
        /// with the highest confidence in the match.</p>
        pub fn max_faces(mut self, input: i32) -> Self {
            self.inner = self.inner.max_faces(input);
            self
        }
        pub fn set_max_faces(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_faces(input);
            self
        }
        /// <p>Optional value specifying the minimum confidence in the face match to return. For
        /// example, don't return any matches where confidence in matches is less than 70%.
        /// The default value is 80%.
        /// </p>
        pub fn face_match_threshold(mut self, input: f32) -> Self {
            self.inner = self.inner.face_match_threshold(input);
            self
        }
        pub fn set_face_match_threshold(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_face_match_threshold(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct SearchFacesByImage<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::search_faces_by_image_input::Builder,
    }
    impl<C> SearchFacesByImage<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::SearchFacesByImageOutput,
            smithy_http::result::SdkError<crate::error::SearchFacesByImageError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>ID of the collection to search.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>The input image as base64-encoded bytes or an S3 object.
        /// If you use the AWS CLI to call Amazon Rekognition operations,
        /// passing base64-encoded image bytes is not supported. </p>
        /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes
        /// passed using the <code>Bytes</code> field.
        /// For more information, see Images in the Amazon Rekognition developer guide.</p>
        pub fn image(mut self, input: crate::model::Image) -> Self {
            self.inner = self.inner.image(input);
            self
        }
        pub fn set_image(mut self, input: std::option::Option<crate::model::Image>) -> Self {
            self.inner = self.inner.set_image(input);
            self
        }
        /// <p>Maximum number of faces to return. The operation returns the maximum number of faces
        /// with the highest confidence in the match.</p>
        pub fn max_faces(mut self, input: i32) -> Self {
            self.inner = self.inner.max_faces(input);
            self
        }
        pub fn set_max_faces(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_max_faces(input);
            self
        }
        /// <p>(Optional) Specifies the minimum confidence in the face match to return. For example,
        /// don't return any matches where confidence in matches is less than 70%.
        /// The default value is 80%.</p>
        pub fn face_match_threshold(mut self, input: f32) -> Self {
            self.inner = self.inner.face_match_threshold(input);
            self
        }
        pub fn set_face_match_threshold(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_face_match_threshold(input);
            self
        }
        /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces.
        /// Filtered faces aren't searched for in the collection. If you specify <code>AUTO</code>, Amazon Rekognition
        /// chooses the quality bar.  If you specify <code>LOW</code>,
        /// <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that
        /// donâ€™t meet the chosen quality bar.  
        /// The quality bar is based on a variety of common use cases. Low-quality
        /// detections can occur for a number of reasons. Some examples are an object that's misidentified
        /// as a face, a face that's too blurry, or a face with a
        /// pose that's too extreme to use. If you specify <code>NONE</code>, no
        /// filtering is performed.  The default value is <code>NONE</code>.
        /// </p>
        /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
        pub fn quality_filter(mut self, input: crate::model::QualityFilter) -> Self {
            self.inner = self.inner.quality_filter(input);
            self
        }
        pub fn set_quality_filter(
            mut self,
            input: std::option::Option<crate::model::QualityFilter>,
        ) -> Self {
            self.inner = self.inner.set_quality_filter(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartCelebrityRecognition<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_celebrity_recognition_input::Builder,
    }
    impl<C> StartCelebrityRecognition<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartCelebrityRecognitionOutput,
            smithy_http::result::SdkError<crate::error::StartCelebrityRecognitionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The video in which you want to recognize celebrities. The video must be stored
        /// in an Amazon S3 bucket.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartCelebrityRecognition</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the
        /// celebrity recognition analysis to.</p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartContentModeration<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_content_moderation_input::Builder,
    }
    impl<C> StartContentModeration<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartContentModerationOutput,
            smithy_http::result::SdkError<crate::error::StartContentModerationError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The video in which you want to detect unsafe content. The video must be stored
        /// in an Amazon S3 bucket.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Specifies the minimum confidence that Amazon Rekognition must have in order to return a moderated content label. Confidence
        /// represents how certain Amazon Rekognition is that the moderated content is correctly identified. 0 is the lowest confidence.
        /// 100 is the highest confidence.  Amazon Rekognition doesn't return any moderated content labels with a confidence level
        /// lower than this specified value. If you don't specify <code>MinConfidence</code>, <code>GetContentModeration</code>
        /// returns labels with confidence values greater than or equal to 50 percent.</p>
        pub fn min_confidence(mut self, input: f32) -> Self {
            self.inner = self.inner.min_confidence(input);
            self
        }
        pub fn set_min_confidence(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_min_confidence(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartContentModeration</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the
        /// unsafe content analysis to.</p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartFaceDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_face_detection_input::Builder,
    }
    impl<C> StartFaceDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartFaceDetectionOutput,
            smithy_http::result::SdkError<crate::error::StartFaceDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The video in which you want to detect faces. The video must be stored
        /// in an Amazon S3 bucket.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartFaceDetection</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the
        /// face detection operation.</p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>The face attributes you want returned.</p>
        /// <p>
        /// <code>DEFAULT</code> - The following subset of facial attributes are returned: BoundingBox, Confidence, Pose, Quality and Landmarks. </p>
        /// <p>
        /// <code>ALL</code> - All facial attributes are returned.</p>
        pub fn face_attributes(mut self, input: crate::model::FaceAttributes) -> Self {
            self.inner = self.inner.face_attributes(input);
            self
        }
        pub fn set_face_attributes(
            mut self,
            input: std::option::Option<crate::model::FaceAttributes>,
        ) -> Self {
            self.inner = self.inner.set_face_attributes(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartFaceSearch<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_face_search_input::Builder,
    }
    impl<C> StartFaceSearch<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartFaceSearchOutput,
            smithy_http::result::SdkError<crate::error::StartFaceSearchError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The video you want to search. The video must be stored in an Amazon S3 bucket. </p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartFaceSearch</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The minimum confidence in the person match to return. For example, don't return any matches where confidence in matches is less than 70%.
        /// The default value is 80%.</p>
        pub fn face_match_threshold(mut self, input: f32) -> Self {
            self.inner = self.inner.face_match_threshold(input);
            self
        }
        pub fn set_face_match_threshold(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_face_match_threshold(input);
            self
        }
        /// <p>ID of the collection that contains the faces you want to search for.</p>
        pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.collection_id(input);
            self
        }
        pub fn set_collection_id(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_collection_id(input);
            self
        }
        /// <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the search. </p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartLabelDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_label_detection_input::Builder,
    }
    impl<C> StartLabelDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartLabelDetectionOutput,
            smithy_http::result::SdkError<crate::error::StartLabelDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The video in which you want to detect labels. The video must be stored
        /// in an Amazon S3 bucket.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartLabelDetection</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected label. Confidence
        /// represents how certain Amazon Rekognition is that a label is correctly identified.0 is the lowest confidence.
        /// 100 is the highest confidence.  Amazon Rekognition Video doesn't return any labels with a confidence level
        /// lower than this specified value.</p>
        /// <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with confidence
        /// values greater than or equal to 50 percent.</p>
        pub fn min_confidence(mut self, input: f32) -> Self {
            self.inner = self.inner.min_confidence(input);
            self
        }
        pub fn set_min_confidence(mut self, input: std::option::Option<f32>) -> Self {
            self.inner = self.inner.set_min_confidence(input);
            self
        }
        /// <p>The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the label detection
        /// operation to. </p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartPersonTracking<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_person_tracking_input::Builder,
    }
    impl<C> StartPersonTracking<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartPersonTrackingOutput,
            smithy_http::result::SdkError<crate::error::StartPersonTrackingError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The video in which you want to detect people. The video must be stored
        /// in an Amazon S3 bucket.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartPersonTracking</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the people detection
        /// operation to.</p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartProjectVersion<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_project_version_input::Builder,
    }
    impl<C> StartProjectVersion<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartProjectVersionOutput,
            smithy_http::result::SdkError<crate::error::StartProjectVersionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The Amazon Resource Name(ARN) of the model version that you want to start.</p>
        pub fn project_version_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_version_arn(input);
            self
        }
        pub fn set_project_version_arn(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_project_version_arn(input);
            self
        }
        /// <p>The minimum number of inference units to use. A single
        /// inference unit represents 1 hour of processing and can support up to 5 Transaction Pers Second (TPS).
        /// Use a higher number to increase the TPS throughput of your model. You are charged for the number
        /// of inference units that you use.
        /// </p>
        pub fn min_inference_units(mut self, input: i32) -> Self {
            self.inner = self.inner.min_inference_units(input);
            self
        }
        pub fn set_min_inference_units(mut self, input: std::option::Option<i32>) -> Self {
            self.inner = self.inner.set_min_inference_units(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartSegmentDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_segment_detection_input::Builder,
    }
    impl<C> StartSegmentDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartSegmentDetectionOutput,
            smithy_http::result::SdkError<crate::error::StartSegmentDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to
        /// specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple
        /// <code>StartSegmentDetection</code> requests, the same <code>JobId</code> is returned. Use
        /// <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the
        /// segment detection operation.</p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.
        /// For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
        /// <p>Filters for technical cue or shot detection.</p>
        pub fn filters(mut self, input: crate::model::StartSegmentDetectionFilters) -> Self {
            self.inner = self.inner.filters(input);
            self
        }
        pub fn set_filters(
            mut self,
            input: std::option::Option<crate::model::StartSegmentDetectionFilters>,
        ) -> Self {
            self.inner = self.inner.set_filters(input);
            self
        }
        /// <p>An array of segment types to detect in the video. Valid values are TECHNICAL_CUE and SHOT.</p>
        pub fn segment_types(mut self, inp: impl Into<crate::model::SegmentType>) -> Self {
            self.inner = self.inner.segment_types(inp);
            self
        }
        pub fn set_segment_types(
            mut self,
            input: std::option::Option<std::vec::Vec<crate::model::SegmentType>>,
        ) -> Self {
            self.inner = self.inner.set_segment_types(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartStreamProcessor<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_stream_processor_input::Builder,
    }
    impl<C> StartStreamProcessor<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartStreamProcessorOutput,
            smithy_http::result::SdkError<crate::error::StartStreamProcessorError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The name of the stream processor to start processing.</p>
        pub fn name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.name(input);
            self
        }
        pub fn set_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_name(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StartTextDetection<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::start_text_detection_input::Builder,
    }
    impl<C> StartTextDetection<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StartTextDetectionOutput,
            smithy_http::result::SdkError<crate::error::StartTextDetectionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to
        /// specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>
        pub fn video(mut self, input: crate::model::Video) -> Self {
            self.inner = self.inner.video(input);
            self
        }
        pub fn set_video(mut self, input: std::option::Option<crate::model::Video>) -> Self {
            self.inner = self.inner.set_video(input);
            self
        }
        /// <p>Idempotent token used to identify the start request. If you use the same token with multiple <code>StartTextDetection</code>
        /// requests, the same <code>JobId</code> is returned. Use <code>ClientRequestToken</code> to prevent the same job
        /// from being accidentaly started more than once.</p>
        pub fn client_request_token(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.client_request_token(input);
            self
        }
        pub fn set_client_request_token(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_client_request_token(input);
            self
        }
        /// <p>The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. For more information, see
        /// <a>api-video</a>.</p>
        pub fn notification_channel(mut self, input: crate::model::NotificationChannel) -> Self {
            self.inner = self.inner.notification_channel(input);
            self
        }
        pub fn set_notification_channel(
            mut self,
            input: std::option::Option<crate::model::NotificationChannel>,
        ) -> Self {
            self.inner = self.inner.set_notification_channel(input);
            self
        }
        /// <p>An identifier returned in the completion status published by your Amazon Simple Notification Service topic.  For example, you can use <code>JobTag</code> to group related jobs
        /// and identify them in the completion notification.</p>
        pub fn job_tag(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.job_tag(input);
            self
        }
        pub fn set_job_tag(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_job_tag(input);
            self
        }
        /// <p>Optional parameters that let you set criteria the text must meet to be included in your response.</p>
        pub fn filters(mut self, input: crate::model::StartTextDetectionFilters) -> Self {
            self.inner = self.inner.filters(input);
            self
        }
        pub fn set_filters(
            mut self,
            input: std::option::Option<crate::model::StartTextDetectionFilters>,
        ) -> Self {
            self.inner = self.inner.set_filters(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StopProjectVersion<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::stop_project_version_input::Builder,
    }
    impl<C> StopProjectVersion<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StopProjectVersionOutput,
            smithy_http::result::SdkError<crate::error::StopProjectVersionError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The Amazon Resource Name (ARN) of the model version that you want to delete.</p>
        /// <p>This operation requires permissions to perform the <code>rekognition:StopProjectVersion</code> action.</p>
        pub fn project_version_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.project_version_arn(input);
            self
        }
        pub fn set_project_version_arn(
            mut self,
            input: std::option::Option<std::string::String>,
        ) -> Self {
            self.inner = self.inner.set_project_version_arn(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct StopStreamProcessor<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::stop_stream_processor_input::Builder,
    }
    impl<C> StopStreamProcessor<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::StopStreamProcessorOutput,
            smithy_http::result::SdkError<crate::error::StopStreamProcessorError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>The name of a stream processor created by <a>CreateStreamProcessor</a>.</p>
        pub fn name(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.name(input);
            self
        }
        pub fn set_name(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_name(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct TagResource<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::tag_resource_input::Builder,
    }
    impl<C> TagResource<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::TagResourceOutput,
            smithy_http::result::SdkError<crate::error::TagResourceError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>
        /// Amazon Resource Name (ARN) of the model, collection, or stream processor that you want to assign the tags to.
        /// </p>
        pub fn resource_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.resource_arn(input);
            self
        }
        pub fn set_resource_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_resource_arn(input);
            self
        }
        /// <p>
        /// The key-value tags to assign to the resource.
        /// </p>
        pub fn tags(
            mut self,
            k: impl Into<std::string::String>,
            v: impl Into<std::string::String>,
        ) -> Self {
            self.inner = self.inner.tags(k, v);
            self
        }
        pub fn set_tags(
            mut self,
            input: std::option::Option<
                std::collections::HashMap<std::string::String, std::string::String>,
            >,
        ) -> Self {
            self.inner = self.inner.set_tags(input);
            self
        }
    }
    #[derive(std::fmt::Debug)]
    pub struct UntagResource<C = aws_hyper::DynConnector> {
        handle: std::sync::Arc<super::Handle<C>>,
        inner: crate::input::untag_resource_input::Builder,
    }
    impl<C> UntagResource<C> {
        pub(crate) fn new(handle: std::sync::Arc<super::Handle<C>>) -> Self {
            Self {
                handle,
                inner: Default::default(),
            }
        }

        pub async fn send(
            self,
        ) -> std::result::Result<
            crate::output::UntagResourceOutput,
            smithy_http::result::SdkError<crate::error::UntagResourceError>,
        >
        where
            C: aws_hyper::SmithyConnector,
        {
            let input = self
                .inner
                .build()
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            let op = input
                .make_operation(&self.handle.conf)
                .map_err(|err| smithy_http::result::SdkError::ConstructionFailure(err.into()))?;
            self.handle.client.call(op).await
        }
        /// <p>
        /// Amazon Resource Name (ARN) of the model, collection, or stream processor that you want to remove the tags from.
        /// </p>
        pub fn resource_arn(mut self, input: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.resource_arn(input);
            self
        }
        pub fn set_resource_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
            self.inner = self.inner.set_resource_arn(input);
            self
        }
        /// <p>
        /// A list of the tags that you want to remove.
        /// </p>
        pub fn tag_keys(mut self, inp: impl Into<std::string::String>) -> Self {
            self.inner = self.inner.tag_keys(inp);
            self
        }
        pub fn set_tag_keys(
            mut self,
            input: std::option::Option<std::vec::Vec<std::string::String>>,
        ) -> Self {
            self.inner = self.inner.set_tag_keys(input);
            self
        }
    }
}
