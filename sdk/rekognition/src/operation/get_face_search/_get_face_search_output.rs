// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct GetFaceSearchOutput  {
    /// <p>The current status of the face search job.</p>
    #[doc(hidden)]
    pub job_status: std::option::Option<crate::types::VideoJobStatus>,
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    #[doc(hidden)]
    pub status_message: std::option::Option<std::string::String>,
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>
    #[doc(hidden)]
    pub next_token: std::option::Option<std::string::String>,
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition Video operation. </p>
    #[doc(hidden)]
    pub video_metadata: std::option::Option<crate::types::VideoMetadata>,
    /// <p>An array of persons, <code>PersonMatch</code>, in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to <code>StartFaceSearch</code>. Each <code>Persons</code> element includes a time the person was matched, face match details (<code>FaceMatches</code>) for matching faces in the collection, and person information (<code>Person</code>) for the matched person. </p>
    #[doc(hidden)]
    pub persons: std::option::Option<std::vec::Vec<crate::types::PersonMatch>>,
    _request_id: Option<String>,
}
impl GetFaceSearchOutput {
    /// <p>The current status of the face search job.</p>
    pub fn job_status(&self) -> std::option::Option<& crate::types::VideoJobStatus> {
        self.job_status.as_ref()
    }
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    pub fn status_message(&self) -> std::option::Option<& str> {
        self.status_message.as_deref()
    }
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>
    pub fn next_token(&self) -> std::option::Option<& str> {
        self.next_token.as_deref()
    }
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition Video operation. </p>
    pub fn video_metadata(&self) -> std::option::Option<& crate::types::VideoMetadata> {
        self.video_metadata.as_ref()
    }
    /// <p>An array of persons, <code>PersonMatch</code>, in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to <code>StartFaceSearch</code>. Each <code>Persons</code> element includes a time the person was matched, face match details (<code>FaceMatches</code>) for matching faces in the collection, and person information (<code>Person</code>) for the matched person. </p>
    pub fn persons(&self) -> std::option::Option<& [crate::types::PersonMatch]> {
        self.persons.as_deref()
    }
}
impl aws_http::request_id::RequestId for GetFaceSearchOutput {
                                fn request_id(&self) -> Option<&str> {
                                    self._request_id.as_deref()
                                }
                            }
impl GetFaceSearchOutput {
    /// Creates a new builder-style object to manufacture [`GetFaceSearchOutput`](crate::operation::get_face_search::GetFaceSearchOutput).
    pub fn builder() -> crate::operation::get_face_search::builders::GetFaceSearchOutputBuilder {
        crate::operation::get_face_search::builders::GetFaceSearchOutputBuilder::default()
    }
}

/// A builder for [`GetFaceSearchOutput`](crate::operation::get_face_search::GetFaceSearchOutput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct GetFaceSearchOutputBuilder {
    pub(crate) job_status: std::option::Option<crate::types::VideoJobStatus>,
    pub(crate) status_message: std::option::Option<std::string::String>,
    pub(crate) next_token: std::option::Option<std::string::String>,
    pub(crate) video_metadata: std::option::Option<crate::types::VideoMetadata>,
    pub(crate) persons: std::option::Option<std::vec::Vec<crate::types::PersonMatch>>,
    _request_id: Option<String>,
}
impl GetFaceSearchOutputBuilder {
    /// <p>The current status of the face search job.</p>
    pub fn job_status(mut self, input: crate::types::VideoJobStatus) -> Self {
        self.job_status = Some(input);
        self
    }
    /// <p>The current status of the face search job.</p>
    pub fn set_job_status(mut self, input: std::option::Option<crate::types::VideoJobStatus>) -> Self {
        self.job_status = input; self
    }
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    pub fn status_message(mut self, input: impl Into<std::string::String>) -> Self {
        self.status_message = Some(input.into());
        self
    }
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    pub fn set_status_message(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.status_message = input; self
    }
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>
    pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
        self.next_token = Some(input.into());
        self
    }
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>
    pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.next_token = input; self
    }
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition Video operation. </p>
    pub fn video_metadata(mut self, input: crate::types::VideoMetadata) -> Self {
        self.video_metadata = Some(input);
        self
    }
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition Video operation. </p>
    pub fn set_video_metadata(mut self, input: std::option::Option<crate::types::VideoMetadata>) -> Self {
        self.video_metadata = input; self
    }
    /// Appends an item to `persons`.
    ///
    /// To override the contents of this collection use [`set_persons`](Self::set_persons).
    ///
    /// <p>An array of persons, <code>PersonMatch</code>, in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to <code>StartFaceSearch</code>. Each <code>Persons</code> element includes a time the person was matched, face match details (<code>FaceMatches</code>) for matching faces in the collection, and person information (<code>Person</code>) for the matched person. </p>
    pub fn persons(mut self, input: crate::types::PersonMatch) -> Self {
        let mut v = self.persons.unwrap_or_default();
                        v.push(input);
                        self.persons = Some(v);
                        self
    }
    /// <p>An array of persons, <code>PersonMatch</code>, in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to <code>StartFaceSearch</code>. Each <code>Persons</code> element includes a time the person was matched, face match details (<code>FaceMatches</code>) for matching faces in the collection, and person information (<code>Person</code>) for the matched person. </p>
    pub fn set_persons(mut self, input: std::option::Option<std::vec::Vec<crate::types::PersonMatch>>) -> Self {
        self.persons = input; self
    }
    pub(crate) fn _request_id(mut self, request_id: impl Into<String>) -> Self {
                                    self._request_id = Some(request_id.into());
                                    self
                                }
    
                                pub(crate) fn _set_request_id(&mut self, request_id: Option<String>) -> &mut Self {
                                    self._request_id = request_id;
                                    self
                                }
    /// Consumes the builder and constructs a [`GetFaceSearchOutput`](crate::operation::get_face_search::GetFaceSearchOutput).
    pub fn build(self) -> crate::operation::get_face_search::GetFaceSearchOutput {
        crate::operation::get_face_search::GetFaceSearchOutput {
            job_status: self.job_status
            ,
            status_message: self.status_message
            ,
            next_token: self.next_token
            ,
            video_metadata: self.video_metadata
            ,
            persons: self.persons
            ,
            _request_id: self._request_id,
        }
    }
}

