// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct GetTextDetectionOutput {
    /// <p>Current status of the text detection job.</p>
    #[doc(hidden)]
    pub job_status: std::option::Option<crate::types::VideoJobStatus>,
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    #[doc(hidden)]
    pub status_message: std::option::Option<std::string::String>,
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition video operation.</p>
    #[doc(hidden)]
    pub video_metadata: std::option::Option<crate::types::VideoMetadata>,
    /// <p>An array of text detected in the video. Each element contains the detected text, the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.</p>
    #[doc(hidden)]
    pub text_detections: std::option::Option<std::vec::Vec<crate::types::TextDetectionResult>>,
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of text.</p>
    #[doc(hidden)]
    pub next_token: std::option::Option<std::string::String>,
    /// <p>Version number of the text detection model that was used to detect text.</p>
    #[doc(hidden)]
    pub text_model_version: std::option::Option<std::string::String>,
    _request_id: Option<String>,
}
impl GetTextDetectionOutput {
    /// <p>Current status of the text detection job.</p>
    pub fn job_status(&self) -> std::option::Option<&crate::types::VideoJobStatus> {
        self.job_status.as_ref()
    }
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    pub fn status_message(&self) -> std::option::Option<&str> {
        self.status_message.as_deref()
    }
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition video operation.</p>
    pub fn video_metadata(&self) -> std::option::Option<&crate::types::VideoMetadata> {
        self.video_metadata.as_ref()
    }
    /// <p>An array of text detected in the video. Each element contains the detected text, the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.</p>
    pub fn text_detections(&self) -> std::option::Option<&[crate::types::TextDetectionResult]> {
        self.text_detections.as_deref()
    }
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of text.</p>
    pub fn next_token(&self) -> std::option::Option<&str> {
        self.next_token.as_deref()
    }
    /// <p>Version number of the text detection model that was used to detect text.</p>
    pub fn text_model_version(&self) -> std::option::Option<&str> {
        self.text_model_version.as_deref()
    }
}
impl aws_http::request_id::RequestId for GetTextDetectionOutput {
    fn request_id(&self) -> Option<&str> {
        self._request_id.as_deref()
    }
}
impl GetTextDetectionOutput {
    /// Creates a new builder-style object to manufacture [`GetTextDetectionOutput`](crate::operation::get_text_detection::GetTextDetectionOutput).
    pub fn builder() -> crate::operation::get_text_detection::builders::GetTextDetectionOutputBuilder
    {
        crate::operation::get_text_detection::builders::GetTextDetectionOutputBuilder::default()
    }
}

/// A builder for [`GetTextDetectionOutput`](crate::operation::get_text_detection::GetTextDetectionOutput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct GetTextDetectionOutputBuilder {
    pub(crate) job_status: std::option::Option<crate::types::VideoJobStatus>,
    pub(crate) status_message: std::option::Option<std::string::String>,
    pub(crate) video_metadata: std::option::Option<crate::types::VideoMetadata>,
    pub(crate) text_detections:
        std::option::Option<std::vec::Vec<crate::types::TextDetectionResult>>,
    pub(crate) next_token: std::option::Option<std::string::String>,
    pub(crate) text_model_version: std::option::Option<std::string::String>,
    _request_id: Option<String>,
}
impl GetTextDetectionOutputBuilder {
    /// <p>Current status of the text detection job.</p>
    pub fn job_status(mut self, input: crate::types::VideoJobStatus) -> Self {
        self.job_status = Some(input);
        self
    }
    /// <p>Current status of the text detection job.</p>
    pub fn set_job_status(
        mut self,
        input: std::option::Option<crate::types::VideoJobStatus>,
    ) -> Self {
        self.job_status = input;
        self
    }
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    pub fn status_message(mut self, input: impl Into<std::string::String>) -> Self {
        self.status_message = Some(input.into());
        self
    }
    /// <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    pub fn set_status_message(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.status_message = input;
        self
    }
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition video operation.</p>
    pub fn video_metadata(mut self, input: crate::types::VideoMetadata) -> Self {
        self.video_metadata = Some(input);
        self
    }
    /// <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition video operation.</p>
    pub fn set_video_metadata(
        mut self,
        input: std::option::Option<crate::types::VideoMetadata>,
    ) -> Self {
        self.video_metadata = input;
        self
    }
    /// Appends an item to `text_detections`.
    ///
    /// To override the contents of this collection use [`set_text_detections`](Self::set_text_detections).
    ///
    /// <p>An array of text detected in the video. Each element contains the detected text, the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.</p>
    pub fn text_detections(mut self, input: crate::types::TextDetectionResult) -> Self {
        let mut v = self.text_detections.unwrap_or_default();
        v.push(input);
        self.text_detections = Some(v);
        self
    }
    /// <p>An array of text detected in the video. Each element contains the detected text, the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.</p>
    pub fn set_text_detections(
        mut self,
        input: std::option::Option<std::vec::Vec<crate::types::TextDetectionResult>>,
    ) -> Self {
        self.text_detections = input;
        self
    }
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of text.</p>
    pub fn next_token(mut self, input: impl Into<std::string::String>) -> Self {
        self.next_token = Some(input.into());
        self
    }
    /// <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of text.</p>
    pub fn set_next_token(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.next_token = input;
        self
    }
    /// <p>Version number of the text detection model that was used to detect text.</p>
    pub fn text_model_version(mut self, input: impl Into<std::string::String>) -> Self {
        self.text_model_version = Some(input.into());
        self
    }
    /// <p>Version number of the text detection model that was used to detect text.</p>
    pub fn set_text_model_version(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.text_model_version = input;
        self
    }
    pub(crate) fn _request_id(mut self, request_id: impl Into<String>) -> Self {
        self._request_id = Some(request_id.into());
        self
    }

    pub(crate) fn _set_request_id(&mut self, request_id: Option<String>) -> &mut Self {
        self._request_id = request_id;
        self
    }
    /// Consumes the builder and constructs a [`GetTextDetectionOutput`](crate::operation::get_text_detection::GetTextDetectionOutput).
    pub fn build(self) -> crate::operation::get_text_detection::GetTextDetectionOutput {
        crate::operation::get_text_detection::GetTextDetectionOutput {
            job_status: self.job_status,
            status_message: self.status_message,
            video_metadata: self.video_metadata,
            text_detections: self.text_detections,
            next_token: self.next_token,
            text_model_version: self.text_model_version,
            _request_id: self._request_id,
        }
    }
}
