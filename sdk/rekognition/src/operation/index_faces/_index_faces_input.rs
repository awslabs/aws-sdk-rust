// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct IndexFacesInput {
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub collection_id: ::std::option::Option<::std::string::String>,
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub image: ::std::option::Option<crate::types::Image>,
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub external_image_id: ::std::option::Option<::std::string::String>,
    /// <p>An array of facial attributes you want to be returned. A <code>DEFAULT</code> subset of facial attributes - <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code> - will always be returned. You can request for specific facial attributes (in addition to the default list) - by using <code>["DEFAULT", "FACE_OCCLUDED"]</code> or just <code>["FACE_OCCLUDED"]</code>. You can request for all facial attributes by using <code>["ALL"]</code>. Requesting more attributes may increase response time.</p>
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub detection_attributes: ::std::option::Option<::std::vec::Vec<crate::types::Attribute>>,
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p>
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p>
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub max_faces: ::std::option::Option<i32>,
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p>
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub quality_filter: ::std::option::Option<crate::types::QualityFilter>,
}
impl IndexFacesInput {
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub fn collection_id(&self) -> ::std::option::Option<&str> {
        self.collection_id.as_deref()
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub fn image(&self) -> ::std::option::Option<&crate::types::Image> {
        self.image.as_ref()
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn external_image_id(&self) -> ::std::option::Option<&str> {
        self.external_image_id.as_deref()
    }
    /// <p>An array of facial attributes you want to be returned. A <code>DEFAULT</code> subset of facial attributes - <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code> - will always be returned. You can request for specific facial attributes (in addition to the default list) - by using <code>["DEFAULT", "FACE_OCCLUDED"]</code> or just <code>["FACE_OCCLUDED"]</code>. You can request for all facial attributes by using <code>["ALL"]</code>. Requesting more attributes may increase response time.</p>
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.detection_attributes.is_none()`.
    pub fn detection_attributes(&self) -> &[crate::types::Attribute] {
        self.detection_attributes.as_deref().unwrap_or_default()
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p>
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p>
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn max_faces(&self) -> ::std::option::Option<i32> {
        self.max_faces
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p>
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn quality_filter(&self) -> ::std::option::Option<&crate::types::QualityFilter> {
        self.quality_filter.as_ref()
    }
}
impl IndexFacesInput {
    /// Creates a new builder-style object to manufacture [`IndexFacesInput`](crate::operation::index_faces::IndexFacesInput).
    pub fn builder() -> crate::operation::index_faces::builders::IndexFacesInputBuilder {
        crate::operation::index_faces::builders::IndexFacesInputBuilder::default()
    }
}

/// A builder for [`IndexFacesInput`](crate::operation::index_faces::IndexFacesInput).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct IndexFacesInputBuilder {
    pub(crate) collection_id: ::std::option::Option<::std::string::String>,
    pub(crate) image: ::std::option::Option<crate::types::Image>,
    pub(crate) external_image_id: ::std::option::Option<::std::string::String>,
    pub(crate) detection_attributes: ::std::option::Option<::std::vec::Vec<crate::types::Attribute>>,
    pub(crate) max_faces: ::std::option::Option<i32>,
    pub(crate) quality_filter: ::std::option::Option<crate::types::QualityFilter>,
}
impl IndexFacesInputBuilder {
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    /// This field is required.
    pub fn collection_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.collection_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub fn set_collection_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.collection_id = input;
        self
    }
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub fn get_collection_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.collection_id
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    /// This field is required.
    pub fn image(mut self, input: crate::types::Image) -> Self {
        self.image = ::std::option::Option::Some(input);
        self
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub fn set_image(mut self, input: ::std::option::Option<crate::types::Image>) -> Self {
        self.image = input;
        self
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub fn get_image(&self) -> &::std::option::Option<crate::types::Image> {
        &self.image
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn external_image_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.external_image_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn set_external_image_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.external_image_id = input;
        self
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn get_external_image_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.external_image_id
    }
    /// Appends an item to `detection_attributes`.
    ///
    /// To override the contents of this collection use [`set_detection_attributes`](Self::set_detection_attributes).
    ///
    /// <p>An array of facial attributes you want to be returned. A <code>DEFAULT</code> subset of facial attributes - <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code> - will always be returned. You can request for specific facial attributes (in addition to the default list) - by using <code>["DEFAULT", "FACE_OCCLUDED"]</code> or just <code>["FACE_OCCLUDED"]</code>. You can request for all facial attributes by using <code>["ALL"]</code>. Requesting more attributes may increase response time.</p>
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub fn detection_attributes(mut self, input: crate::types::Attribute) -> Self {
        let mut v = self.detection_attributes.unwrap_or_default();
        v.push(input);
        self.detection_attributes = ::std::option::Option::Some(v);
        self
    }
    /// <p>An array of facial attributes you want to be returned. A <code>DEFAULT</code> subset of facial attributes - <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code> - will always be returned. You can request for specific facial attributes (in addition to the default list) - by using <code>["DEFAULT", "FACE_OCCLUDED"]</code> or just <code>["FACE_OCCLUDED"]</code>. You can request for all facial attributes by using <code>["ALL"]</code>. Requesting more attributes may increase response time.</p>
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub fn set_detection_attributes(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::Attribute>>) -> Self {
        self.detection_attributes = input;
        self
    }
    /// <p>An array of facial attributes you want to be returned. A <code>DEFAULT</code> subset of facial attributes - <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code> - will always be returned. You can request for specific facial attributes (in addition to the default list) - by using <code>["DEFAULT", "FACE_OCCLUDED"]</code> or just <code>["FACE_OCCLUDED"]</code>. You can request for all facial attributes by using <code>["ALL"]</code>. Requesting more attributes may increase response time.</p>
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub fn get_detection_attributes(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::Attribute>> {
        &self.detection_attributes
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p>
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p>
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn max_faces(mut self, input: i32) -> Self {
        self.max_faces = ::std::option::Option::Some(input);
        self
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p>
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p>
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn set_max_faces(mut self, input: ::std::option::Option<i32>) -> Self {
        self.max_faces = input;
        self
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p>
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p>
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn get_max_faces(&self) -> &::std::option::Option<i32> {
        &self.max_faces
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p>
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn quality_filter(mut self, input: crate::types::QualityFilter) -> Self {
        self.quality_filter = ::std::option::Option::Some(input);
        self
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p>
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn set_quality_filter(mut self, input: ::std::option::Option<crate::types::QualityFilter>) -> Self {
        self.quality_filter = input;
        self
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p>
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn get_quality_filter(&self) -> &::std::option::Option<crate::types::QualityFilter> {
        &self.quality_filter
    }
    /// Consumes the builder and constructs a [`IndexFacesInput`](crate::operation::index_faces::IndexFacesInput).
    pub fn build(self) -> ::std::result::Result<crate::operation::index_faces::IndexFacesInput, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::operation::index_faces::IndexFacesInput {
            collection_id: self.collection_id,
            image: self.image,
            external_image_id: self.external_image_id,
            detection_attributes: self.detection_attributes,
            max_faces: self.max_faces,
            quality_filter: self.quality_filter,
        })
    }
}
