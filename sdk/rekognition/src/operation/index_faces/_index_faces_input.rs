// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct IndexFacesInput  {
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    #[doc(hidden)]
    pub collection_id: std::option::Option<std::string::String>,
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p> 
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    #[doc(hidden)]
    pub image: std::option::Option<crate::types::Image>,
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    #[doc(hidden)]
    pub external_image_id: std::option::Option<std::string::String>,
    /// <p>An array of facial attributes that you want to be returned. This can be the default list of attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if you specify <code>["DEFAULT"]</code>, the API returns the following subset of facial attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>["ALL"]</code>, all facial attributes are returned, but the operation takes longer to complete.</p> 
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    #[doc(hidden)]
    pub detection_attributes: std::option::Option<std::vec::Vec<crate::types::Attribute>>,
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p> 
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p> 
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p> 
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    #[doc(hidden)]
    pub max_faces: std::option::Option<i32>,
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p> 
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    #[doc(hidden)]
    pub quality_filter: std::option::Option<crate::types::QualityFilter>,
}
impl IndexFacesInput {
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub fn collection_id(&self) -> std::option::Option<& str> {
        self.collection_id.as_deref()
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p> 
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub fn image(&self) -> std::option::Option<& crate::types::Image> {
        self.image.as_ref()
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn external_image_id(&self) -> std::option::Option<& str> {
        self.external_image_id.as_deref()
    }
    /// <p>An array of facial attributes that you want to be returned. This can be the default list of attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if you specify <code>["DEFAULT"]</code>, the API returns the following subset of facial attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>["ALL"]</code>, all facial attributes are returned, but the operation takes longer to complete.</p> 
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub fn detection_attributes(&self) -> std::option::Option<& [crate::types::Attribute]> {
        self.detection_attributes.as_deref()
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p> 
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p> 
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p> 
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn max_faces(&self) -> std::option::Option<i32> {
        self.max_faces
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p> 
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn quality_filter(&self) -> std::option::Option<& crate::types::QualityFilter> {
        self.quality_filter.as_ref()
    }
}
impl IndexFacesInput {
    /// Creates a new builder-style object to manufacture [`IndexFacesInput`](crate::operation::index_faces::IndexFacesInput).
    pub fn builder() -> crate::operation::index_faces::builders::IndexFacesInputBuilder {
        crate::operation::index_faces::builders::IndexFacesInputBuilder::default()
    }
}

/// A builder for [`IndexFacesInput`](crate::operation::index_faces::IndexFacesInput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct IndexFacesInputBuilder {
    pub(crate) collection_id: std::option::Option<std::string::String>,
    pub(crate) image: std::option::Option<crate::types::Image>,
    pub(crate) external_image_id: std::option::Option<std::string::String>,
    pub(crate) detection_attributes: std::option::Option<std::vec::Vec<crate::types::Attribute>>,
    pub(crate) max_faces: std::option::Option<i32>,
    pub(crate) quality_filter: std::option::Option<crate::types::QualityFilter>,
}
impl IndexFacesInputBuilder {
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub fn collection_id(mut self, input: impl Into<std::string::String>) -> Self {
        self.collection_id = Some(input.into());
        self
    }
    /// <p>The ID of an existing collection to which you want to add the faces that are detected in the input images.</p>
    pub fn set_collection_id(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.collection_id = input; self
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p> 
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub fn image(mut self, input: crate::types::Image) -> Self {
        self.image = Some(input);
        self
    }
    /// <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p> 
    /// <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    pub fn set_image(mut self, input: std::option::Option<crate::types::Image>) -> Self {
        self.image = input; self
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn external_image_id(mut self, input: impl Into<std::string::String>) -> Self {
        self.external_image_id = Some(input.into());
        self
    }
    /// <p>The ID you want to assign to all the faces detected in the image.</p>
    pub fn set_external_image_id(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.external_image_id = input; self
    }
    /// Appends an item to `detection_attributes`.
    ///
    /// To override the contents of this collection use [`set_detection_attributes`](Self::set_detection_attributes).
    ///
    /// <p>An array of facial attributes that you want to be returned. This can be the default list of attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if you specify <code>["DEFAULT"]</code>, the API returns the following subset of facial attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>["ALL"]</code>, all facial attributes are returned, but the operation takes longer to complete.</p> 
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub fn detection_attributes(mut self, input: crate::types::Attribute) -> Self {
        let mut v = self.detection_attributes.unwrap_or_default();
                        v.push(input);
                        self.detection_attributes = Some(v);
                        self
    }
    /// <p>An array of facial attributes that you want to be returned. This can be the default list of attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if you specify <code>["DEFAULT"]</code>, the API returns the following subset of facial attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>, <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>["ALL"]</code>, all facial attributes are returned, but the operation takes longer to complete.</p> 
    /// <p>If you provide both, <code>["ALL", "DEFAULT"]</code>, the service uses a logical AND operator to determine which attributes to return (in this case, all attributes). </p>
    pub fn set_detection_attributes(mut self, input: std::option::Option<std::vec::Vec<crate::types::Attribute>>) -> Self {
        self.detection_attributes = input; self
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p> 
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p> 
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p> 
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn max_faces(mut self, input: i32) -> Self {
        self.max_faces = Some(input);
        self
    }
    /// <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an image, even if you specify a larger value for <code>MaxFaces</code>.</p> 
    /// <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the faces with the lowest quality are filtered out first. If there are still more faces than the value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p> 
    /// <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face bounding box size to the smallest size, in descending order.</p> 
    /// <p> <code>MaxFaces</code> can be used with a collection associated with any version of the face model.</p>
    pub fn set_max_faces(mut self, input: std::option::Option<i32>) -> Self {
        self.max_faces = input; self
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p> 
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn quality_filter(mut self, input: crate::types::QualityFilter) -> Self {
        self.quality_filter = Some(input);
        self
    }
    /// <p>A filter that specifies a quality bar for how much filtering is done to identify faces. Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar. If you specify <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that don’t meet the chosen quality bar. The default value is <code>AUTO</code>. The quality bar is based on a variety of common use cases. Low-quality detections can occur for a number of reasons. Some examples are an object that's misidentified as a face, a face that's too blurry, or a face with a pose that's too extreme to use. If you specify <code>NONE</code>, no filtering is performed. </p> 
    /// <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>
    pub fn set_quality_filter(mut self, input: std::option::Option<crate::types::QualityFilter>) -> Self {
        self.quality_filter = input; self
    }
    /// Consumes the builder and constructs a [`IndexFacesInput`](crate::operation::index_faces::IndexFacesInput).
    pub fn build(self) -> Result<crate::operation::index_faces::IndexFacesInput, aws_smithy_http::operation::error::BuildError> {
        Ok(
            crate::operation::index_faces::IndexFacesInput {
                collection_id: self.collection_id
                ,
                image: self.image
                ,
                external_image_id: self.external_image_id
                ,
                detection_attributes: self.detection_attributes
                ,
                max_faces: self.max_faces
                ,
                quality_filter: self.quality_filter
                ,
            }
        )
    }
}

