// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
impl super::Client {
    /// Constructs a fluent builder for the [`GetFaceDetection`](crate::client::fluent_builders::GetFaceDetection) operation.
    /// This operation supports pagination; See [`into_paginator()`](crate::client::fluent_builders::GetFaceDetection::into_paginator).
    ///
    /// - The fluent builder is configurable:
    ///   - [`job_id(impl Into<String>)`](crate::client::fluent_builders::GetFaceDetection::job_id) / [`set_job_id(Option<String>)`](crate::client::fluent_builders::GetFaceDetection::set_job_id): <p>Unique identifier for the face detection job. The <code>JobId</code> is returned from <code>StartFaceDetection</code>.</p>
    ///   - [`max_results(i32)`](crate::client::fluent_builders::GetFaceDetection::max_results) / [`set_max_results(Option<i32>)`](crate::client::fluent_builders::GetFaceDetection::set_max_results): <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000. If you specify a value greater than 1000, a maximum of 1000 results is returned. The default value is 1000.</p>
    ///   - [`next_token(impl Into<String>)`](crate::client::fluent_builders::GetFaceDetection::next_token) / [`set_next_token(Option<String>)`](crate::client::fluent_builders::GetFaceDetection::set_next_token): <p>If the previous response was incomplete (because there are more faces to retrieve), Amazon Rekognition Video returns a pagination token in the response. You can use this pagination token to retrieve the next set of faces.</p>
    /// - On success, responds with [`GetFaceDetectionOutput`](crate::output::GetFaceDetectionOutput) with field(s):
    ///   - [`job_status(Option<VideoJobStatus>)`](crate::output::GetFaceDetectionOutput::job_status): <p>The current status of the face detection job.</p>
    ///   - [`status_message(Option<String>)`](crate::output::GetFaceDetectionOutput::status_message): <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>
    ///   - [`video_metadata(Option<VideoMetadata>)`](crate::output::GetFaceDetectionOutput::video_metadata): <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in every page of paginated responses from a Amazon Rekognition video operation.</p>
    ///   - [`next_token(Option<String>)`](crate::output::GetFaceDetectionOutput::next_token): <p>If the response is truncated, Amazon Rekognition returns this token that you can use in the subsequent request to retrieve the next set of faces. </p>
    ///   - [`faces(Option<Vec<FaceDetection>>)`](crate::output::GetFaceDetectionOutput::faces): <p>An array of faces detected in the video. Each element contains a detected face's details and the time, in milliseconds from the start of the video, the face was detected. </p>
    /// - On failure, responds with [`SdkError<GetFaceDetectionError>`](crate::error::GetFaceDetectionError)
    pub fn get_face_detection(&self) -> crate::client::fluent_builders::GetFaceDetection {
        crate::client::fluent_builders::GetFaceDetection::new(self.handle.clone())
    }
}
