// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
impl super::Client {
    /// Constructs a fluent builder for the [`DetectModerationLabels`](crate::client::fluent_builders::DetectModerationLabels) operation.
    ///
    /// - The fluent builder is configurable:
    ///   - [`image(Image)`](crate::client::fluent_builders::DetectModerationLabels::image) / [`set_image(Option<Image>)`](crate::client::fluent_builders::DetectModerationLabels::set_image): <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>  <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes passed using the <code>Bytes</code> field. For more information, see Images in the Amazon Rekognition developer guide.</p>
    ///   - [`min_confidence(f32)`](crate::client::fluent_builders::DetectModerationLabels::min_confidence) / [`set_min_confidence(Option<f32>)`](crate::client::fluent_builders::DetectModerationLabels::set_min_confidence): <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't return any labels with a confidence level lower than this specified value.</p>  <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with confidence values greater than or equal to 50 percent.</p>
    ///   - [`human_loop_config(HumanLoopConfig)`](crate::client::fluent_builders::DetectModerationLabels::human_loop_config) / [`set_human_loop_config(Option<HumanLoopConfig>)`](crate::client::fluent_builders::DetectModerationLabels::set_human_loop_config): <p>Sets up the configuration for human evaluation, including the FlowDefinition the image will be sent to.</p>
    /// - On success, responds with [`DetectModerationLabelsOutput`](crate::output::DetectModerationLabelsOutput) with field(s):
    ///   - [`moderation_labels(Option<Vec<ModerationLabel>>)`](crate::output::DetectModerationLabelsOutput::moderation_labels): <p>Array of detected Moderation labels and the time, in milliseconds from the start of the video, they were detected.</p>
    ///   - [`moderation_model_version(Option<String>)`](crate::output::DetectModerationLabelsOutput::moderation_model_version): <p>Version number of the moderation detection model that was used to detect unsafe content.</p>
    ///   - [`human_loop_activation_output(Option<HumanLoopActivationOutput>)`](crate::output::DetectModerationLabelsOutput::human_loop_activation_output): <p>Shows the results of the human in the loop evaluation.</p>
    /// - On failure, responds with [`SdkError<DetectModerationLabelsError>`](crate::error::DetectModerationLabelsError)
    pub fn detect_moderation_labels(
        &self,
    ) -> crate::client::fluent_builders::DetectModerationLabels {
        crate::client::fluent_builders::DetectModerationLabels::new(self.handle.clone())
    }
}
