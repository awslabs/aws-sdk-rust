// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>The comprehensive result of an evaluation containing the score, explanation, evaluator metadata, and execution details. Provides both quantitative ratings and qualitative insights about agent performance.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq)]
pub struct EvaluationResultContent {
    /// <p>The Amazon Resource Name (ARN) of the evaluator used to generate this result. For custom evaluators, this is the full ARN; for built-in evaluators, this follows the pattern <code>Builtin.{EvaluatorName}</code>.</p>
    pub evaluator_arn: ::std::string::String,
    /// <p>The unique identifier of the evaluator that produced this result. This matches the <code>evaluatorId</code> provided in the evaluation request and can be used to identify which evaluator generated specific results.</p>
    pub evaluator_id: ::std::string::String,
    /// <p>The human-readable name of the evaluator used for this evaluation. For built-in evaluators, this is the descriptive name (e.g., "Helpfulness", "Correctness"); for custom evaluators, this is the user-defined name.</p>
    pub evaluator_name: ::std::string::String,
    /// <p>The detailed explanation provided by the evaluator describing the reasoning behind the assigned score. This qualitative feedback helps understand why specific ratings were given and provides actionable insights for improvement.</p>
    pub explanation: ::std::option::Option<::std::string::String>,
    /// <p>The contextual information associated with this evaluation result, including span context details that identify the specific traces and sessions that were evaluated.</p>
    pub context: ::std::option::Option<crate::types::Context>,
    /// <p>The numerical score assigned by the evaluator according to its configured rating scale. For numerical scales, this is a decimal value within the defined range. This field is not allowed for categorical scales.</p>
    pub value: ::std::option::Option<f64>,
    /// <p>The categorical label assigned by the evaluator when using a categorical rating scale. This provides a human-readable description of the evaluation result (e.g., "Excellent", "Good", "Poor") corresponding to the numerical value. For numerical scales, this field is optional and provides a natural language explanation of what the value means (e.g., value 0.5 = "Somewhat Helpful").</p>
    pub label: ::std::option::Option<::std::string::String>,
    /// <p>The token consumption statistics for this evaluation, including input tokens, output tokens, and total tokens used by the underlying language model during the evaluation process.</p>
    pub token_usage: ::std::option::Option<crate::types::TokenUsage>,
    /// <p>The error message describing what went wrong if the evaluation failed. Provides detailed information about evaluation failures to help diagnose and resolve issues with evaluator configuration or input data.</p>
    pub error_message: ::std::option::Option<::std::string::String>,
    /// <p>The error code indicating the type of failure that occurred during evaluation. Used to programmatically identify and handle different categories of evaluation errors.</p>
    pub error_code: ::std::option::Option<::std::string::String>,
}
impl EvaluationResultContent {
    /// <p>The Amazon Resource Name (ARN) of the evaluator used to generate this result. For custom evaluators, this is the full ARN; for built-in evaluators, this follows the pattern <code>Builtin.{EvaluatorName}</code>.</p>
    pub fn evaluator_arn(&self) -> &str {
        use std::ops::Deref;
        self.evaluator_arn.deref()
    }
    /// <p>The unique identifier of the evaluator that produced this result. This matches the <code>evaluatorId</code> provided in the evaluation request and can be used to identify which evaluator generated specific results.</p>
    pub fn evaluator_id(&self) -> &str {
        use std::ops::Deref;
        self.evaluator_id.deref()
    }
    /// <p>The human-readable name of the evaluator used for this evaluation. For built-in evaluators, this is the descriptive name (e.g., "Helpfulness", "Correctness"); for custom evaluators, this is the user-defined name.</p>
    pub fn evaluator_name(&self) -> &str {
        use std::ops::Deref;
        self.evaluator_name.deref()
    }
    /// <p>The detailed explanation provided by the evaluator describing the reasoning behind the assigned score. This qualitative feedback helps understand why specific ratings were given and provides actionable insights for improvement.</p>
    pub fn explanation(&self) -> ::std::option::Option<&str> {
        self.explanation.as_deref()
    }
    /// <p>The contextual information associated with this evaluation result, including span context details that identify the specific traces and sessions that were evaluated.</p>
    pub fn context(&self) -> ::std::option::Option<&crate::types::Context> {
        self.context.as_ref()
    }
    /// <p>The numerical score assigned by the evaluator according to its configured rating scale. For numerical scales, this is a decimal value within the defined range. This field is not allowed for categorical scales.</p>
    pub fn value(&self) -> ::std::option::Option<f64> {
        self.value
    }
    /// <p>The categorical label assigned by the evaluator when using a categorical rating scale. This provides a human-readable description of the evaluation result (e.g., "Excellent", "Good", "Poor") corresponding to the numerical value. For numerical scales, this field is optional and provides a natural language explanation of what the value means (e.g., value 0.5 = "Somewhat Helpful").</p>
    pub fn label(&self) -> ::std::option::Option<&str> {
        self.label.as_deref()
    }
    /// <p>The token consumption statistics for this evaluation, including input tokens, output tokens, and total tokens used by the underlying language model during the evaluation process.</p>
    pub fn token_usage(&self) -> ::std::option::Option<&crate::types::TokenUsage> {
        self.token_usage.as_ref()
    }
    /// <p>The error message describing what went wrong if the evaluation failed. Provides detailed information about evaluation failures to help diagnose and resolve issues with evaluator configuration or input data.</p>
    pub fn error_message(&self) -> ::std::option::Option<&str> {
        self.error_message.as_deref()
    }
    /// <p>The error code indicating the type of failure that occurred during evaluation. Used to programmatically identify and handle different categories of evaluation errors.</p>
    pub fn error_code(&self) -> ::std::option::Option<&str> {
        self.error_code.as_deref()
    }
}
impl ::std::fmt::Debug for EvaluationResultContent {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("EvaluationResultContent");
        formatter.field("evaluator_arn", &self.evaluator_arn);
        formatter.field("evaluator_id", &self.evaluator_id);
        formatter.field("evaluator_name", &self.evaluator_name);
        formatter.field("explanation", &"*** Sensitive Data Redacted ***");
        formatter.field("context", &self.context);
        formatter.field("value", &self.value);
        formatter.field("label", &self.label);
        formatter.field("token_usage", &self.token_usage);
        formatter.field("error_message", &self.error_message);
        formatter.field("error_code", &self.error_code);
        formatter.finish()
    }
}
impl EvaluationResultContent {
    /// Creates a new builder-style object to manufacture [`EvaluationResultContent`](crate::types::EvaluationResultContent).
    pub fn builder() -> crate::types::builders::EvaluationResultContentBuilder {
        crate::types::builders::EvaluationResultContentBuilder::default()
    }
}

/// A builder for [`EvaluationResultContent`](crate::types::EvaluationResultContent).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default)]
#[non_exhaustive]
pub struct EvaluationResultContentBuilder {
    pub(crate) evaluator_arn: ::std::option::Option<::std::string::String>,
    pub(crate) evaluator_id: ::std::option::Option<::std::string::String>,
    pub(crate) evaluator_name: ::std::option::Option<::std::string::String>,
    pub(crate) explanation: ::std::option::Option<::std::string::String>,
    pub(crate) context: ::std::option::Option<crate::types::Context>,
    pub(crate) value: ::std::option::Option<f64>,
    pub(crate) label: ::std::option::Option<::std::string::String>,
    pub(crate) token_usage: ::std::option::Option<crate::types::TokenUsage>,
    pub(crate) error_message: ::std::option::Option<::std::string::String>,
    pub(crate) error_code: ::std::option::Option<::std::string::String>,
}
impl EvaluationResultContentBuilder {
    /// <p>The Amazon Resource Name (ARN) of the evaluator used to generate this result. For custom evaluators, this is the full ARN; for built-in evaluators, this follows the pattern <code>Builtin.{EvaluatorName}</code>.</p>
    /// This field is required.
    pub fn evaluator_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.evaluator_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the evaluator used to generate this result. For custom evaluators, this is the full ARN; for built-in evaluators, this follows the pattern <code>Builtin.{EvaluatorName}</code>.</p>
    pub fn set_evaluator_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.evaluator_arn = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the evaluator used to generate this result. For custom evaluators, this is the full ARN; for built-in evaluators, this follows the pattern <code>Builtin.{EvaluatorName}</code>.</p>
    pub fn get_evaluator_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.evaluator_arn
    }
    /// <p>The unique identifier of the evaluator that produced this result. This matches the <code>evaluatorId</code> provided in the evaluation request and can be used to identify which evaluator generated specific results.</p>
    /// This field is required.
    pub fn evaluator_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.evaluator_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The unique identifier of the evaluator that produced this result. This matches the <code>evaluatorId</code> provided in the evaluation request and can be used to identify which evaluator generated specific results.</p>
    pub fn set_evaluator_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.evaluator_id = input;
        self
    }
    /// <p>The unique identifier of the evaluator that produced this result. This matches the <code>evaluatorId</code> provided in the evaluation request and can be used to identify which evaluator generated specific results.</p>
    pub fn get_evaluator_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.evaluator_id
    }
    /// <p>The human-readable name of the evaluator used for this evaluation. For built-in evaluators, this is the descriptive name (e.g., "Helpfulness", "Correctness"); for custom evaluators, this is the user-defined name.</p>
    /// This field is required.
    pub fn evaluator_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.evaluator_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The human-readable name of the evaluator used for this evaluation. For built-in evaluators, this is the descriptive name (e.g., "Helpfulness", "Correctness"); for custom evaluators, this is the user-defined name.</p>
    pub fn set_evaluator_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.evaluator_name = input;
        self
    }
    /// <p>The human-readable name of the evaluator used for this evaluation. For built-in evaluators, this is the descriptive name (e.g., "Helpfulness", "Correctness"); for custom evaluators, this is the user-defined name.</p>
    pub fn get_evaluator_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.evaluator_name
    }
    /// <p>The detailed explanation provided by the evaluator describing the reasoning behind the assigned score. This qualitative feedback helps understand why specific ratings were given and provides actionable insights for improvement.</p>
    pub fn explanation(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.explanation = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The detailed explanation provided by the evaluator describing the reasoning behind the assigned score. This qualitative feedback helps understand why specific ratings were given and provides actionable insights for improvement.</p>
    pub fn set_explanation(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.explanation = input;
        self
    }
    /// <p>The detailed explanation provided by the evaluator describing the reasoning behind the assigned score. This qualitative feedback helps understand why specific ratings were given and provides actionable insights for improvement.</p>
    pub fn get_explanation(&self) -> &::std::option::Option<::std::string::String> {
        &self.explanation
    }
    /// <p>The contextual information associated with this evaluation result, including span context details that identify the specific traces and sessions that were evaluated.</p>
    /// This field is required.
    pub fn context(mut self, input: crate::types::Context) -> Self {
        self.context = ::std::option::Option::Some(input);
        self
    }
    /// <p>The contextual information associated with this evaluation result, including span context details that identify the specific traces and sessions that were evaluated.</p>
    pub fn set_context(mut self, input: ::std::option::Option<crate::types::Context>) -> Self {
        self.context = input;
        self
    }
    /// <p>The contextual information associated with this evaluation result, including span context details that identify the specific traces and sessions that were evaluated.</p>
    pub fn get_context(&self) -> &::std::option::Option<crate::types::Context> {
        &self.context
    }
    /// <p>The numerical score assigned by the evaluator according to its configured rating scale. For numerical scales, this is a decimal value within the defined range. This field is not allowed for categorical scales.</p>
    pub fn value(mut self, input: f64) -> Self {
        self.value = ::std::option::Option::Some(input);
        self
    }
    /// <p>The numerical score assigned by the evaluator according to its configured rating scale. For numerical scales, this is a decimal value within the defined range. This field is not allowed for categorical scales.</p>
    pub fn set_value(mut self, input: ::std::option::Option<f64>) -> Self {
        self.value = input;
        self
    }
    /// <p>The numerical score assigned by the evaluator according to its configured rating scale. For numerical scales, this is a decimal value within the defined range. This field is not allowed for categorical scales.</p>
    pub fn get_value(&self) -> &::std::option::Option<f64> {
        &self.value
    }
    /// <p>The categorical label assigned by the evaluator when using a categorical rating scale. This provides a human-readable description of the evaluation result (e.g., "Excellent", "Good", "Poor") corresponding to the numerical value. For numerical scales, this field is optional and provides a natural language explanation of what the value means (e.g., value 0.5 = "Somewhat Helpful").</p>
    pub fn label(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.label = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The categorical label assigned by the evaluator when using a categorical rating scale. This provides a human-readable description of the evaluation result (e.g., "Excellent", "Good", "Poor") corresponding to the numerical value. For numerical scales, this field is optional and provides a natural language explanation of what the value means (e.g., value 0.5 = "Somewhat Helpful").</p>
    pub fn set_label(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.label = input;
        self
    }
    /// <p>The categorical label assigned by the evaluator when using a categorical rating scale. This provides a human-readable description of the evaluation result (e.g., "Excellent", "Good", "Poor") corresponding to the numerical value. For numerical scales, this field is optional and provides a natural language explanation of what the value means (e.g., value 0.5 = "Somewhat Helpful").</p>
    pub fn get_label(&self) -> &::std::option::Option<::std::string::String> {
        &self.label
    }
    /// <p>The token consumption statistics for this evaluation, including input tokens, output tokens, and total tokens used by the underlying language model during the evaluation process.</p>
    pub fn token_usage(mut self, input: crate::types::TokenUsage) -> Self {
        self.token_usage = ::std::option::Option::Some(input);
        self
    }
    /// <p>The token consumption statistics for this evaluation, including input tokens, output tokens, and total tokens used by the underlying language model during the evaluation process.</p>
    pub fn set_token_usage(mut self, input: ::std::option::Option<crate::types::TokenUsage>) -> Self {
        self.token_usage = input;
        self
    }
    /// <p>The token consumption statistics for this evaluation, including input tokens, output tokens, and total tokens used by the underlying language model during the evaluation process.</p>
    pub fn get_token_usage(&self) -> &::std::option::Option<crate::types::TokenUsage> {
        &self.token_usage
    }
    /// <p>The error message describing what went wrong if the evaluation failed. Provides detailed information about evaluation failures to help diagnose and resolve issues with evaluator configuration or input data.</p>
    pub fn error_message(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.error_message = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The error message describing what went wrong if the evaluation failed. Provides detailed information about evaluation failures to help diagnose and resolve issues with evaluator configuration or input data.</p>
    pub fn set_error_message(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.error_message = input;
        self
    }
    /// <p>The error message describing what went wrong if the evaluation failed. Provides detailed information about evaluation failures to help diagnose and resolve issues with evaluator configuration or input data.</p>
    pub fn get_error_message(&self) -> &::std::option::Option<::std::string::String> {
        &self.error_message
    }
    /// <p>The error code indicating the type of failure that occurred during evaluation. Used to programmatically identify and handle different categories of evaluation errors.</p>
    pub fn error_code(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.error_code = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The error code indicating the type of failure that occurred during evaluation. Used to programmatically identify and handle different categories of evaluation errors.</p>
    pub fn set_error_code(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.error_code = input;
        self
    }
    /// <p>The error code indicating the type of failure that occurred during evaluation. Used to programmatically identify and handle different categories of evaluation errors.</p>
    pub fn get_error_code(&self) -> &::std::option::Option<::std::string::String> {
        &self.error_code
    }
    /// Consumes the builder and constructs a [`EvaluationResultContent`](crate::types::EvaluationResultContent).
    /// This method will fail if any of the following fields are not set:
    /// - [`evaluator_arn`](crate::types::builders::EvaluationResultContentBuilder::evaluator_arn)
    /// - [`evaluator_id`](crate::types::builders::EvaluationResultContentBuilder::evaluator_id)
    /// - [`evaluator_name`](crate::types::builders::EvaluationResultContentBuilder::evaluator_name)
    pub fn build(self) -> ::std::result::Result<crate::types::EvaluationResultContent, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::types::EvaluationResultContent {
            evaluator_arn: self.evaluator_arn.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "evaluator_arn",
                    "evaluator_arn was not specified but it is required when building EvaluationResultContent",
                )
            })?,
            evaluator_id: self.evaluator_id.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "evaluator_id",
                    "evaluator_id was not specified but it is required when building EvaluationResultContent",
                )
            })?,
            evaluator_name: self.evaluator_name.ok_or_else(|| {
                ::aws_smithy_types::error::operation::BuildError::missing_field(
                    "evaluator_name",
                    "evaluator_name was not specified but it is required when building EvaluationResultContent",
                )
            })?,
            explanation: self.explanation,
            context: self.context,
            value: self.value,
            label: self.label,
            token_usage: self.token_usage,
            error_message: self.error_message,
            error_code: self.error_code,
        })
    }
}
impl ::std::fmt::Debug for EvaluationResultContentBuilder {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("EvaluationResultContentBuilder");
        formatter.field("evaluator_arn", &self.evaluator_arn);
        formatter.field("evaluator_id", &self.evaluator_id);
        formatter.field("evaluator_name", &self.evaluator_name);
        formatter.field("explanation", &"*** Sensitive Data Redacted ***");
        formatter.field("context", &self.context);
        formatter.field("value", &self.value);
        formatter.field("label", &self.label);
        formatter.field("token_usage", &self.token_usage);
        formatter.field("error_message", &self.error_message);
        formatter.field("error_code", &self.error_code);
        formatter.finish()
    }
}
