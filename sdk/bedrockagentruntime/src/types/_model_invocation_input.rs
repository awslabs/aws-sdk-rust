// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>The input for the pre-processing step.</p>
/// <ul>
/// <li>
/// <p>The <code>type</code> matches the agent step.</p></li>
/// <li>
/// <p>The <code>text</code> contains the prompt.</p></li>
/// <li>
/// <p>The <code>inferenceConfiguration</code>, <code>parserMode</code>, and <code>overrideLambda</code> values are set in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object that was set when the agent was created or updated.</p></li>
/// </ul>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq)]
pub struct ModelInvocationInput {
    /// <p>The unique identifier of the trace.</p>
    pub trace_id: ::std::option::Option<::std::string::String>,
    /// <p>The text that prompted the agent at this step.</p>
    pub text: ::std::option::Option<::std::string::String>,
    /// <p>The step in the agent sequence.</p>
    pub r#type: ::std::option::Option<crate::types::PromptType>,
    /// <p>Specifications about the inference parameters that were provided alongside the prompt. These are specified in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object that was set when the agent was created or updated. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html">Inference parameters for foundation models</a>.</p>
    pub inference_configuration: ::std::option::Option<crate::types::InferenceConfiguration>,
    /// <p>The ARN of the Lambda function to use when parsing the raw foundation model output in parts of the agent sequence.</p>
    pub override_lambda: ::std::option::Option<::std::string::String>,
    /// <p>Specifies whether the default prompt template was <code>OVERRIDDEN</code>. If it was, the <code>basePromptTemplate</code> that was set in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object when the agent was created or updated is used instead.</p>
    pub prompt_creation_mode: ::std::option::Option<crate::types::CreationMode>,
    /// <p>Specifies whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the <code>promptType</code>.</p>
    pub parser_mode: ::std::option::Option<crate::types::CreationMode>,
}
impl ModelInvocationInput {
    /// <p>The unique identifier of the trace.</p>
    pub fn trace_id(&self) -> ::std::option::Option<&str> {
        self.trace_id.as_deref()
    }
    /// <p>The text that prompted the agent at this step.</p>
    pub fn text(&self) -> ::std::option::Option<&str> {
        self.text.as_deref()
    }
    /// <p>The step in the agent sequence.</p>
    pub fn r#type(&self) -> ::std::option::Option<&crate::types::PromptType> {
        self.r#type.as_ref()
    }
    /// <p>Specifications about the inference parameters that were provided alongside the prompt. These are specified in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object that was set when the agent was created or updated. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html">Inference parameters for foundation models</a>.</p>
    pub fn inference_configuration(&self) -> ::std::option::Option<&crate::types::InferenceConfiguration> {
        self.inference_configuration.as_ref()
    }
    /// <p>The ARN of the Lambda function to use when parsing the raw foundation model output in parts of the agent sequence.</p>
    pub fn override_lambda(&self) -> ::std::option::Option<&str> {
        self.override_lambda.as_deref()
    }
    /// <p>Specifies whether the default prompt template was <code>OVERRIDDEN</code>. If it was, the <code>basePromptTemplate</code> that was set in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object when the agent was created or updated is used instead.</p>
    pub fn prompt_creation_mode(&self) -> ::std::option::Option<&crate::types::CreationMode> {
        self.prompt_creation_mode.as_ref()
    }
    /// <p>Specifies whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the <code>promptType</code>.</p>
    pub fn parser_mode(&self) -> ::std::option::Option<&crate::types::CreationMode> {
        self.parser_mode.as_ref()
    }
}
impl ::std::fmt::Debug for ModelInvocationInput {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("ModelInvocationInput");
        formatter.field("trace_id", &"*** Sensitive Data Redacted ***");
        formatter.field("text", &"*** Sensitive Data Redacted ***");
        formatter.field("r#type", &"*** Sensitive Data Redacted ***");
        formatter.field("inference_configuration", &"*** Sensitive Data Redacted ***");
        formatter.field("override_lambda", &"*** Sensitive Data Redacted ***");
        formatter.field("prompt_creation_mode", &"*** Sensitive Data Redacted ***");
        formatter.field("parser_mode", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}
impl ModelInvocationInput {
    /// Creates a new builder-style object to manufacture [`ModelInvocationInput`](crate::types::ModelInvocationInput).
    pub fn builder() -> crate::types::builders::ModelInvocationInputBuilder {
        crate::types::builders::ModelInvocationInputBuilder::default()
    }
}

/// A builder for [`ModelInvocationInput`](crate::types::ModelInvocationInput).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default)]
#[non_exhaustive]
pub struct ModelInvocationInputBuilder {
    pub(crate) trace_id: ::std::option::Option<::std::string::String>,
    pub(crate) text: ::std::option::Option<::std::string::String>,
    pub(crate) r#type: ::std::option::Option<crate::types::PromptType>,
    pub(crate) inference_configuration: ::std::option::Option<crate::types::InferenceConfiguration>,
    pub(crate) override_lambda: ::std::option::Option<::std::string::String>,
    pub(crate) prompt_creation_mode: ::std::option::Option<crate::types::CreationMode>,
    pub(crate) parser_mode: ::std::option::Option<crate::types::CreationMode>,
}
impl ModelInvocationInputBuilder {
    /// <p>The unique identifier of the trace.</p>
    pub fn trace_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.trace_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The unique identifier of the trace.</p>
    pub fn set_trace_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.trace_id = input;
        self
    }
    /// <p>The unique identifier of the trace.</p>
    pub fn get_trace_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.trace_id
    }
    /// <p>The text that prompted the agent at this step.</p>
    pub fn text(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.text = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The text that prompted the agent at this step.</p>
    pub fn set_text(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.text = input;
        self
    }
    /// <p>The text that prompted the agent at this step.</p>
    pub fn get_text(&self) -> &::std::option::Option<::std::string::String> {
        &self.text
    }
    /// <p>The step in the agent sequence.</p>
    pub fn r#type(mut self, input: crate::types::PromptType) -> Self {
        self.r#type = ::std::option::Option::Some(input);
        self
    }
    /// <p>The step in the agent sequence.</p>
    pub fn set_type(mut self, input: ::std::option::Option<crate::types::PromptType>) -> Self {
        self.r#type = input;
        self
    }
    /// <p>The step in the agent sequence.</p>
    pub fn get_type(&self) -> &::std::option::Option<crate::types::PromptType> {
        &self.r#type
    }
    /// <p>Specifications about the inference parameters that were provided alongside the prompt. These are specified in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object that was set when the agent was created or updated. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html">Inference parameters for foundation models</a>.</p>
    pub fn inference_configuration(mut self, input: crate::types::InferenceConfiguration) -> Self {
        self.inference_configuration = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifications about the inference parameters that were provided alongside the prompt. These are specified in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object that was set when the agent was created or updated. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html">Inference parameters for foundation models</a>.</p>
    pub fn set_inference_configuration(mut self, input: ::std::option::Option<crate::types::InferenceConfiguration>) -> Self {
        self.inference_configuration = input;
        self
    }
    /// <p>Specifications about the inference parameters that were provided alongside the prompt. These are specified in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object that was set when the agent was created or updated. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html">Inference parameters for foundation models</a>.</p>
    pub fn get_inference_configuration(&self) -> &::std::option::Option<crate::types::InferenceConfiguration> {
        &self.inference_configuration
    }
    /// <p>The ARN of the Lambda function to use when parsing the raw foundation model output in parts of the agent sequence.</p>
    pub fn override_lambda(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.override_lambda = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The ARN of the Lambda function to use when parsing the raw foundation model output in parts of the agent sequence.</p>
    pub fn set_override_lambda(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.override_lambda = input;
        self
    }
    /// <p>The ARN of the Lambda function to use when parsing the raw foundation model output in parts of the agent sequence.</p>
    pub fn get_override_lambda(&self) -> &::std::option::Option<::std::string::String> {
        &self.override_lambda
    }
    /// <p>Specifies whether the default prompt template was <code>OVERRIDDEN</code>. If it was, the <code>basePromptTemplate</code> that was set in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object when the agent was created or updated is used instead.</p>
    pub fn prompt_creation_mode(mut self, input: crate::types::CreationMode) -> Self {
        self.prompt_creation_mode = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies whether the default prompt template was <code>OVERRIDDEN</code>. If it was, the <code>basePromptTemplate</code> that was set in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object when the agent was created or updated is used instead.</p>
    pub fn set_prompt_creation_mode(mut self, input: ::std::option::Option<crate::types::CreationMode>) -> Self {
        self.prompt_creation_mode = input;
        self
    }
    /// <p>Specifies whether the default prompt template was <code>OVERRIDDEN</code>. If it was, the <code>basePromptTemplate</code> that was set in the <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html">PromptOverrideConfiguration</a> object when the agent was created or updated is used instead.</p>
    pub fn get_prompt_creation_mode(&self) -> &::std::option::Option<crate::types::CreationMode> {
        &self.prompt_creation_mode
    }
    /// <p>Specifies whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the <code>promptType</code>.</p>
    pub fn parser_mode(mut self, input: crate::types::CreationMode) -> Self {
        self.parser_mode = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the <code>promptType</code>.</p>
    pub fn set_parser_mode(mut self, input: ::std::option::Option<crate::types::CreationMode>) -> Self {
        self.parser_mode = input;
        self
    }
    /// <p>Specifies whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the <code>promptType</code>.</p>
    pub fn get_parser_mode(&self) -> &::std::option::Option<crate::types::CreationMode> {
        &self.parser_mode
    }
    /// Consumes the builder and constructs a [`ModelInvocationInput`](crate::types::ModelInvocationInput).
    pub fn build(self) -> crate::types::ModelInvocationInput {
        crate::types::ModelInvocationInput {
            trace_id: self.trace_id,
            text: self.text,
            r#type: self.r#type,
            inference_configuration: self.inference_configuration,
            override_lambda: self.override_lambda,
            prompt_creation_mode: self.prompt_creation_mode,
            parser_mode: self.parser_mode,
        }
    }
}
impl ::std::fmt::Debug for ModelInvocationInputBuilder {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("ModelInvocationInputBuilder");
        formatter.field("trace_id", &"*** Sensitive Data Redacted ***");
        formatter.field("text", &"*** Sensitive Data Redacted ***");
        formatter.field("r#type", &"*** Sensitive Data Redacted ***");
        formatter.field("inference_configuration", &"*** Sensitive Data Redacted ***");
        formatter.field("override_lambda", &"*** Sensitive Data Redacted ***");
        formatter.field("prompt_creation_mode", &"*** Sensitive Data Redacted ***");
        formatter.field("parser_mode", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}
