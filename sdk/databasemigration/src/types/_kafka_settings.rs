// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Provides information that describes an Apache Kafka endpoint. This information includes the output format of records applied to the endpoint and details of transaction and control table data information.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq)]
pub struct KafkaSettings  {
    /// <p>A comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form <code> <i>broker-hostname-or-ip</i>:<i>port</i> </code>. For example, <code>"ec2-12-345-678-901.compute-1.amazonaws.com:2345"</code>. For more information and examples of specifying a list of broker locations, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html">Using Apache Kafka as a target for Database Migration Service</a> in the <i>Database Migration Service User Guide</i>. </p>
    #[doc(hidden)]
    pub broker: std::option::Option<std::string::String>,
    /// <p>The topic to which you migrate the data. If you don't specify a topic, DMS specifies <code>"kafka-default-topic"</code> as the migration topic.</p>
    #[doc(hidden)]
    pub topic: std::option::Option<std::string::String>,
    /// <p>The output format for the records created on the endpoint. The message format is <code>JSON</code> (default) or <code>JSON_UNFORMATTED</code> (a single line with no tab).</p>
    #[doc(hidden)]
    pub message_format: std::option::Option<crate::types::MessageFormatValue>,
    /// <p>Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for <code>transaction_id</code>, previous <code>transaction_id</code>, and <code>transaction_record_id</code> (the record offset within a transaction). The default is <code>false</code>.</p>
    #[doc(hidden)]
    pub include_transaction_details: std::option::Option<bool>,
    /// <p>Shows the partition value within the Kafka message output unless the partition type is <code>schema-table-type</code>. The default is <code>false</code>.</p>
    #[doc(hidden)]
    pub include_partition_value: std::option::Option<bool>,
    /// <p>Prefixes schema and table names to partition values, when the partition type is <code>primary-key-type</code>. Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is <code>false</code>.</p>
    #[doc(hidden)]
    pub partition_include_schema_table: std::option::Option<bool>,
    /// <p>Includes any data definition language (DDL) operations that change the table in the control data, such as <code>rename-table</code>, <code>drop-table</code>, <code>add-column</code>, <code>drop-column</code>, and <code>rename-column</code>. The default is <code>false</code>.</p>
    #[doc(hidden)]
    pub include_table_alter_operations: std::option::Option<bool>,
    /// <p>Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is <code>false</code>.</p>
    #[doc(hidden)]
    pub include_control_details: std::option::Option<bool>,
    /// <p>The maximum size in bytes for records created on the endpoint The default is 1,000,000.</p>
    #[doc(hidden)]
    pub message_max_bytes: std::option::Option<i32>,
    /// <p>Include NULL and empty columns for records migrated to the endpoint. The default is <code>false</code>.</p>
    #[doc(hidden)]
    pub include_null_and_empty: std::option::Option<bool>,
    /// <p>Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include <code>ssl-encryption</code>, <code>ssl-authentication</code>, and <code>sasl-ssl</code>. <code>sasl-ssl</code> requires <code>SaslUsername</code> and <code>SaslPassword</code>.</p>
    #[doc(hidden)]
    pub security_protocol: std::option::Option<crate::types::KafkaSecurityProtocol>,
    /// <p>The Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.</p>
    #[doc(hidden)]
    pub ssl_client_certificate_arn: std::option::Option<std::string::String>,
    /// <p>The Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.</p>
    #[doc(hidden)]
    pub ssl_client_key_arn: std::option::Option<std::string::String>,
    /// <p> The password for the client private key used to securely connect to a Kafka target endpoint.</p>
    #[doc(hidden)]
    pub ssl_client_key_password: std::option::Option<std::string::String>,
    /// <p> The Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.</p>
    #[doc(hidden)]
    pub ssl_ca_certificate_arn: std::option::Option<std::string::String>,
    /// <p> The secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    #[doc(hidden)]
    pub sasl_username: std::option::Option<std::string::String>,
    /// <p>The secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    #[doc(hidden)]
    pub sasl_password: std::option::Option<std::string::String>,
    /// <p>Set this optional parameter to <code>true</code> to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the <code>NoHexPrefix</code> endpoint setting to enable migration of RAW data type columns without adding the '0x' prefix.</p>
    #[doc(hidden)]
    pub no_hex_prefix: std::option::Option<bool>,
}
impl KafkaSettings {
    /// <p>A comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form <code> <i>broker-hostname-or-ip</i>:<i>port</i> </code>. For example, <code>"ec2-12-345-678-901.compute-1.amazonaws.com:2345"</code>. For more information and examples of specifying a list of broker locations, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html">Using Apache Kafka as a target for Database Migration Service</a> in the <i>Database Migration Service User Guide</i>. </p>
    pub fn broker(&self) -> std::option::Option<& str> {
        self.broker.as_deref()
    }
    /// <p>The topic to which you migrate the data. If you don't specify a topic, DMS specifies <code>"kafka-default-topic"</code> as the migration topic.</p>
    pub fn topic(&self) -> std::option::Option<& str> {
        self.topic.as_deref()
    }
    /// <p>The output format for the records created on the endpoint. The message format is <code>JSON</code> (default) or <code>JSON_UNFORMATTED</code> (a single line with no tab).</p>
    pub fn message_format(&self) -> std::option::Option<& crate::types::MessageFormatValue> {
        self.message_format.as_ref()
    }
    /// <p>Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for <code>transaction_id</code>, previous <code>transaction_id</code>, and <code>transaction_record_id</code> (the record offset within a transaction). The default is <code>false</code>.</p>
    pub fn include_transaction_details(&self) -> std::option::Option<bool> {
        self.include_transaction_details
    }
    /// <p>Shows the partition value within the Kafka message output unless the partition type is <code>schema-table-type</code>. The default is <code>false</code>.</p>
    pub fn include_partition_value(&self) -> std::option::Option<bool> {
        self.include_partition_value
    }
    /// <p>Prefixes schema and table names to partition values, when the partition type is <code>primary-key-type</code>. Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is <code>false</code>.</p>
    pub fn partition_include_schema_table(&self) -> std::option::Option<bool> {
        self.partition_include_schema_table
    }
    /// <p>Includes any data definition language (DDL) operations that change the table in the control data, such as <code>rename-table</code>, <code>drop-table</code>, <code>add-column</code>, <code>drop-column</code>, and <code>rename-column</code>. The default is <code>false</code>.</p>
    pub fn include_table_alter_operations(&self) -> std::option::Option<bool> {
        self.include_table_alter_operations
    }
    /// <p>Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is <code>false</code>.</p>
    pub fn include_control_details(&self) -> std::option::Option<bool> {
        self.include_control_details
    }
    /// <p>The maximum size in bytes for records created on the endpoint The default is 1,000,000.</p>
    pub fn message_max_bytes(&self) -> std::option::Option<i32> {
        self.message_max_bytes
    }
    /// <p>Include NULL and empty columns for records migrated to the endpoint. The default is <code>false</code>.</p>
    pub fn include_null_and_empty(&self) -> std::option::Option<bool> {
        self.include_null_and_empty
    }
    /// <p>Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include <code>ssl-encryption</code>, <code>ssl-authentication</code>, and <code>sasl-ssl</code>. <code>sasl-ssl</code> requires <code>SaslUsername</code> and <code>SaslPassword</code>.</p>
    pub fn security_protocol(&self) -> std::option::Option<& crate::types::KafkaSecurityProtocol> {
        self.security_protocol.as_ref()
    }
    /// <p>The Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.</p>
    pub fn ssl_client_certificate_arn(&self) -> std::option::Option<& str> {
        self.ssl_client_certificate_arn.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.</p>
    pub fn ssl_client_key_arn(&self) -> std::option::Option<& str> {
        self.ssl_client_key_arn.as_deref()
    }
    /// <p> The password for the client private key used to securely connect to a Kafka target endpoint.</p>
    pub fn ssl_client_key_password(&self) -> std::option::Option<& str> {
        self.ssl_client_key_password.as_deref()
    }
    /// <p> The Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.</p>
    pub fn ssl_ca_certificate_arn(&self) -> std::option::Option<& str> {
        self.ssl_ca_certificate_arn.as_deref()
    }
    /// <p> The secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    pub fn sasl_username(&self) -> std::option::Option<& str> {
        self.sasl_username.as_deref()
    }
    /// <p>The secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    pub fn sasl_password(&self) -> std::option::Option<& str> {
        self.sasl_password.as_deref()
    }
    /// <p>Set this optional parameter to <code>true</code> to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the <code>NoHexPrefix</code> endpoint setting to enable migration of RAW data type columns without adding the '0x' prefix.</p>
    pub fn no_hex_prefix(&self) -> std::option::Option<bool> {
        self.no_hex_prefix
    }
}
impl  std::fmt::Debug for KafkaSettings  {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("KafkaSettings");
        formatter.field("broker", &self.broker);
        formatter.field("topic", &self.topic);
        formatter.field("message_format", &self.message_format);
        formatter.field("include_transaction_details", &self.include_transaction_details);
        formatter.field("include_partition_value", &self.include_partition_value);
        formatter.field("partition_include_schema_table", &self.partition_include_schema_table);
        formatter.field("include_table_alter_operations", &self.include_table_alter_operations);
        formatter.field("include_control_details", &self.include_control_details);
        formatter.field("message_max_bytes", &self.message_max_bytes);
        formatter.field("include_null_and_empty", &self.include_null_and_empty);
        formatter.field("security_protocol", &self.security_protocol);
        formatter.field("ssl_client_certificate_arn", &self.ssl_client_certificate_arn);
        formatter.field("ssl_client_key_arn", &self.ssl_client_key_arn);
        formatter.field("ssl_client_key_password", &"*** Sensitive Data Redacted ***");
        formatter.field("ssl_ca_certificate_arn", &self.ssl_ca_certificate_arn);
        formatter.field("sasl_username", &self.sasl_username);
        formatter.field("sasl_password", &"*** Sensitive Data Redacted ***");
        formatter.field("no_hex_prefix", &self.no_hex_prefix);
        formatter.finish()
    }
}
impl KafkaSettings {
    /// Creates a new builder-style object to manufacture [`KafkaSettings`](crate::types::KafkaSettings).
    pub fn builder() -> crate::types::builders::KafkaSettingsBuilder {
        crate::types::builders::KafkaSettingsBuilder::default()
    }
}

/// A builder for [`KafkaSettings`](crate::types::KafkaSettings).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default)]
pub struct KafkaSettingsBuilder {
    pub(crate) broker: std::option::Option<std::string::String>,
    pub(crate) topic: std::option::Option<std::string::String>,
    pub(crate) message_format: std::option::Option<crate::types::MessageFormatValue>,
    pub(crate) include_transaction_details: std::option::Option<bool>,
    pub(crate) include_partition_value: std::option::Option<bool>,
    pub(crate) partition_include_schema_table: std::option::Option<bool>,
    pub(crate) include_table_alter_operations: std::option::Option<bool>,
    pub(crate) include_control_details: std::option::Option<bool>,
    pub(crate) message_max_bytes: std::option::Option<i32>,
    pub(crate) include_null_and_empty: std::option::Option<bool>,
    pub(crate) security_protocol: std::option::Option<crate::types::KafkaSecurityProtocol>,
    pub(crate) ssl_client_certificate_arn: std::option::Option<std::string::String>,
    pub(crate) ssl_client_key_arn: std::option::Option<std::string::String>,
    pub(crate) ssl_client_key_password: std::option::Option<std::string::String>,
    pub(crate) ssl_ca_certificate_arn: std::option::Option<std::string::String>,
    pub(crate) sasl_username: std::option::Option<std::string::String>,
    pub(crate) sasl_password: std::option::Option<std::string::String>,
    pub(crate) no_hex_prefix: std::option::Option<bool>,
}
impl KafkaSettingsBuilder {
    /// <p>A comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form <code> <i>broker-hostname-or-ip</i>:<i>port</i> </code>. For example, <code>"ec2-12-345-678-901.compute-1.amazonaws.com:2345"</code>. For more information and examples of specifying a list of broker locations, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html">Using Apache Kafka as a target for Database Migration Service</a> in the <i>Database Migration Service User Guide</i>. </p>
    pub fn broker(mut self, input: impl Into<std::string::String>) -> Self {
        self.broker = Some(input.into());
        self
    }
    /// <p>A comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form <code> <i>broker-hostname-or-ip</i>:<i>port</i> </code>. For example, <code>"ec2-12-345-678-901.compute-1.amazonaws.com:2345"</code>. For more information and examples of specifying a list of broker locations, see <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html">Using Apache Kafka as a target for Database Migration Service</a> in the <i>Database Migration Service User Guide</i>. </p>
    pub fn set_broker(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.broker = input; self
    }
    /// <p>The topic to which you migrate the data. If you don't specify a topic, DMS specifies <code>"kafka-default-topic"</code> as the migration topic.</p>
    pub fn topic(mut self, input: impl Into<std::string::String>) -> Self {
        self.topic = Some(input.into());
        self
    }
    /// <p>The topic to which you migrate the data. If you don't specify a topic, DMS specifies <code>"kafka-default-topic"</code> as the migration topic.</p>
    pub fn set_topic(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.topic = input; self
    }
    /// <p>The output format for the records created on the endpoint. The message format is <code>JSON</code> (default) or <code>JSON_UNFORMATTED</code> (a single line with no tab).</p>
    pub fn message_format(mut self, input: crate::types::MessageFormatValue) -> Self {
        self.message_format = Some(input);
        self
    }
    /// <p>The output format for the records created on the endpoint. The message format is <code>JSON</code> (default) or <code>JSON_UNFORMATTED</code> (a single line with no tab).</p>
    pub fn set_message_format(mut self, input: std::option::Option<crate::types::MessageFormatValue>) -> Self {
        self.message_format = input; self
    }
    /// <p>Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for <code>transaction_id</code>, previous <code>transaction_id</code>, and <code>transaction_record_id</code> (the record offset within a transaction). The default is <code>false</code>.</p>
    pub fn include_transaction_details(mut self, input: bool) -> Self {
        self.include_transaction_details = Some(input);
        self
    }
    /// <p>Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for <code>transaction_id</code>, previous <code>transaction_id</code>, and <code>transaction_record_id</code> (the record offset within a transaction). The default is <code>false</code>.</p>
    pub fn set_include_transaction_details(mut self, input: std::option::Option<bool>) -> Self {
        self.include_transaction_details = input; self
    }
    /// <p>Shows the partition value within the Kafka message output unless the partition type is <code>schema-table-type</code>. The default is <code>false</code>.</p>
    pub fn include_partition_value(mut self, input: bool) -> Self {
        self.include_partition_value = Some(input);
        self
    }
    /// <p>Shows the partition value within the Kafka message output unless the partition type is <code>schema-table-type</code>. The default is <code>false</code>.</p>
    pub fn set_include_partition_value(mut self, input: std::option::Option<bool>) -> Self {
        self.include_partition_value = input; self
    }
    /// <p>Prefixes schema and table names to partition values, when the partition type is <code>primary-key-type</code>. Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is <code>false</code>.</p>
    pub fn partition_include_schema_table(mut self, input: bool) -> Self {
        self.partition_include_schema_table = Some(input);
        self
    }
    /// <p>Prefixes schema and table names to partition values, when the partition type is <code>primary-key-type</code>. Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is <code>false</code>.</p>
    pub fn set_partition_include_schema_table(mut self, input: std::option::Option<bool>) -> Self {
        self.partition_include_schema_table = input; self
    }
    /// <p>Includes any data definition language (DDL) operations that change the table in the control data, such as <code>rename-table</code>, <code>drop-table</code>, <code>add-column</code>, <code>drop-column</code>, and <code>rename-column</code>. The default is <code>false</code>.</p>
    pub fn include_table_alter_operations(mut self, input: bool) -> Self {
        self.include_table_alter_operations = Some(input);
        self
    }
    /// <p>Includes any data definition language (DDL) operations that change the table in the control data, such as <code>rename-table</code>, <code>drop-table</code>, <code>add-column</code>, <code>drop-column</code>, and <code>rename-column</code>. The default is <code>false</code>.</p>
    pub fn set_include_table_alter_operations(mut self, input: std::option::Option<bool>) -> Self {
        self.include_table_alter_operations = input; self
    }
    /// <p>Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is <code>false</code>.</p>
    pub fn include_control_details(mut self, input: bool) -> Self {
        self.include_control_details = Some(input);
        self
    }
    /// <p>Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is <code>false</code>.</p>
    pub fn set_include_control_details(mut self, input: std::option::Option<bool>) -> Self {
        self.include_control_details = input; self
    }
    /// <p>The maximum size in bytes for records created on the endpoint The default is 1,000,000.</p>
    pub fn message_max_bytes(mut self, input: i32) -> Self {
        self.message_max_bytes = Some(input);
        self
    }
    /// <p>The maximum size in bytes for records created on the endpoint The default is 1,000,000.</p>
    pub fn set_message_max_bytes(mut self, input: std::option::Option<i32>) -> Self {
        self.message_max_bytes = input; self
    }
    /// <p>Include NULL and empty columns for records migrated to the endpoint. The default is <code>false</code>.</p>
    pub fn include_null_and_empty(mut self, input: bool) -> Self {
        self.include_null_and_empty = Some(input);
        self
    }
    /// <p>Include NULL and empty columns for records migrated to the endpoint. The default is <code>false</code>.</p>
    pub fn set_include_null_and_empty(mut self, input: std::option::Option<bool>) -> Self {
        self.include_null_and_empty = input; self
    }
    /// <p>Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include <code>ssl-encryption</code>, <code>ssl-authentication</code>, and <code>sasl-ssl</code>. <code>sasl-ssl</code> requires <code>SaslUsername</code> and <code>SaslPassword</code>.</p>
    pub fn security_protocol(mut self, input: crate::types::KafkaSecurityProtocol) -> Self {
        self.security_protocol = Some(input);
        self
    }
    /// <p>Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include <code>ssl-encryption</code>, <code>ssl-authentication</code>, and <code>sasl-ssl</code>. <code>sasl-ssl</code> requires <code>SaslUsername</code> and <code>SaslPassword</code>.</p>
    pub fn set_security_protocol(mut self, input: std::option::Option<crate::types::KafkaSecurityProtocol>) -> Self {
        self.security_protocol = input; self
    }
    /// <p>The Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.</p>
    pub fn ssl_client_certificate_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.ssl_client_certificate_arn = Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.</p>
    pub fn set_ssl_client_certificate_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.ssl_client_certificate_arn = input; self
    }
    /// <p>The Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.</p>
    pub fn ssl_client_key_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.ssl_client_key_arn = Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.</p>
    pub fn set_ssl_client_key_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.ssl_client_key_arn = input; self
    }
    /// <p> The password for the client private key used to securely connect to a Kafka target endpoint.</p>
    pub fn ssl_client_key_password(mut self, input: impl Into<std::string::String>) -> Self {
        self.ssl_client_key_password = Some(input.into());
        self
    }
    /// <p> The password for the client private key used to securely connect to a Kafka target endpoint.</p>
    pub fn set_ssl_client_key_password(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.ssl_client_key_password = input; self
    }
    /// <p> The Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.</p>
    pub fn ssl_ca_certificate_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.ssl_ca_certificate_arn = Some(input.into());
        self
    }
    /// <p> The Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.</p>
    pub fn set_ssl_ca_certificate_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.ssl_ca_certificate_arn = input; self
    }
    /// <p> The secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    pub fn sasl_username(mut self, input: impl Into<std::string::String>) -> Self {
        self.sasl_username = Some(input.into());
        self
    }
    /// <p> The secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    pub fn set_sasl_username(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.sasl_username = input; self
    }
    /// <p>The secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    pub fn sasl_password(mut self, input: impl Into<std::string::String>) -> Self {
        self.sasl_password = Some(input.into());
        self
    }
    /// <p>The secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.</p>
    pub fn set_sasl_password(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.sasl_password = input; self
    }
    /// <p>Set this optional parameter to <code>true</code> to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the <code>NoHexPrefix</code> endpoint setting to enable migration of RAW data type columns without adding the '0x' prefix.</p>
    pub fn no_hex_prefix(mut self, input: bool) -> Self {
        self.no_hex_prefix = Some(input);
        self
    }
    /// <p>Set this optional parameter to <code>true</code> to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the <code>NoHexPrefix</code> endpoint setting to enable migration of RAW data type columns without adding the '0x' prefix.</p>
    pub fn set_no_hex_prefix(mut self, input: std::option::Option<bool>) -> Self {
        self.no_hex_prefix = input; self
    }
    /// Consumes the builder and constructs a [`KafkaSettings`](crate::types::KafkaSettings).
    pub fn build(self) -> crate::types::KafkaSettings {
        crate::types::KafkaSettings {
            broker: self.broker
            ,
            topic: self.topic
            ,
            message_format: self.message_format
            ,
            include_transaction_details: self.include_transaction_details
            ,
            include_partition_value: self.include_partition_value
            ,
            partition_include_schema_table: self.partition_include_schema_table
            ,
            include_table_alter_operations: self.include_table_alter_operations
            ,
            include_control_details: self.include_control_details
            ,
            message_max_bytes: self.message_max_bytes
            ,
            include_null_and_empty: self.include_null_and_empty
            ,
            security_protocol: self.security_protocol
            ,
            ssl_client_certificate_arn: self.ssl_client_certificate_arn
            ,
            ssl_client_key_arn: self.ssl_client_key_arn
            ,
            ssl_client_key_password: self.ssl_client_key_password
            ,
            ssl_ca_certificate_arn: self.ssl_ca_certificate_arn
            ,
            sasl_username: self.sasl_username
            ,
            sasl_password: self.sasl_password
            ,
            no_hex_prefix: self.no_hex_prefix
            ,
        }
    }
}
impl std::fmt::Debug for KafkaSettingsBuilder {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut formatter = f.debug_struct("KafkaSettingsBuilder");
        formatter.field("broker", &self.broker);
        formatter.field("topic", &self.topic);
        formatter.field("message_format", &self.message_format);
        formatter.field("include_transaction_details", &self.include_transaction_details);
        formatter.field("include_partition_value", &self.include_partition_value);
        formatter.field("partition_include_schema_table", &self.partition_include_schema_table);
        formatter.field("include_table_alter_operations", &self.include_table_alter_operations);
        formatter.field("include_control_details", &self.include_control_details);
        formatter.field("message_max_bytes", &self.message_max_bytes);
        formatter.field("include_null_and_empty", &self.include_null_and_empty);
        formatter.field("security_protocol", &self.security_protocol);
        formatter.field("ssl_client_certificate_arn", &self.ssl_client_certificate_arn);
        formatter.field("ssl_client_key_arn", &self.ssl_client_key_arn);
        formatter.field("ssl_client_key_password", &"*** Sensitive Data Redacted ***");
        formatter.field("ssl_ca_certificate_arn", &self.ssl_ca_certificate_arn);
        formatter.field("sasl_username", &self.sasl_username);
        formatter.field("sasl_password", &"*** Sensitive Data Redacted ***");
        formatter.field("no_hex_prefix", &self.no_hex_prefix);
        formatter.finish()
    }
}

