// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
pub use crate::operation::start_ml_data_processing_job::_start_ml_data_processing_job_output::StartMlDataProcessingJobOutputBuilder;

pub use crate::operation::start_ml_data_processing_job::_start_ml_data_processing_job_input::StartMlDataProcessingJobInputBuilder;

impl crate::operation::start_ml_data_processing_job::builders::StartMlDataProcessingJobInputBuilder {
    /// Sends a request with this input using the given client.
    pub async fn send_with(
        self,
        client: &crate::Client,
    ) -> ::std::result::Result<
        crate::operation::start_ml_data_processing_job::StartMlDataProcessingJobOutput,
        ::aws_smithy_runtime_api::client::result::SdkError<
            crate::operation::start_ml_data_processing_job::StartMLDataProcessingJobError,
            ::aws_smithy_runtime_api::client::orchestrator::HttpResponse,
        >,
    > {
        let mut fluent_builder = client.start_ml_data_processing_job();
        fluent_builder.inner = self;
        fluent_builder.send().await
    }
}
/// Fluent builder constructing a request to `StartMLDataProcessingJob`.
///
/// <p>Creates a new Neptune ML data processing job for processing the graph data exported from Neptune for training. See <a href="https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-api-dataprocessing.html">The <code>dataprocessing</code> command</a>.</p>
/// <p>When invoking this operation in a Neptune cluster that has IAM authentication enabled, the IAM user or role making the request must have a policy attached that allows the <a href="https://docs.aws.amazon.com/neptune/latest/userguide/iam-dp-actions.html#startmlmodeldataprocessingjob">neptune-db:StartMLModelDataProcessingJob</a> IAM action in that cluster.</p>
#[derive(::std::clone::Clone, ::std::fmt::Debug)]
pub struct StartMLDataProcessingJobFluentBuilder {
    handle: ::std::sync::Arc<crate::client::Handle>,
    inner: crate::operation::start_ml_data_processing_job::builders::StartMlDataProcessingJobInputBuilder,
    config_override: ::std::option::Option<crate::config::Builder>,
}
impl
    crate::client::customize::internal::CustomizableSend<
        crate::operation::start_ml_data_processing_job::StartMlDataProcessingJobOutput,
        crate::operation::start_ml_data_processing_job::StartMLDataProcessingJobError,
    > for StartMLDataProcessingJobFluentBuilder
{
    fn send(
        self,
        config_override: crate::config::Builder,
    ) -> crate::client::customize::internal::BoxFuture<
        crate::client::customize::internal::SendResult<
            crate::operation::start_ml_data_processing_job::StartMlDataProcessingJobOutput,
            crate::operation::start_ml_data_processing_job::StartMLDataProcessingJobError,
        >,
    > {
        ::std::boxed::Box::pin(async move { self.config_override(config_override).send().await })
    }
}
impl StartMLDataProcessingJobFluentBuilder {
    /// Creates a new `StartMLDataProcessingJob`.
    pub(crate) fn new(handle: ::std::sync::Arc<crate::client::Handle>) -> Self {
        Self {
            handle,
            inner: ::std::default::Default::default(),
            config_override: ::std::option::Option::None,
        }
    }
    /// Access the StartMLDataProcessingJob as a reference.
    pub fn as_input(&self) -> &crate::operation::start_ml_data_processing_job::builders::StartMlDataProcessingJobInputBuilder {
        &self.inner
    }
    /// Sends the request and returns the response.
    ///
    /// If an error occurs, an `SdkError` will be returned with additional details that
    /// can be matched against.
    ///
    /// By default, any retryable failures will be retried twice. Retry behavior
    /// is configurable with the [RetryConfig](aws_smithy_types::retry::RetryConfig), which can be
    /// set when configuring the client.
    pub async fn send(
        self,
    ) -> ::std::result::Result<
        crate::operation::start_ml_data_processing_job::StartMlDataProcessingJobOutput,
        ::aws_smithy_runtime_api::client::result::SdkError<
            crate::operation::start_ml_data_processing_job::StartMLDataProcessingJobError,
            ::aws_smithy_runtime_api::client::orchestrator::HttpResponse,
        >,
    > {
        let input = self
            .inner
            .build()
            .map_err(::aws_smithy_runtime_api::client::result::SdkError::construction_failure)?;
        let runtime_plugins = crate::operation::start_ml_data_processing_job::StartMLDataProcessingJob::operation_runtime_plugins(
            self.handle.runtime_plugins.clone(),
            &self.handle.conf,
            self.config_override,
        );
        crate::operation::start_ml_data_processing_job::StartMLDataProcessingJob::orchestrate(&runtime_plugins, input).await
    }

    /// Consumes this builder, creating a customizable operation that can be modified before being sent.
    pub fn customize(
        self,
    ) -> crate::client::customize::CustomizableOperation<
        crate::operation::start_ml_data_processing_job::StartMlDataProcessingJobOutput,
        crate::operation::start_ml_data_processing_job::StartMLDataProcessingJobError,
        Self,
    > {
        crate::client::customize::CustomizableOperation::new(self)
    }
    pub(crate) fn config_override(mut self, config_override: impl ::std::convert::Into<crate::config::Builder>) -> Self {
        self.set_config_override(::std::option::Option::Some(config_override.into()));
        self
    }

    pub(crate) fn set_config_override(&mut self, config_override: ::std::option::Option<crate::config::Builder>) -> &mut Self {
        self.config_override = config_override;
        self
    }
    /// <p>A unique identifier for the new job. The default is an autogenerated UUID.</p>
    pub fn id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.id(input.into());
        self
    }
    /// <p>A unique identifier for the new job. The default is an autogenerated UUID.</p>
    pub fn set_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_id(input);
        self
    }
    /// <p>A unique identifier for the new job. The default is an autogenerated UUID.</p>
    pub fn get_id(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_id()
    }
    /// <p>The job ID of a completed data processing job run on an earlier version of the data.</p>
    pub fn previous_data_processing_job_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.previous_data_processing_job_id(input.into());
        self
    }
    /// <p>The job ID of a completed data processing job run on an earlier version of the data.</p>
    pub fn set_previous_data_processing_job_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_previous_data_processing_job_id(input);
        self
    }
    /// <p>The job ID of a completed data processing job run on an earlier version of the data.</p>
    pub fn get_previous_data_processing_job_id(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_previous_data_processing_job_id()
    }
    /// <p>The URI of the Amazon S3 location where you want SageMaker to download the data needed to run the data processing job.</p>
    pub fn input_data_s3_location(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.input_data_s3_location(input.into());
        self
    }
    /// <p>The URI of the Amazon S3 location where you want SageMaker to download the data needed to run the data processing job.</p>
    pub fn set_input_data_s3_location(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_input_data_s3_location(input);
        self
    }
    /// <p>The URI of the Amazon S3 location where you want SageMaker to download the data needed to run the data processing job.</p>
    pub fn get_input_data_s3_location(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_input_data_s3_location()
    }
    /// <p>The URI of the Amazon S3 location where you want SageMaker to save the results of a data processing job.</p>
    pub fn processed_data_s3_location(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.processed_data_s3_location(input.into());
        self
    }
    /// <p>The URI of the Amazon S3 location where you want SageMaker to save the results of a data processing job.</p>
    pub fn set_processed_data_s3_location(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_processed_data_s3_location(input);
        self
    }
    /// <p>The URI of the Amazon S3 location where you want SageMaker to save the results of a data processing job.</p>
    pub fn get_processed_data_s3_location(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_processed_data_s3_location()
    }
    /// <p>The ARN of an IAM role for SageMaker execution. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn sagemaker_iam_role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.sagemaker_iam_role_arn(input.into());
        self
    }
    /// <p>The ARN of an IAM role for SageMaker execution. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn set_sagemaker_iam_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_sagemaker_iam_role_arn(input);
        self
    }
    /// <p>The ARN of an IAM role for SageMaker execution. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn get_sagemaker_iam_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_sagemaker_iam_role_arn()
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that SageMaker can assume to perform tasks on your behalf. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn neptune_iam_role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.neptune_iam_role_arn(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that SageMaker can assume to perform tasks on your behalf. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn set_neptune_iam_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_neptune_iam_role_arn(input);
        self
    }
    /// <p>The Amazon Resource Name (ARN) of an IAM role that SageMaker can assume to perform tasks on your behalf. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn get_neptune_iam_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_neptune_iam_role_arn()
    }
    /// <p>The type of ML instance used during data processing. Its memory should be large enough to hold the processed dataset. The default is the smallest ml.r5 type whose memory is ten times larger than the size of the exported graph data on disk.</p>
    pub fn processing_instance_type(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.processing_instance_type(input.into());
        self
    }
    /// <p>The type of ML instance used during data processing. Its memory should be large enough to hold the processed dataset. The default is the smallest ml.r5 type whose memory is ten times larger than the size of the exported graph data on disk.</p>
    pub fn set_processing_instance_type(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_processing_instance_type(input);
        self
    }
    /// <p>The type of ML instance used during data processing. Its memory should be large enough to hold the processed dataset. The default is the smallest ml.r5 type whose memory is ten times larger than the size of the exported graph data on disk.</p>
    pub fn get_processing_instance_type(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_processing_instance_type()
    }
    /// <p>The disk volume size of the processing instance. Both input data and processed data are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML chooses the volume size automatically based on the data size.</p>
    pub fn processing_instance_volume_size_in_gb(mut self, input: i32) -> Self {
        self.inner = self.inner.processing_instance_volume_size_in_gb(input);
        self
    }
    /// <p>The disk volume size of the processing instance. Both input data and processed data are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML chooses the volume size automatically based on the data size.</p>
    pub fn set_processing_instance_volume_size_in_gb(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_processing_instance_volume_size_in_gb(input);
        self
    }
    /// <p>The disk volume size of the processing instance. Both input data and processed data are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML chooses the volume size automatically based on the data size.</p>
    pub fn get_processing_instance_volume_size_in_gb(&self) -> &::std::option::Option<i32> {
        self.inner.get_processing_instance_volume_size_in_gb()
    }
    /// <p>Timeout in seconds for the data processing job. The default is 86,400 (1 day).</p>
    pub fn processing_time_out_in_seconds(mut self, input: i32) -> Self {
        self.inner = self.inner.processing_time_out_in_seconds(input);
        self
    }
    /// <p>Timeout in seconds for the data processing job. The default is 86,400 (1 day).</p>
    pub fn set_processing_time_out_in_seconds(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_processing_time_out_in_seconds(input);
        self
    }
    /// <p>Timeout in seconds for the data processing job. The default is 86,400 (1 day).</p>
    pub fn get_processing_time_out_in_seconds(&self) -> &::std::option::Option<i32> {
        self.inner.get_processing_time_out_in_seconds()
    }
    /// <p>One of the two model types that Neptune ML currently supports: heterogeneous graph models (<code>heterogeneous</code>), and knowledge graph (<code>kge</code>). The default is none. If not specified, Neptune ML chooses the model type automatically based on the data.</p>
    pub fn model_type(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.model_type(input.into());
        self
    }
    /// <p>One of the two model types that Neptune ML currently supports: heterogeneous graph models (<code>heterogeneous</code>), and knowledge graph (<code>kge</code>). The default is none. If not specified, Neptune ML chooses the model type automatically based on the data.</p>
    pub fn set_model_type(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_model_type(input);
        self
    }
    /// <p>One of the two model types that Neptune ML currently supports: heterogeneous graph models (<code>heterogeneous</code>), and knowledge graph (<code>kge</code>). The default is none. If not specified, Neptune ML chooses the model type automatically based on the data.</p>
    pub fn get_model_type(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_model_type()
    }
    /// <p>A data specification file that describes how to load the exported graph data for training. The file is automatically generated by the Neptune export toolkit. The default is <code>training-data-configuration.json</code>.</p>
    pub fn config_file_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.config_file_name(input.into());
        self
    }
    /// <p>A data specification file that describes how to load the exported graph data for training. The file is automatically generated by the Neptune export toolkit. The default is <code>training-data-configuration.json</code>.</p>
    pub fn set_config_file_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_config_file_name(input);
        self
    }
    /// <p>A data specification file that describes how to load the exported graph data for training. The file is automatically generated by the Neptune export toolkit. The default is <code>training-data-configuration.json</code>.</p>
    pub fn get_config_file_name(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_config_file_name()
    }
    ///
    /// Appends an item to `subnets`.
    ///
    /// To override the contents of this collection use [`set_subnets`](Self::set_subnets).
    ///
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn subnets(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.subnets(input.into());
        self
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn set_subnets(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.inner = self.inner.set_subnets(input);
        self
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn get_subnets(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        self.inner.get_subnets()
    }
    ///
    /// Appends an item to `securityGroupIds`.
    ///
    /// To override the contents of this collection use [`set_security_group_ids`](Self::set_security_group_ids).
    ///
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn security_group_ids(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.security_group_ids(input.into());
        self
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn set_security_group_ids(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.inner = self.inner.set_security_group_ids(input);
        self
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn get_security_group_ids(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        self.inner.get_security_group_ids()
    }
    /// <p>The Amazon Key Management Service (Amazon KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn volume_encryption_kms_key(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.volume_encryption_kms_key(input.into());
        self
    }
    /// <p>The Amazon Key Management Service (Amazon KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn set_volume_encryption_kms_key(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_volume_encryption_kms_key(input);
        self
    }
    /// <p>The Amazon Key Management Service (Amazon KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn get_volume_encryption_kms_key(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_volume_encryption_kms_key()
    }
    /// <p>The Amazon Key Management Service (Amazon KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn s3_output_encryption_kms_key(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.s3_output_encryption_kms_key(input.into());
        self
    }
    /// <p>The Amazon Key Management Service (Amazon KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn set_s3_output_encryption_kms_key(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_s3_output_encryption_kms_key(input);
        self
    }
    /// <p>The Amazon Key Management Service (Amazon KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn get_s3_output_encryption_kms_key(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_s3_output_encryption_kms_key()
    }
}
