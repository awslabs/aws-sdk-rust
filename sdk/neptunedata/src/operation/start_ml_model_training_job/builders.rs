// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
pub use crate::operation::start_ml_model_training_job::_start_ml_model_training_job_output::StartMlModelTrainingJobOutputBuilder;

pub use crate::operation::start_ml_model_training_job::_start_ml_model_training_job_input::StartMlModelTrainingJobInputBuilder;

impl StartMlModelTrainingJobInputBuilder {
    /// Sends a request with this input using the given client.
    pub async fn send_with(
        self,
        client: &crate::Client,
    ) -> ::std::result::Result<
        crate::operation::start_ml_model_training_job::StartMlModelTrainingJobOutput,
        ::aws_smithy_runtime_api::client::result::SdkError<
            crate::operation::start_ml_model_training_job::StartMLModelTrainingJobError,
            ::aws_smithy_runtime_api::client::orchestrator::HttpResponse,
        >,
    > {
        let mut fluent_builder = client.start_ml_model_training_job();
        fluent_builder.inner = self;
        fluent_builder.send().await
    }
}
/// Fluent builder constructing a request to `StartMLModelTrainingJob`.
///
/// <p>Creates a new Neptune ML model training job. See <a href="https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-api-modeltraining.html">Model training using the <code>modeltraining</code> command</a>.</p>
/// <p>When invoking this operation in a Neptune cluster that has IAM authentication enabled, the IAM user or role making the request must have a policy attached that allows the <a href="https://docs.aws.amazon.com/neptune/latest/userguide/iam-dp-actions.html#startmlmodeltrainingjob">neptune-db:StartMLModelTrainingJob</a> IAM action in that cluster.</p>
#[derive(::std::clone::Clone, ::std::fmt::Debug)]
pub struct StartMLModelTrainingJobFluentBuilder {
    handle: ::std::sync::Arc<crate::client::Handle>,
    inner: crate::operation::start_ml_model_training_job::builders::StartMlModelTrainingJobInputBuilder,
    config_override: ::std::option::Option<crate::config::Builder>,
}
impl
    crate::client::customize::internal::CustomizableSend<
        crate::operation::start_ml_model_training_job::StartMlModelTrainingJobOutput,
        crate::operation::start_ml_model_training_job::StartMLModelTrainingJobError,
    > for StartMLModelTrainingJobFluentBuilder
{
    fn send(
        self,
        config_override: crate::config::Builder,
    ) -> crate::client::customize::internal::BoxFuture<
        crate::client::customize::internal::SendResult<
            crate::operation::start_ml_model_training_job::StartMlModelTrainingJobOutput,
            crate::operation::start_ml_model_training_job::StartMLModelTrainingJobError,
        >,
    > {
        ::std::boxed::Box::pin(async move { self.config_override(config_override).send().await })
    }
}
impl StartMLModelTrainingJobFluentBuilder {
    /// Creates a new `StartMLModelTrainingJob`.
    pub(crate) fn new(handle: ::std::sync::Arc<crate::client::Handle>) -> Self {
        Self {
            handle,
            inner: ::std::default::Default::default(),
            config_override: ::std::option::Option::None,
        }
    }
    /// Access the StartMLModelTrainingJob as a reference.
    pub fn as_input(&self) -> &crate::operation::start_ml_model_training_job::builders::StartMlModelTrainingJobInputBuilder {
        &self.inner
    }
    /// Sends the request and returns the response.
    ///
    /// If an error occurs, an `SdkError` will be returned with additional details that
    /// can be matched against.
    ///
    /// By default, any retryable failures will be retried twice. Retry behavior
    /// is configurable with the [RetryConfig](aws_smithy_types::retry::RetryConfig), which can be
    /// set when configuring the client.
    pub async fn send(
        self,
    ) -> ::std::result::Result<
        crate::operation::start_ml_model_training_job::StartMlModelTrainingJobOutput,
        ::aws_smithy_runtime_api::client::result::SdkError<
            crate::operation::start_ml_model_training_job::StartMLModelTrainingJobError,
            ::aws_smithy_runtime_api::client::orchestrator::HttpResponse,
        >,
    > {
        let input = self
            .inner
            .build()
            .map_err(::aws_smithy_runtime_api::client::result::SdkError::construction_failure)?;
        let runtime_plugins = crate::operation::start_ml_model_training_job::StartMLModelTrainingJob::operation_runtime_plugins(
            self.handle.runtime_plugins.clone(),
            &self.handle.conf,
            self.config_override,
        );
        crate::operation::start_ml_model_training_job::StartMLModelTrainingJob::orchestrate(&runtime_plugins, input).await
    }

    /// Consumes this builder, creating a customizable operation that can be modified before being sent.
    pub fn customize(
        self,
    ) -> crate::client::customize::CustomizableOperation<
        crate::operation::start_ml_model_training_job::StartMlModelTrainingJobOutput,
        crate::operation::start_ml_model_training_job::StartMLModelTrainingJobError,
        Self,
    > {
        crate::client::customize::CustomizableOperation::new(self)
    }
    pub(crate) fn config_override(mut self, config_override: impl Into<crate::config::Builder>) -> Self {
        self.set_config_override(Some(config_override.into()));
        self
    }

    pub(crate) fn set_config_override(&mut self, config_override: Option<crate::config::Builder>) -> &mut Self {
        self.config_override = config_override;
        self
    }
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.id(input.into());
        self
    }
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn set_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_id(input);
        self
    }
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn get_id(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_id()
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn previous_model_training_job_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.previous_model_training_job_id(input.into());
        self
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn set_previous_model_training_job_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_previous_model_training_job_id(input);
        self
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn get_previous_model_training_job_id(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_previous_model_training_job_id()
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub fn data_processing_job_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.data_processing_job_id(input.into());
        self
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub fn set_data_processing_job_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_data_processing_job_id(input);
        self
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub fn get_data_processing_job_id(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_data_processing_job_id()
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub fn train_model_s3_location(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.train_model_s3_location(input.into());
        self
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub fn set_train_model_s3_location(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_train_model_s3_location(input);
        self
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub fn get_train_model_s3_location(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_train_model_s3_location()
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn sagemaker_iam_role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.sagemaker_iam_role_arn(input.into());
        self
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn set_sagemaker_iam_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_sagemaker_iam_role_arn(input);
        self
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn get_sagemaker_iam_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_sagemaker_iam_role_arn()
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn neptune_iam_role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.neptune_iam_role_arn(input.into());
        self
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn set_neptune_iam_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_neptune_iam_role_arn(input);
        self
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn get_neptune_iam_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_neptune_iam_role_arn()
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn base_processing_instance_type(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.base_processing_instance_type(input.into());
        self
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn set_base_processing_instance_type(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_base_processing_instance_type(input);
        self
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn get_base_processing_instance_type(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_base_processing_instance_type()
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn training_instance_type(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.training_instance_type(input.into());
        self
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn set_training_instance_type(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_training_instance_type(input);
        self
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn get_training_instance_type(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_training_instance_type()
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn training_instance_volume_size_in_gb(mut self, input: i32) -> Self {
        self.inner = self.inner.training_instance_volume_size_in_gb(input);
        self
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn set_training_instance_volume_size_in_gb(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_training_instance_volume_size_in_gb(input);
        self
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn get_training_instance_volume_size_in_gb(&self) -> &::std::option::Option<i32> {
        self.inner.get_training_instance_volume_size_in_gb()
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn training_time_out_in_seconds(mut self, input: i32) -> Self {
        self.inner = self.inner.training_time_out_in_seconds(input);
        self
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn set_training_time_out_in_seconds(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_training_time_out_in_seconds(input);
        self
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn get_training_time_out_in_seconds(&self) -> &::std::option::Option<i32> {
        self.inner.get_training_time_out_in_seconds()
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn max_hpo_number_of_training_jobs(mut self, input: i32) -> Self {
        self.inner = self.inner.max_hpo_number_of_training_jobs(input);
        self
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn set_max_hpo_number_of_training_jobs(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_max_hpo_number_of_training_jobs(input);
        self
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn get_max_hpo_number_of_training_jobs(&self) -> &::std::option::Option<i32> {
        self.inner.get_max_hpo_number_of_training_jobs()
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn max_hpo_parallel_training_jobs(mut self, input: i32) -> Self {
        self.inner = self.inner.max_hpo_parallel_training_jobs(input);
        self
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn set_max_hpo_parallel_training_jobs(mut self, input: ::std::option::Option<i32>) -> Self {
        self.inner = self.inner.set_max_hpo_parallel_training_jobs(input);
        self
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn get_max_hpo_parallel_training_jobs(&self) -> &::std::option::Option<i32> {
        self.inner.get_max_hpo_parallel_training_jobs()
    }
    /// Appends an item to `subnets`.
    ///
    /// To override the contents of this collection use [`set_subnets`](Self::set_subnets).
    ///
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn subnets(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.subnets(input.into());
        self
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn set_subnets(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.inner = self.inner.set_subnets(input);
        self
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn get_subnets(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        self.inner.get_subnets()
    }
    /// Appends an item to `securityGroupIds`.
    ///
    /// To override the contents of this collection use [`set_security_group_ids`](Self::set_security_group_ids).
    ///
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn security_group_ids(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.security_group_ids(input.into());
        self
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn set_security_group_ids(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.inner = self.inner.set_security_group_ids(input);
        self
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn get_security_group_ids(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        self.inner.get_security_group_ids()
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn volume_encryption_kms_key(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.volume_encryption_kms_key(input.into());
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn set_volume_encryption_kms_key(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_volume_encryption_kms_key(input);
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn get_volume_encryption_kms_key(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_volume_encryption_kms_key()
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn s3_output_encryption_kms_key(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inner = self.inner.s3_output_encryption_kms_key(input.into());
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn set_s3_output_encryption_kms_key(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inner = self.inner.set_s3_output_encryption_kms_key(input);
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn get_s3_output_encryption_kms_key(&self) -> &::std::option::Option<::std::string::String> {
        self.inner.get_s3_output_encryption_kms_key()
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn enable_managed_spot_training(mut self, input: bool) -> Self {
        self.inner = self.inner.enable_managed_spot_training(input);
        self
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn set_enable_managed_spot_training(mut self, input: ::std::option::Option<bool>) -> Self {
        self.inner = self.inner.set_enable_managed_spot_training(input);
        self
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn get_enable_managed_spot_training(&self) -> &::std::option::Option<bool> {
        self.inner.get_enable_managed_spot_training()
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn custom_model_training_parameters(mut self, input: crate::types::CustomModelTrainingParameters) -> Self {
        self.inner = self.inner.custom_model_training_parameters(input);
        self
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn set_custom_model_training_parameters(mut self, input: ::std::option::Option<crate::types::CustomModelTrainingParameters>) -> Self {
        self.inner = self.inner.set_custom_model_training_parameters(input);
        self
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn get_custom_model_training_parameters(&self) -> &::std::option::Option<crate::types::CustomModelTrainingParameters> {
        self.inner.get_custom_model_training_parameters()
    }
}
