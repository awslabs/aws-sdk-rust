// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct StartMlModelTrainingJobInput {
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub id: ::std::option::Option<::std::string::String>,
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub previous_model_training_job_id: ::std::option::Option<::std::string::String>,
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub data_processing_job_id: ::std::option::Option<::std::string::String>,
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub train_model_s3_location: ::std::option::Option<::std::string::String>,
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub sagemaker_iam_role_arn: ::std::option::Option<::std::string::String>,
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub neptune_iam_role_arn: ::std::option::Option<::std::string::String>,
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub base_processing_instance_type: ::std::option::Option<::std::string::String>,
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub training_instance_type: ::std::option::Option<::std::string::String>,
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub training_instance_volume_size_in_gb: ::std::option::Option<i32>,
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub training_time_out_in_seconds: ::std::option::Option<i32>,
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub max_hpo_number_of_training_jobs: ::std::option::Option<i32>,
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub max_hpo_parallel_training_jobs: ::std::option::Option<i32>,
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub subnets: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    /// <p>The VPC security group IDs. The default is None.</p>
    pub security_group_ids: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub volume_encryption_kms_key: ::std::option::Option<::std::string::String>,
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub s3_output_encryption_kms_key: ::std::option::Option<::std::string::String>,
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub enable_managed_spot_training: ::std::option::Option<bool>,
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub custom_model_training_parameters: ::std::option::Option<crate::types::CustomModelTrainingParameters>,
}
impl StartMlModelTrainingJobInput {
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn id(&self) -> ::std::option::Option<&str> {
        self.id.as_deref()
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn previous_model_training_job_id(&self) -> ::std::option::Option<&str> {
        self.previous_model_training_job_id.as_deref()
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub fn data_processing_job_id(&self) -> ::std::option::Option<&str> {
        self.data_processing_job_id.as_deref()
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub fn train_model_s3_location(&self) -> ::std::option::Option<&str> {
        self.train_model_s3_location.as_deref()
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn sagemaker_iam_role_arn(&self) -> ::std::option::Option<&str> {
        self.sagemaker_iam_role_arn.as_deref()
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn neptune_iam_role_arn(&self) -> ::std::option::Option<&str> {
        self.neptune_iam_role_arn.as_deref()
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn base_processing_instance_type(&self) -> ::std::option::Option<&str> {
        self.base_processing_instance_type.as_deref()
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn training_instance_type(&self) -> ::std::option::Option<&str> {
        self.training_instance_type.as_deref()
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn training_instance_volume_size_in_gb(&self) -> ::std::option::Option<i32> {
        self.training_instance_volume_size_in_gb
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn training_time_out_in_seconds(&self) -> ::std::option::Option<i32> {
        self.training_time_out_in_seconds
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn max_hpo_number_of_training_jobs(&self) -> ::std::option::Option<i32> {
        self.max_hpo_number_of_training_jobs
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn max_hpo_parallel_training_jobs(&self) -> ::std::option::Option<i32> {
        self.max_hpo_parallel_training_jobs
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.subnets.is_none()`.
    pub fn subnets(&self) -> &[::std::string::String] {
        self.subnets.as_deref().unwrap_or_default()
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.security_group_ids.is_none()`.
    pub fn security_group_ids(&self) -> &[::std::string::String] {
        self.security_group_ids.as_deref().unwrap_or_default()
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn volume_encryption_kms_key(&self) -> ::std::option::Option<&str> {
        self.volume_encryption_kms_key.as_deref()
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn s3_output_encryption_kms_key(&self) -> ::std::option::Option<&str> {
        self.s3_output_encryption_kms_key.as_deref()
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn enable_managed_spot_training(&self) -> ::std::option::Option<bool> {
        self.enable_managed_spot_training
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn custom_model_training_parameters(&self) -> ::std::option::Option<&crate::types::CustomModelTrainingParameters> {
        self.custom_model_training_parameters.as_ref()
    }
}
impl StartMlModelTrainingJobInput {
    /// Creates a new builder-style object to manufacture [`StartMlModelTrainingJobInput`](crate::operation::start_ml_model_training_job::StartMlModelTrainingJobInput).
    pub fn builder() -> crate::operation::start_ml_model_training_job::builders::StartMlModelTrainingJobInputBuilder {
        crate::operation::start_ml_model_training_job::builders::StartMlModelTrainingJobInputBuilder::default()
    }
}

/// A builder for [`StartMlModelTrainingJobInput`](crate::operation::start_ml_model_training_job::StartMlModelTrainingJobInput).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct StartMlModelTrainingJobInputBuilder {
    pub(crate) id: ::std::option::Option<::std::string::String>,
    pub(crate) previous_model_training_job_id: ::std::option::Option<::std::string::String>,
    pub(crate) data_processing_job_id: ::std::option::Option<::std::string::String>,
    pub(crate) train_model_s3_location: ::std::option::Option<::std::string::String>,
    pub(crate) sagemaker_iam_role_arn: ::std::option::Option<::std::string::String>,
    pub(crate) neptune_iam_role_arn: ::std::option::Option<::std::string::String>,
    pub(crate) base_processing_instance_type: ::std::option::Option<::std::string::String>,
    pub(crate) training_instance_type: ::std::option::Option<::std::string::String>,
    pub(crate) training_instance_volume_size_in_gb: ::std::option::Option<i32>,
    pub(crate) training_time_out_in_seconds: ::std::option::Option<i32>,
    pub(crate) max_hpo_number_of_training_jobs: ::std::option::Option<i32>,
    pub(crate) max_hpo_parallel_training_jobs: ::std::option::Option<i32>,
    pub(crate) subnets: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    pub(crate) security_group_ids: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    pub(crate) volume_encryption_kms_key: ::std::option::Option<::std::string::String>,
    pub(crate) s3_output_encryption_kms_key: ::std::option::Option<::std::string::String>,
    pub(crate) enable_managed_spot_training: ::std::option::Option<bool>,
    pub(crate) custom_model_training_parameters: ::std::option::Option<crate::types::CustomModelTrainingParameters>,
}
impl StartMlModelTrainingJobInputBuilder {
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn set_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.id = input;
        self
    }
    /// <p>A unique identifier for the new job. The default is An autogenerated UUID.</p>
    pub fn get_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.id
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn previous_model_training_job_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.previous_model_training_job_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn set_previous_model_training_job_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.previous_model_training_job_id = input;
        self
    }
    /// <p>The job ID of a completed model-training job that you want to update incrementally based on updated data.</p>
    pub fn get_previous_model_training_job_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.previous_model_training_job_id
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    /// This field is required.
    pub fn data_processing_job_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.data_processing_job_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub fn set_data_processing_job_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.data_processing_job_id = input;
        self
    }
    /// <p>The job ID of the completed data-processing job that has created the data that the training will work with.</p>
    pub fn get_data_processing_job_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.data_processing_job_id
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    /// This field is required.
    pub fn train_model_s3_location(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.train_model_s3_location = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub fn set_train_model_s3_location(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.train_model_s3_location = input;
        self
    }
    /// <p>The location in Amazon S3 where the model artifacts are to be stored.</p>
    pub fn get_train_model_s3_location(&self) -> &::std::option::Option<::std::string::String> {
        &self.train_model_s3_location
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn sagemaker_iam_role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.sagemaker_iam_role_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn set_sagemaker_iam_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.sagemaker_iam_role_arn = input;
        self
    }
    /// <p>The ARN of an IAM role for SageMaker execution.This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn get_sagemaker_iam_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.sagemaker_iam_role_arn
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn neptune_iam_role_arn(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.neptune_iam_role_arn = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn set_neptune_iam_role_arn(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.neptune_iam_role_arn = input;
        self
    }
    /// <p>The ARN of an IAM role that provides Neptune access to SageMaker and Amazon S3 resources. This must be listed in your DB cluster parameter group or an error will occur.</p>
    pub fn get_neptune_iam_role_arn(&self) -> &::std::option::Option<::std::string::String> {
        &self.neptune_iam_role_arn
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn base_processing_instance_type(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.base_processing_instance_type = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn set_base_processing_instance_type(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.base_processing_instance_type = input;
        self
    }
    /// <p>The type of ML instance used in preparing and managing training of ML models. This is a CPU instance chosen based on memory requirements for processing the training data and model.</p>
    pub fn get_base_processing_instance_type(&self) -> &::std::option::Option<::std::string::String> {
        &self.base_processing_instance_type
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn training_instance_type(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.training_instance_type = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn set_training_instance_type(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.training_instance_type = input;
        self
    }
    /// <p>The type of ML instance used for model training. All Neptune ML models support CPU, GPU, and multiGPU training. The default is <code>ml.p3.2xlarge</code>. Choosing the right instance type for training depends on the task type, graph size, and your budget.</p>
    pub fn get_training_instance_type(&self) -> &::std::option::Option<::std::string::String> {
        &self.training_instance_type
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn training_instance_volume_size_in_gb(mut self, input: i32) -> Self {
        self.training_instance_volume_size_in_gb = ::std::option::Option::Some(input);
        self
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn set_training_instance_volume_size_in_gb(mut self, input: ::std::option::Option<i32>) -> Self {
        self.training_instance_volume_size_in_gb = input;
        self
    }
    /// <p>The disk volume size of the training instance. Both input data and the output model are stored on disk, so the volume size must be large enough to hold both data sets. The default is 0. If not specified or 0, Neptune ML selects a disk volume size based on the recommendation generated in the data processing step.</p>
    pub fn get_training_instance_volume_size_in_gb(&self) -> &::std::option::Option<i32> {
        &self.training_instance_volume_size_in_gb
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn training_time_out_in_seconds(mut self, input: i32) -> Self {
        self.training_time_out_in_seconds = ::std::option::Option::Some(input);
        self
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn set_training_time_out_in_seconds(mut self, input: ::std::option::Option<i32>) -> Self {
        self.training_time_out_in_seconds = input;
        self
    }
    /// <p>Timeout in seconds for the training job. The default is 86,400 (1 day).</p>
    pub fn get_training_time_out_in_seconds(&self) -> &::std::option::Option<i32> {
        &self.training_time_out_in_seconds
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn max_hpo_number_of_training_jobs(mut self, input: i32) -> Self {
        self.max_hpo_number_of_training_jobs = ::std::option::Option::Some(input);
        self
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn set_max_hpo_number_of_training_jobs(mut self, input: ::std::option::Option<i32>) -> Self {
        self.max_hpo_number_of_training_jobs = input;
        self
    }
    /// <p>Maximum total number of training jobs to start for the hyperparameter tuning job. The default is 2. Neptune ML automatically tunes the hyperparameters of the machine learning model. To obtain a model that performs well, use at least 10 jobs (in other words, set <code>maxHPONumberOfTrainingJobs</code> to 10). In general, the more tuning runs, the better the results.</p>
    pub fn get_max_hpo_number_of_training_jobs(&self) -> &::std::option::Option<i32> {
        &self.max_hpo_number_of_training_jobs
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn max_hpo_parallel_training_jobs(mut self, input: i32) -> Self {
        self.max_hpo_parallel_training_jobs = ::std::option::Option::Some(input);
        self
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn set_max_hpo_parallel_training_jobs(mut self, input: ::std::option::Option<i32>) -> Self {
        self.max_hpo_parallel_training_jobs = input;
        self
    }
    /// <p>Maximum number of parallel training jobs to start for the hyperparameter tuning job. The default is 2. The number of parallel jobs you can run is limited by the available resources on your training instance.</p>
    pub fn get_max_hpo_parallel_training_jobs(&self) -> &::std::option::Option<i32> {
        &self.max_hpo_parallel_training_jobs
    }
    /// Appends an item to `subnets`.
    ///
    /// To override the contents of this collection use [`set_subnets`](Self::set_subnets).
    ///
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn subnets(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.subnets.unwrap_or_default();
        v.push(input.into());
        self.subnets = ::std::option::Option::Some(v);
        self
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn set_subnets(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.subnets = input;
        self
    }
    /// <p>The IDs of the subnets in the Neptune VPC. The default is None.</p>
    pub fn get_subnets(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.subnets
    }
    /// Appends an item to `security_group_ids`.
    ///
    /// To override the contents of this collection use [`set_security_group_ids`](Self::set_security_group_ids).
    ///
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn security_group_ids(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.security_group_ids.unwrap_or_default();
        v.push(input.into());
        self.security_group_ids = ::std::option::Option::Some(v);
        self
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn set_security_group_ids(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.security_group_ids = input;
        self
    }
    /// <p>The VPC security group IDs. The default is None.</p>
    pub fn get_security_group_ids(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.security_group_ids
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn volume_encryption_kms_key(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.volume_encryption_kms_key = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn set_volume_encryption_kms_key(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.volume_encryption_kms_key = input;
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt data on the storage volume attached to the ML compute instances that run the training job. The default is None.</p>
    pub fn get_volume_encryption_kms_key(&self) -> &::std::option::Option<::std::string::String> {
        &self.volume_encryption_kms_key
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn s3_output_encryption_kms_key(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.s3_output_encryption_kms_key = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn set_s3_output_encryption_kms_key(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.s3_output_encryption_kms_key = input;
        self
    }
    /// <p>The Amazon Key Management Service (KMS) key that SageMaker uses to encrypt the output of the processing job. The default is none.</p>
    pub fn get_s3_output_encryption_kms_key(&self) -> &::std::option::Option<::std::string::String> {
        &self.s3_output_encryption_kms_key
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn enable_managed_spot_training(mut self, input: bool) -> Self {
        self.enable_managed_spot_training = ::std::option::Option::Some(input);
        self
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn set_enable_managed_spot_training(mut self, input: ::std::option::Option<bool>) -> Self {
        self.enable_managed_spot_training = input;
        self
    }
    /// <p>Optimizes the cost of training machine-learning models by using Amazon Elastic Compute Cloud spot instances. The default is <code>False</code>.</p>
    pub fn get_enable_managed_spot_training(&self) -> &::std::option::Option<bool> {
        &self.enable_managed_spot_training
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn custom_model_training_parameters(mut self, input: crate::types::CustomModelTrainingParameters) -> Self {
        self.custom_model_training_parameters = ::std::option::Option::Some(input);
        self
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn set_custom_model_training_parameters(mut self, input: ::std::option::Option<crate::types::CustomModelTrainingParameters>) -> Self {
        self.custom_model_training_parameters = input;
        self
    }
    /// <p>The configuration for custom model training. This is a JSON object.</p>
    pub fn get_custom_model_training_parameters(&self) -> &::std::option::Option<crate::types::CustomModelTrainingParameters> {
        &self.custom_model_training_parameters
    }
    /// Consumes the builder and constructs a [`StartMlModelTrainingJobInput`](crate::operation::start_ml_model_training_job::StartMlModelTrainingJobInput).
    pub fn build(
        self,
    ) -> ::std::result::Result<
        crate::operation::start_ml_model_training_job::StartMlModelTrainingJobInput,
        ::aws_smithy_types::error::operation::BuildError,
    > {
        ::std::result::Result::Ok(crate::operation::start_ml_model_training_job::StartMlModelTrainingJobInput {
            id: self.id,
            previous_model_training_job_id: self.previous_model_training_job_id,
            data_processing_job_id: self.data_processing_job_id,
            train_model_s3_location: self.train_model_s3_location,
            sagemaker_iam_role_arn: self.sagemaker_iam_role_arn,
            neptune_iam_role_arn: self.neptune_iam_role_arn,
            base_processing_instance_type: self.base_processing_instance_type,
            training_instance_type: self.training_instance_type,
            training_instance_volume_size_in_gb: self.training_instance_volume_size_in_gb,
            training_time_out_in_seconds: self.training_time_out_in_seconds,
            max_hpo_number_of_training_jobs: self.max_hpo_number_of_training_jobs,
            max_hpo_parallel_training_jobs: self.max_hpo_parallel_training_jobs,
            subnets: self.subnets,
            security_group_ids: self.security_group_ids,
            volume_encryption_kms_key: self.volume_encryption_kms_key,
            s3_output_encryption_kms_key: self.s3_output_encryption_kms_key,
            enable_managed_spot_training: self.enable_managed_spot_training,
            custom_model_training_parameters: self.custom_model_training_parameters,
        })
    }
}
