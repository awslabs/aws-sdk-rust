// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>The configuration of web URLs that you want to crawl. You should be authorized to crawl the URLs.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq)]
pub struct WebCrawlerConfiguration {
    /// <p>The configuration of crawl limits for the web URLs.</p>
    pub crawler_limits: ::std::option::Option<crate::types::WebCrawlerLimits>,
    /// <p>A list of one or more inclusion regular expression patterns to include certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub inclusion_filters: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    /// <p>A list of one or more exclusion regular expression patterns to exclude certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub exclusion_filters: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    /// <p>The scope of what is crawled for your URLs.</p>
    /// <p>You can choose to crawl only web pages that belong to the same host or primary domain. For example, only web pages that contain the seed URL "https://docs.aws.amazon.com/bedrock/latest/userguide/" and no other domains. You can choose to include sub domains in addition to the host or primary domain. For example, web pages that contain "aws.amazon.com" can also include sub domain "docs.aws.amazon.com".</p>
    pub scope: ::std::option::Option<crate::types::WebScopeType>,
    /// <p>A string used for identifying the crawler or a bot when it accesses a web server. By default, this is set to <code>bedrockbot_UUID</code> for your crawler. You can optionally append a custom string to <code>bedrockbot_UUID</code> to allowlist a specific user agent permitted to access your source URLs.</p>
    pub user_agent: ::std::option::Option<::std::string::String>,
}
impl WebCrawlerConfiguration {
    /// <p>The configuration of crawl limits for the web URLs.</p>
    pub fn crawler_limits(&self) -> ::std::option::Option<&crate::types::WebCrawlerLimits> {
        self.crawler_limits.as_ref()
    }
    /// <p>A list of one or more inclusion regular expression patterns to include certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.inclusion_filters.is_none()`.
    pub fn inclusion_filters(&self) -> &[::std::string::String] {
        self.inclusion_filters.as_deref().unwrap_or_default()
    }
    /// <p>A list of one or more exclusion regular expression patterns to exclude certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.exclusion_filters.is_none()`.
    pub fn exclusion_filters(&self) -> &[::std::string::String] {
        self.exclusion_filters.as_deref().unwrap_or_default()
    }
    /// <p>The scope of what is crawled for your URLs.</p>
    /// <p>You can choose to crawl only web pages that belong to the same host or primary domain. For example, only web pages that contain the seed URL "https://docs.aws.amazon.com/bedrock/latest/userguide/" and no other domains. You can choose to include sub domains in addition to the host or primary domain. For example, web pages that contain "aws.amazon.com" can also include sub domain "docs.aws.amazon.com".</p>
    pub fn scope(&self) -> ::std::option::Option<&crate::types::WebScopeType> {
        self.scope.as_ref()
    }
    /// <p>A string used for identifying the crawler or a bot when it accesses a web server. By default, this is set to <code>bedrockbot_UUID</code> for your crawler. You can optionally append a custom string to <code>bedrockbot_UUID</code> to allowlist a specific user agent permitted to access your source URLs.</p>
    pub fn user_agent(&self) -> ::std::option::Option<&str> {
        self.user_agent.as_deref()
    }
}
impl ::std::fmt::Debug for WebCrawlerConfiguration {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("WebCrawlerConfiguration");
        formatter.field("crawler_limits", &self.crawler_limits);
        formatter.field("inclusion_filters", &"*** Sensitive Data Redacted ***");
        formatter.field("exclusion_filters", &"*** Sensitive Data Redacted ***");
        formatter.field("scope", &self.scope);
        formatter.field("user_agent", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}
impl WebCrawlerConfiguration {
    /// Creates a new builder-style object to manufacture [`WebCrawlerConfiguration`](crate::types::WebCrawlerConfiguration).
    pub fn builder() -> crate::types::builders::WebCrawlerConfigurationBuilder {
        crate::types::builders::WebCrawlerConfigurationBuilder::default()
    }
}

/// A builder for [`WebCrawlerConfiguration`](crate::types::WebCrawlerConfiguration).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default)]
#[non_exhaustive]
pub struct WebCrawlerConfigurationBuilder {
    pub(crate) crawler_limits: ::std::option::Option<crate::types::WebCrawlerLimits>,
    pub(crate) inclusion_filters: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    pub(crate) exclusion_filters: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
    pub(crate) scope: ::std::option::Option<crate::types::WebScopeType>,
    pub(crate) user_agent: ::std::option::Option<::std::string::String>,
}
impl WebCrawlerConfigurationBuilder {
    /// <p>The configuration of crawl limits for the web URLs.</p>
    pub fn crawler_limits(mut self, input: crate::types::WebCrawlerLimits) -> Self {
        self.crawler_limits = ::std::option::Option::Some(input);
        self
    }
    /// <p>The configuration of crawl limits for the web URLs.</p>
    pub fn set_crawler_limits(mut self, input: ::std::option::Option<crate::types::WebCrawlerLimits>) -> Self {
        self.crawler_limits = input;
        self
    }
    /// <p>The configuration of crawl limits for the web URLs.</p>
    pub fn get_crawler_limits(&self) -> &::std::option::Option<crate::types::WebCrawlerLimits> {
        &self.crawler_limits
    }
    /// Appends an item to `inclusion_filters`.
    ///
    /// To override the contents of this collection use [`set_inclusion_filters`](Self::set_inclusion_filters).
    ///
    /// <p>A list of one or more inclusion regular expression patterns to include certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub fn inclusion_filters(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.inclusion_filters.unwrap_or_default();
        v.push(input.into());
        self.inclusion_filters = ::std::option::Option::Some(v);
        self
    }
    /// <p>A list of one or more inclusion regular expression patterns to include certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub fn set_inclusion_filters(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.inclusion_filters = input;
        self
    }
    /// <p>A list of one or more inclusion regular expression patterns to include certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub fn get_inclusion_filters(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.inclusion_filters
    }
    /// Appends an item to `exclusion_filters`.
    ///
    /// To override the contents of this collection use [`set_exclusion_filters`](Self::set_exclusion_filters).
    ///
    /// <p>A list of one or more exclusion regular expression patterns to exclude certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub fn exclusion_filters(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.exclusion_filters.unwrap_or_default();
        v.push(input.into());
        self.exclusion_filters = ::std::option::Option::Some(v);
        self
    }
    /// <p>A list of one or more exclusion regular expression patterns to exclude certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub fn set_exclusion_filters(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.exclusion_filters = input;
        self
    }
    /// <p>A list of one or more exclusion regular expression patterns to exclude certain URLs. If you specify an inclusion and exclusion filter/pattern and both match a URL, the exclusion filter takes precedence and the web content of the URL isn’t crawled.</p>
    pub fn get_exclusion_filters(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.exclusion_filters
    }
    /// <p>The scope of what is crawled for your URLs.</p>
    /// <p>You can choose to crawl only web pages that belong to the same host or primary domain. For example, only web pages that contain the seed URL "https://docs.aws.amazon.com/bedrock/latest/userguide/" and no other domains. You can choose to include sub domains in addition to the host or primary domain. For example, web pages that contain "aws.amazon.com" can also include sub domain "docs.aws.amazon.com".</p>
    pub fn scope(mut self, input: crate::types::WebScopeType) -> Self {
        self.scope = ::std::option::Option::Some(input);
        self
    }
    /// <p>The scope of what is crawled for your URLs.</p>
    /// <p>You can choose to crawl only web pages that belong to the same host or primary domain. For example, only web pages that contain the seed URL "https://docs.aws.amazon.com/bedrock/latest/userguide/" and no other domains. You can choose to include sub domains in addition to the host or primary domain. For example, web pages that contain "aws.amazon.com" can also include sub domain "docs.aws.amazon.com".</p>
    pub fn set_scope(mut self, input: ::std::option::Option<crate::types::WebScopeType>) -> Self {
        self.scope = input;
        self
    }
    /// <p>The scope of what is crawled for your URLs.</p>
    /// <p>You can choose to crawl only web pages that belong to the same host or primary domain. For example, only web pages that contain the seed URL "https://docs.aws.amazon.com/bedrock/latest/userguide/" and no other domains. You can choose to include sub domains in addition to the host or primary domain. For example, web pages that contain "aws.amazon.com" can also include sub domain "docs.aws.amazon.com".</p>
    pub fn get_scope(&self) -> &::std::option::Option<crate::types::WebScopeType> {
        &self.scope
    }
    /// <p>A string used for identifying the crawler or a bot when it accesses a web server. By default, this is set to <code>bedrockbot_UUID</code> for your crawler. You can optionally append a custom string to <code>bedrockbot_UUID</code> to allowlist a specific user agent permitted to access your source URLs.</p>
    pub fn user_agent(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.user_agent = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>A string used for identifying the crawler or a bot when it accesses a web server. By default, this is set to <code>bedrockbot_UUID</code> for your crawler. You can optionally append a custom string to <code>bedrockbot_UUID</code> to allowlist a specific user agent permitted to access your source URLs.</p>
    pub fn set_user_agent(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.user_agent = input;
        self
    }
    /// <p>A string used for identifying the crawler or a bot when it accesses a web server. By default, this is set to <code>bedrockbot_UUID</code> for your crawler. You can optionally append a custom string to <code>bedrockbot_UUID</code> to allowlist a specific user agent permitted to access your source URLs.</p>
    pub fn get_user_agent(&self) -> &::std::option::Option<::std::string::String> {
        &self.user_agent
    }
    /// Consumes the builder and constructs a [`WebCrawlerConfiguration`](crate::types::WebCrawlerConfiguration).
    pub fn build(self) -> crate::types::WebCrawlerConfiguration {
        crate::types::WebCrawlerConfiguration {
            crawler_limits: self.crawler_limits,
            inclusion_filters: self.inclusion_filters,
            exclusion_filters: self.exclusion_filters,
            scope: self.scope,
            user_agent: self.user_agent,
        }
    }
}
impl ::std::fmt::Debug for WebCrawlerConfigurationBuilder {
    fn fmt(&self, f: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        let mut formatter = f.debug_struct("WebCrawlerConfigurationBuilder");
        formatter.field("crawler_limits", &self.crawler_limits);
        formatter.field("inclusion_filters", &"*** Sensitive Data Redacted ***");
        formatter.field("exclusion_filters", &"*** Sensitive Data Redacted ***");
        formatter.field("scope", &self.scope);
        formatter.field("user_agent", &"*** Sensitive Data Redacted ***");
        formatter.finish()
    }
}
