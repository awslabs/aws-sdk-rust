// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct GetScalingConfigurationRecommendationInput {
    /// <p>The name of a previously completed Inference Recommender job.</p>
    pub inference_recommendations_job_name: ::std::option::Option<::std::string::String>,
    /// <p>The recommendation ID of a previously completed inference recommendation. This ID should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>EndpointName</code> field.</p>
    pub recommendation_id: ::std::option::Option<::std::string::String>,
    /// <p>The name of an endpoint benchmarked during a previously completed inference recommendation job. This name should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>RecommendationId</code> field.</p>
    pub endpoint_name: ::std::option::Option<::std::string::String>,
    /// <p>The percentage of how much utilization you want an instance to use before autoscaling. The default value is 50%.</p>
    pub target_cpu_utilization_per_core: ::std::option::Option<i32>,
    /// <p>An object where you specify the anticipated traffic pattern for an endpoint.</p>
    pub scaling_policy_objective: ::std::option::Option<crate::types::ScalingPolicyObjective>,
}
impl GetScalingConfigurationRecommendationInput {
    /// <p>The name of a previously completed Inference Recommender job.</p>
    pub fn inference_recommendations_job_name(&self) -> ::std::option::Option<&str> {
        self.inference_recommendations_job_name.as_deref()
    }
    /// <p>The recommendation ID of a previously completed inference recommendation. This ID should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>EndpointName</code> field.</p>
    pub fn recommendation_id(&self) -> ::std::option::Option<&str> {
        self.recommendation_id.as_deref()
    }
    /// <p>The name of an endpoint benchmarked during a previously completed inference recommendation job. This name should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>RecommendationId</code> field.</p>
    pub fn endpoint_name(&self) -> ::std::option::Option<&str> {
        self.endpoint_name.as_deref()
    }
    /// <p>The percentage of how much utilization you want an instance to use before autoscaling. The default value is 50%.</p>
    pub fn target_cpu_utilization_per_core(&self) -> ::std::option::Option<i32> {
        self.target_cpu_utilization_per_core
    }
    /// <p>An object where you specify the anticipated traffic pattern for an endpoint.</p>
    pub fn scaling_policy_objective(&self) -> ::std::option::Option<&crate::types::ScalingPolicyObjective> {
        self.scaling_policy_objective.as_ref()
    }
}
impl GetScalingConfigurationRecommendationInput {
    /// Creates a new builder-style object to manufacture [`GetScalingConfigurationRecommendationInput`](crate::operation::get_scaling_configuration_recommendation::GetScalingConfigurationRecommendationInput).
    pub fn builder() -> crate::operation::get_scaling_configuration_recommendation::builders::GetScalingConfigurationRecommendationInputBuilder {
        crate::operation::get_scaling_configuration_recommendation::builders::GetScalingConfigurationRecommendationInputBuilder::default()
    }
}

/// A builder for [`GetScalingConfigurationRecommendationInput`](crate::operation::get_scaling_configuration_recommendation::GetScalingConfigurationRecommendationInput).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct GetScalingConfigurationRecommendationInputBuilder {
    pub(crate) inference_recommendations_job_name: ::std::option::Option<::std::string::String>,
    pub(crate) recommendation_id: ::std::option::Option<::std::string::String>,
    pub(crate) endpoint_name: ::std::option::Option<::std::string::String>,
    pub(crate) target_cpu_utilization_per_core: ::std::option::Option<i32>,
    pub(crate) scaling_policy_objective: ::std::option::Option<crate::types::ScalingPolicyObjective>,
}
impl GetScalingConfigurationRecommendationInputBuilder {
    /// <p>The name of a previously completed Inference Recommender job.</p>
    /// This field is required.
    pub fn inference_recommendations_job_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.inference_recommendations_job_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The name of a previously completed Inference Recommender job.</p>
    pub fn set_inference_recommendations_job_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.inference_recommendations_job_name = input;
        self
    }
    /// <p>The name of a previously completed Inference Recommender job.</p>
    pub fn get_inference_recommendations_job_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.inference_recommendations_job_name
    }
    /// <p>The recommendation ID of a previously completed inference recommendation. This ID should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>EndpointName</code> field.</p>
    pub fn recommendation_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.recommendation_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The recommendation ID of a previously completed inference recommendation. This ID should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>EndpointName</code> field.</p>
    pub fn set_recommendation_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.recommendation_id = input;
        self
    }
    /// <p>The recommendation ID of a previously completed inference recommendation. This ID should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>EndpointName</code> field.</p>
    pub fn get_recommendation_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.recommendation_id
    }
    /// <p>The name of an endpoint benchmarked during a previously completed inference recommendation job. This name should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>RecommendationId</code> field.</p>
    pub fn endpoint_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.endpoint_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The name of an endpoint benchmarked during a previously completed inference recommendation job. This name should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>RecommendationId</code> field.</p>
    pub fn set_endpoint_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.endpoint_name = input;
        self
    }
    /// <p>The name of an endpoint benchmarked during a previously completed inference recommendation job. This name should come from one of the recommendations returned by the job specified in the <code>InferenceRecommendationsJobName</code> field.</p>
    /// <p>Specify either this field or the <code>RecommendationId</code> field.</p>
    pub fn get_endpoint_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.endpoint_name
    }
    /// <p>The percentage of how much utilization you want an instance to use before autoscaling. The default value is 50%.</p>
    pub fn target_cpu_utilization_per_core(mut self, input: i32) -> Self {
        self.target_cpu_utilization_per_core = ::std::option::Option::Some(input);
        self
    }
    /// <p>The percentage of how much utilization you want an instance to use before autoscaling. The default value is 50%.</p>
    pub fn set_target_cpu_utilization_per_core(mut self, input: ::std::option::Option<i32>) -> Self {
        self.target_cpu_utilization_per_core = input;
        self
    }
    /// <p>The percentage of how much utilization you want an instance to use before autoscaling. The default value is 50%.</p>
    pub fn get_target_cpu_utilization_per_core(&self) -> &::std::option::Option<i32> {
        &self.target_cpu_utilization_per_core
    }
    /// <p>An object where you specify the anticipated traffic pattern for an endpoint.</p>
    pub fn scaling_policy_objective(mut self, input: crate::types::ScalingPolicyObjective) -> Self {
        self.scaling_policy_objective = ::std::option::Option::Some(input);
        self
    }
    /// <p>An object where you specify the anticipated traffic pattern for an endpoint.</p>
    pub fn set_scaling_policy_objective(mut self, input: ::std::option::Option<crate::types::ScalingPolicyObjective>) -> Self {
        self.scaling_policy_objective = input;
        self
    }
    /// <p>An object where you specify the anticipated traffic pattern for an endpoint.</p>
    pub fn get_scaling_policy_objective(&self) -> &::std::option::Option<crate::types::ScalingPolicyObjective> {
        &self.scaling_policy_objective
    }
    /// Consumes the builder and constructs a [`GetScalingConfigurationRecommendationInput`](crate::operation::get_scaling_configuration_recommendation::GetScalingConfigurationRecommendationInput).
    pub fn build(
        self,
    ) -> ::std::result::Result<
        crate::operation::get_scaling_configuration_recommendation::GetScalingConfigurationRecommendationInput,
        ::aws_smithy_types::error::operation::BuildError,
    > {
        ::std::result::Result::Ok(
            crate::operation::get_scaling_configuration_recommendation::GetScalingConfigurationRecommendationInput {
                inference_recommendations_job_name: self.inference_recommendations_job_name,
                recommendation_id: self.recommendation_id,
                endpoint_name: self.endpoint_name,
                target_cpu_utilization_per_core: self.target_cpu_utilization_per_core,
                scaling_policy_objective: self.scaling_policy_objective,
            },
        )
    }
}
