// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Details about the resources to deploy with this inference component, including the model, container, and compute resources.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct InferenceComponentSpecification {
    /// <p>The name of an existing SageMaker AI model object in your account that you want to deploy with the inference component.</p>
    pub model_name: ::std::option::Option<::std::string::String>,
    /// <p>Defines a container that provides the runtime environment for a model that you deploy with an inference component.</p>
    pub container: ::std::option::Option<crate::types::InferenceComponentContainerSpecification>,
    /// <p>Settings that take effect while the model container starts up.</p>
    pub startup_parameters: ::std::option::Option<crate::types::InferenceComponentStartupParameters>,
    /// <p>The compute resources allocated to run the model, plus any adapter models, that you assign to the inference component.</p>
    /// <p>Omit this parameter if your request is meant to create an adapter inference component. An adapter inference component is loaded by a base inference component, and it uses the compute resources of the base inference component.</p>
    pub compute_resource_requirements: ::std::option::Option<crate::types::InferenceComponentComputeResourceRequirements>,
    /// <p>The name of an existing inference component that is to contain the inference component that you're creating with your request.</p>
    /// <p>Specify this parameter only if your request is meant to create an adapter inference component. An adapter inference component contains the path to an adapter model. The purpose of the adapter model is to tailor the inference output of a base foundation model, which is hosted by the base inference component. The adapter inference component uses the compute resources that you assigned to the base inference component.</p>
    /// <p>When you create an adapter inference component, use the <code>Container</code> parameter to specify the location of the adapter artifacts. In the parameter value, use the <code>ArtifactUrl</code> parameter of the <code>InferenceComponentContainerSpecification</code> data type.</p>
    /// <p>Before you can create an adapter inference component, you must have an existing inference component that contains the foundation model that you want to adapt.</p>
    pub base_inference_component_name: ::std::option::Option<::std::string::String>,
    /// <p>Settings that affect how the inference component caches data.</p>
    pub data_cache_config: ::std::option::Option<crate::types::InferenceComponentDataCacheConfig>,
}
impl InferenceComponentSpecification {
    /// <p>The name of an existing SageMaker AI model object in your account that you want to deploy with the inference component.</p>
    pub fn model_name(&self) -> ::std::option::Option<&str> {
        self.model_name.as_deref()
    }
    /// <p>Defines a container that provides the runtime environment for a model that you deploy with an inference component.</p>
    pub fn container(&self) -> ::std::option::Option<&crate::types::InferenceComponentContainerSpecification> {
        self.container.as_ref()
    }
    /// <p>Settings that take effect while the model container starts up.</p>
    pub fn startup_parameters(&self) -> ::std::option::Option<&crate::types::InferenceComponentStartupParameters> {
        self.startup_parameters.as_ref()
    }
    /// <p>The compute resources allocated to run the model, plus any adapter models, that you assign to the inference component.</p>
    /// <p>Omit this parameter if your request is meant to create an adapter inference component. An adapter inference component is loaded by a base inference component, and it uses the compute resources of the base inference component.</p>
    pub fn compute_resource_requirements(&self) -> ::std::option::Option<&crate::types::InferenceComponentComputeResourceRequirements> {
        self.compute_resource_requirements.as_ref()
    }
    /// <p>The name of an existing inference component that is to contain the inference component that you're creating with your request.</p>
    /// <p>Specify this parameter only if your request is meant to create an adapter inference component. An adapter inference component contains the path to an adapter model. The purpose of the adapter model is to tailor the inference output of a base foundation model, which is hosted by the base inference component. The adapter inference component uses the compute resources that you assigned to the base inference component.</p>
    /// <p>When you create an adapter inference component, use the <code>Container</code> parameter to specify the location of the adapter artifacts. In the parameter value, use the <code>ArtifactUrl</code> parameter of the <code>InferenceComponentContainerSpecification</code> data type.</p>
    /// <p>Before you can create an adapter inference component, you must have an existing inference component that contains the foundation model that you want to adapt.</p>
    pub fn base_inference_component_name(&self) -> ::std::option::Option<&str> {
        self.base_inference_component_name.as_deref()
    }
    /// <p>Settings that affect how the inference component caches data.</p>
    pub fn data_cache_config(&self) -> ::std::option::Option<&crate::types::InferenceComponentDataCacheConfig> {
        self.data_cache_config.as_ref()
    }
}
impl InferenceComponentSpecification {
    /// Creates a new builder-style object to manufacture [`InferenceComponentSpecification`](crate::types::InferenceComponentSpecification).
    pub fn builder() -> crate::types::builders::InferenceComponentSpecificationBuilder {
        crate::types::builders::InferenceComponentSpecificationBuilder::default()
    }
}

/// A builder for [`InferenceComponentSpecification`](crate::types::InferenceComponentSpecification).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct InferenceComponentSpecificationBuilder {
    pub(crate) model_name: ::std::option::Option<::std::string::String>,
    pub(crate) container: ::std::option::Option<crate::types::InferenceComponentContainerSpecification>,
    pub(crate) startup_parameters: ::std::option::Option<crate::types::InferenceComponentStartupParameters>,
    pub(crate) compute_resource_requirements: ::std::option::Option<crate::types::InferenceComponentComputeResourceRequirements>,
    pub(crate) base_inference_component_name: ::std::option::Option<::std::string::String>,
    pub(crate) data_cache_config: ::std::option::Option<crate::types::InferenceComponentDataCacheConfig>,
}
impl InferenceComponentSpecificationBuilder {
    /// <p>The name of an existing SageMaker AI model object in your account that you want to deploy with the inference component.</p>
    pub fn model_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.model_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The name of an existing SageMaker AI model object in your account that you want to deploy with the inference component.</p>
    pub fn set_model_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.model_name = input;
        self
    }
    /// <p>The name of an existing SageMaker AI model object in your account that you want to deploy with the inference component.</p>
    pub fn get_model_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.model_name
    }
    /// <p>Defines a container that provides the runtime environment for a model that you deploy with an inference component.</p>
    pub fn container(mut self, input: crate::types::InferenceComponentContainerSpecification) -> Self {
        self.container = ::std::option::Option::Some(input);
        self
    }
    /// <p>Defines a container that provides the runtime environment for a model that you deploy with an inference component.</p>
    pub fn set_container(mut self, input: ::std::option::Option<crate::types::InferenceComponentContainerSpecification>) -> Self {
        self.container = input;
        self
    }
    /// <p>Defines a container that provides the runtime environment for a model that you deploy with an inference component.</p>
    pub fn get_container(&self) -> &::std::option::Option<crate::types::InferenceComponentContainerSpecification> {
        &self.container
    }
    /// <p>Settings that take effect while the model container starts up.</p>
    pub fn startup_parameters(mut self, input: crate::types::InferenceComponentStartupParameters) -> Self {
        self.startup_parameters = ::std::option::Option::Some(input);
        self
    }
    /// <p>Settings that take effect while the model container starts up.</p>
    pub fn set_startup_parameters(mut self, input: ::std::option::Option<crate::types::InferenceComponentStartupParameters>) -> Self {
        self.startup_parameters = input;
        self
    }
    /// <p>Settings that take effect while the model container starts up.</p>
    pub fn get_startup_parameters(&self) -> &::std::option::Option<crate::types::InferenceComponentStartupParameters> {
        &self.startup_parameters
    }
    /// <p>The compute resources allocated to run the model, plus any adapter models, that you assign to the inference component.</p>
    /// <p>Omit this parameter if your request is meant to create an adapter inference component. An adapter inference component is loaded by a base inference component, and it uses the compute resources of the base inference component.</p>
    pub fn compute_resource_requirements(mut self, input: crate::types::InferenceComponentComputeResourceRequirements) -> Self {
        self.compute_resource_requirements = ::std::option::Option::Some(input);
        self
    }
    /// <p>The compute resources allocated to run the model, plus any adapter models, that you assign to the inference component.</p>
    /// <p>Omit this parameter if your request is meant to create an adapter inference component. An adapter inference component is loaded by a base inference component, and it uses the compute resources of the base inference component.</p>
    pub fn set_compute_resource_requirements(
        mut self,
        input: ::std::option::Option<crate::types::InferenceComponentComputeResourceRequirements>,
    ) -> Self {
        self.compute_resource_requirements = input;
        self
    }
    /// <p>The compute resources allocated to run the model, plus any adapter models, that you assign to the inference component.</p>
    /// <p>Omit this parameter if your request is meant to create an adapter inference component. An adapter inference component is loaded by a base inference component, and it uses the compute resources of the base inference component.</p>
    pub fn get_compute_resource_requirements(&self) -> &::std::option::Option<crate::types::InferenceComponentComputeResourceRequirements> {
        &self.compute_resource_requirements
    }
    /// <p>The name of an existing inference component that is to contain the inference component that you're creating with your request.</p>
    /// <p>Specify this parameter only if your request is meant to create an adapter inference component. An adapter inference component contains the path to an adapter model. The purpose of the adapter model is to tailor the inference output of a base foundation model, which is hosted by the base inference component. The adapter inference component uses the compute resources that you assigned to the base inference component.</p>
    /// <p>When you create an adapter inference component, use the <code>Container</code> parameter to specify the location of the adapter artifacts. In the parameter value, use the <code>ArtifactUrl</code> parameter of the <code>InferenceComponentContainerSpecification</code> data type.</p>
    /// <p>Before you can create an adapter inference component, you must have an existing inference component that contains the foundation model that you want to adapt.</p>
    pub fn base_inference_component_name(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.base_inference_component_name = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The name of an existing inference component that is to contain the inference component that you're creating with your request.</p>
    /// <p>Specify this parameter only if your request is meant to create an adapter inference component. An adapter inference component contains the path to an adapter model. The purpose of the adapter model is to tailor the inference output of a base foundation model, which is hosted by the base inference component. The adapter inference component uses the compute resources that you assigned to the base inference component.</p>
    /// <p>When you create an adapter inference component, use the <code>Container</code> parameter to specify the location of the adapter artifacts. In the parameter value, use the <code>ArtifactUrl</code> parameter of the <code>InferenceComponentContainerSpecification</code> data type.</p>
    /// <p>Before you can create an adapter inference component, you must have an existing inference component that contains the foundation model that you want to adapt.</p>
    pub fn set_base_inference_component_name(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.base_inference_component_name = input;
        self
    }
    /// <p>The name of an existing inference component that is to contain the inference component that you're creating with your request.</p>
    /// <p>Specify this parameter only if your request is meant to create an adapter inference component. An adapter inference component contains the path to an adapter model. The purpose of the adapter model is to tailor the inference output of a base foundation model, which is hosted by the base inference component. The adapter inference component uses the compute resources that you assigned to the base inference component.</p>
    /// <p>When you create an adapter inference component, use the <code>Container</code> parameter to specify the location of the adapter artifacts. In the parameter value, use the <code>ArtifactUrl</code> parameter of the <code>InferenceComponentContainerSpecification</code> data type.</p>
    /// <p>Before you can create an adapter inference component, you must have an existing inference component that contains the foundation model that you want to adapt.</p>
    pub fn get_base_inference_component_name(&self) -> &::std::option::Option<::std::string::String> {
        &self.base_inference_component_name
    }
    /// <p>Settings that affect how the inference component caches data.</p>
    pub fn data_cache_config(mut self, input: crate::types::InferenceComponentDataCacheConfig) -> Self {
        self.data_cache_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Settings that affect how the inference component caches data.</p>
    pub fn set_data_cache_config(mut self, input: ::std::option::Option<crate::types::InferenceComponentDataCacheConfig>) -> Self {
        self.data_cache_config = input;
        self
    }
    /// <p>Settings that affect how the inference component caches data.</p>
    pub fn get_data_cache_config(&self) -> &::std::option::Option<crate::types::InferenceComponentDataCacheConfig> {
        &self.data_cache_config
    }
    /// Consumes the builder and constructs a [`InferenceComponentSpecification`](crate::types::InferenceComponentSpecification).
    pub fn build(self) -> crate::types::InferenceComponentSpecification {
        crate::types::InferenceComponentSpecification {
            model_name: self.model_name,
            container: self.container,
            startup_parameters: self.startup_parameters,
            compute_resource_requirements: self.compute_resource_requirements,
            base_inference_component_name: self.base_inference_component_name,
            data_cache_config: self.data_cache_config,
        }
    }
}
