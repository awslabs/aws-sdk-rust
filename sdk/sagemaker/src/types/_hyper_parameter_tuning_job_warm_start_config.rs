// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Specifies the configuration for a hyperparameter tuning job that uses one or more previous hyperparameter tuning jobs as a starting point. The results of previous tuning jobs are used to inform which combinations of hyperparameters to search over in the new tuning job.</p>
/// <p>All training jobs launched by the new hyperparameter tuning job are evaluated by using the objective metric, and the training job that performs the best is compared to the best training jobs from the parent tuning jobs. From these, the training job that performs the best as measured by the objective metric is returned as the overall best training job.</p> <note>
/// <p>All training jobs launched by parent hyperparameter tuning jobs and the new hyperparameter tuning jobs count against the limit of training jobs for the tuning job.</p>
/// </note>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct HyperParameterTuningJobWarmStartConfig {
    /// <p>An array of hyperparameter tuning jobs that are used as the starting point for the new hyperparameter tuning job. For more information about warm starting a hyperparameter tuning job, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html">Using a Previous Hyperparameter Tuning Job as a Starting Point</a>.</p>
    /// <p>Hyperparameter tuning jobs created before October 1, 2018 cannot be used as parent jobs for warm start tuning jobs.</p>
    pub parent_hyper_parameter_tuning_jobs: ::std::option::Option<::std::vec::Vec<crate::types::ParentHyperParameterTuningJob>>,
    /// <p>Specifies one of the following:</p>
    /// <dl>
    /// <dt>
    /// IDENTICAL_DATA_AND_ALGORITHM
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. You can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. You cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. For example, changes that improve logging or adding support for a different data format are allowed. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// <dt>
    /// TRANSFER_LEARNING
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. The training image can also be a different version from the version used in the parent hyperparameter tuning job. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// </dl>
    pub warm_start_type: ::std::option::Option<crate::types::HyperParameterTuningJobWarmStartType>,
}
impl HyperParameterTuningJobWarmStartConfig {
    /// <p>An array of hyperparameter tuning jobs that are used as the starting point for the new hyperparameter tuning job. For more information about warm starting a hyperparameter tuning job, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html">Using a Previous Hyperparameter Tuning Job as a Starting Point</a>.</p>
    /// <p>Hyperparameter tuning jobs created before October 1, 2018 cannot be used as parent jobs for warm start tuning jobs.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.parent_hyper_parameter_tuning_jobs.is_none()`.
    pub fn parent_hyper_parameter_tuning_jobs(&self) -> &[crate::types::ParentHyperParameterTuningJob] {
        self.parent_hyper_parameter_tuning_jobs.as_deref().unwrap_or_default()
    }
    /// <p>Specifies one of the following:</p>
    /// <dl>
    /// <dt>
    /// IDENTICAL_DATA_AND_ALGORITHM
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. You can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. You cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. For example, changes that improve logging or adding support for a different data format are allowed. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// <dt>
    /// TRANSFER_LEARNING
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. The training image can also be a different version from the version used in the parent hyperparameter tuning job. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// </dl>
    pub fn warm_start_type(&self) -> ::std::option::Option<&crate::types::HyperParameterTuningJobWarmStartType> {
        self.warm_start_type.as_ref()
    }
}
impl HyperParameterTuningJobWarmStartConfig {
    /// Creates a new builder-style object to manufacture [`HyperParameterTuningJobWarmStartConfig`](crate::types::HyperParameterTuningJobWarmStartConfig).
    pub fn builder() -> crate::types::builders::HyperParameterTuningJobWarmStartConfigBuilder {
        crate::types::builders::HyperParameterTuningJobWarmStartConfigBuilder::default()
    }
}

/// A builder for [`HyperParameterTuningJobWarmStartConfig`](crate::types::HyperParameterTuningJobWarmStartConfig).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct HyperParameterTuningJobWarmStartConfigBuilder {
    pub(crate) parent_hyper_parameter_tuning_jobs: ::std::option::Option<::std::vec::Vec<crate::types::ParentHyperParameterTuningJob>>,
    pub(crate) warm_start_type: ::std::option::Option<crate::types::HyperParameterTuningJobWarmStartType>,
}
impl HyperParameterTuningJobWarmStartConfigBuilder {
    /// Appends an item to `parent_hyper_parameter_tuning_jobs`.
    ///
    /// To override the contents of this collection use [`set_parent_hyper_parameter_tuning_jobs`](Self::set_parent_hyper_parameter_tuning_jobs).
    ///
    /// <p>An array of hyperparameter tuning jobs that are used as the starting point for the new hyperparameter tuning job. For more information about warm starting a hyperparameter tuning job, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html">Using a Previous Hyperparameter Tuning Job as a Starting Point</a>.</p>
    /// <p>Hyperparameter tuning jobs created before October 1, 2018 cannot be used as parent jobs for warm start tuning jobs.</p>
    pub fn parent_hyper_parameter_tuning_jobs(mut self, input: crate::types::ParentHyperParameterTuningJob) -> Self {
        let mut v = self.parent_hyper_parameter_tuning_jobs.unwrap_or_default();
        v.push(input);
        self.parent_hyper_parameter_tuning_jobs = ::std::option::Option::Some(v);
        self
    }
    /// <p>An array of hyperparameter tuning jobs that are used as the starting point for the new hyperparameter tuning job. For more information about warm starting a hyperparameter tuning job, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html">Using a Previous Hyperparameter Tuning Job as a Starting Point</a>.</p>
    /// <p>Hyperparameter tuning jobs created before October 1, 2018 cannot be used as parent jobs for warm start tuning jobs.</p>
    pub fn set_parent_hyper_parameter_tuning_jobs(
        mut self,
        input: ::std::option::Option<::std::vec::Vec<crate::types::ParentHyperParameterTuningJob>>,
    ) -> Self {
        self.parent_hyper_parameter_tuning_jobs = input;
        self
    }
    /// <p>An array of hyperparameter tuning jobs that are used as the starting point for the new hyperparameter tuning job. For more information about warm starting a hyperparameter tuning job, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html">Using a Previous Hyperparameter Tuning Job as a Starting Point</a>.</p>
    /// <p>Hyperparameter tuning jobs created before October 1, 2018 cannot be used as parent jobs for warm start tuning jobs.</p>
    pub fn get_parent_hyper_parameter_tuning_jobs(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::ParentHyperParameterTuningJob>> {
        &self.parent_hyper_parameter_tuning_jobs
    }
    /// <p>Specifies one of the following:</p>
    /// <dl>
    /// <dt>
    /// IDENTICAL_DATA_AND_ALGORITHM
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. You can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. You cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. For example, changes that improve logging or adding support for a different data format are allowed. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// <dt>
    /// TRANSFER_LEARNING
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. The training image can also be a different version from the version used in the parent hyperparameter tuning job. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// </dl>
    /// This field is required.
    pub fn warm_start_type(mut self, input: crate::types::HyperParameterTuningJobWarmStartType) -> Self {
        self.warm_start_type = ::std::option::Option::Some(input);
        self
    }
    /// <p>Specifies one of the following:</p>
    /// <dl>
    /// <dt>
    /// IDENTICAL_DATA_AND_ALGORITHM
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. You can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. You cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. For example, changes that improve logging or adding support for a different data format are allowed. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// <dt>
    /// TRANSFER_LEARNING
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. The training image can also be a different version from the version used in the parent hyperparameter tuning job. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// </dl>
    pub fn set_warm_start_type(mut self, input: ::std::option::Option<crate::types::HyperParameterTuningJobWarmStartType>) -> Self {
        self.warm_start_type = input;
        self
    }
    /// <p>Specifies one of the following:</p>
    /// <dl>
    /// <dt>
    /// IDENTICAL_DATA_AND_ALGORITHM
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job uses the same input data and training image as the parent tuning jobs. You can change the hyperparameter ranges to search and the maximum number of training jobs that the hyperparameter tuning job launches. You cannot use a new version of the training algorithm, unless the changes in the new version do not affect the algorithm itself. For example, changes that improve logging or adding support for a different data format are allowed. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// <dt>
    /// TRANSFER_LEARNING
    /// </dt>
    /// <dd>
    /// <p>The new hyperparameter tuning job can include input data, hyperparameter ranges, maximum number of concurrent training jobs, and maximum number of training jobs that are different than those of its parent hyperparameter tuning jobs. The training image can also be a different version from the version used in the parent hyperparameter tuning job. You can also change hyperparameters from tunable to static, and from static to tunable, but the total number of static plus tunable hyperparameters must remain the same as it is in all parent jobs. The objective metric for the new tuning job must be the same as for all parent jobs.</p>
    /// </dd>
    /// </dl>
    pub fn get_warm_start_type(&self) -> &::std::option::Option<crate::types::HyperParameterTuningJobWarmStartType> {
        &self.warm_start_type
    }
    /// Consumes the builder and constructs a [`HyperParameterTuningJobWarmStartConfig`](crate::types::HyperParameterTuningJobWarmStartConfig).
    pub fn build(self) -> crate::types::HyperParameterTuningJobWarmStartConfig {
        crate::types::HyperParameterTuningJobWarmStartConfig {
            parent_hyper_parameter_tuning_jobs: self.parent_hyper_parameter_tuning_jobs,
            warm_start_type: self.warm_start_type,
        }
    }
}
