// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Describes the input source of a transform job and the way the transform job consumes it.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct TransformInput {
    /// <p>Describes the location of the channel data, which is, the S3 location of the input data that the model can consume.</p>
    #[doc(hidden)]
    pub data_source: std::option::Option<crate::types::TransformDataSource>,
    /// <p>The multipurpose internet mail extension (MIME) type of the data. Amazon SageMaker uses the MIME type with each http call to transfer data to the transform job.</p>
    #[doc(hidden)]
    pub content_type: std::option::Option<std::string::String>,
    /// <p>If your transform data is compressed, specify the compression type. Amazon SageMaker automatically decompresses the data for the transform job accordingly. The default value is <code>None</code>.</p>
    #[doc(hidden)]
    pub compression_type: std::option::Option<crate::types::CompressionType>,
    /// <p>The method to use to split the transform job's data files into smaller batches. Splitting is necessary when the total size of each object is too large to fit in a single request. You can also use data splitting to improve performance by processing multiple concurrent mini-batches. The default value for <code>SplitType</code> is <code>None</code>, which indicates that input data files are not split, and request payloads contain the entire contents of an input object. Set the value of this parameter to <code>Line</code> to split records on a newline character boundary. <code>SplitType</code> also supports a number of record-oriented binary data formats. Currently, the supported record formats are:</p>
    /// <ul>
    /// <li> <p>RecordIO</p> </li>
    /// <li> <p>TFRecord</p> </li>
    /// </ul>
    /// <p>When splitting is enabled, the size of a mini-batch depends on the values of the <code>BatchStrategy</code> and <code>MaxPayloadInMB</code> parameters. When the value of <code>BatchStrategy</code> is <code>MultiRecord</code>, Amazon SageMaker sends the maximum number of records in each request, up to the <code>MaxPayloadInMB</code> limit. If the value of <code>BatchStrategy</code> is <code>SingleRecord</code>, Amazon SageMaker sends individual records in each request.</p> <note>
    /// <p>Some data formats represent a record as a binary payload wrapped with extra padding bytes. When splitting is applied to a binary data format, padding is removed if the value of <code>BatchStrategy</code> is set to <code>SingleRecord</code>. Padding is not removed if the value of <code>BatchStrategy</code> is set to <code>MultiRecord</code>.</p>
    /// <p>For more information about <code>RecordIO</code>, see <a href="https://mxnet.apache.org/api/faq/recordio">Create a Dataset Using RecordIO</a> in the MXNet documentation. For more information about <code>TFRecord</code>, see <a href="https://www.tensorflow.org/guide/data#consuming_tfrecord_data">Consuming TFRecord data</a> in the TensorFlow documentation.</p>
    /// </note>
    #[doc(hidden)]
    pub split_type: std::option::Option<crate::types::SplitType>,
}
impl TransformInput {
    /// <p>Describes the location of the channel data, which is, the S3 location of the input data that the model can consume.</p>
    pub fn data_source(&self) -> std::option::Option<&crate::types::TransformDataSource> {
        self.data_source.as_ref()
    }
    /// <p>The multipurpose internet mail extension (MIME) type of the data. Amazon SageMaker uses the MIME type with each http call to transfer data to the transform job.</p>
    pub fn content_type(&self) -> std::option::Option<&str> {
        self.content_type.as_deref()
    }
    /// <p>If your transform data is compressed, specify the compression type. Amazon SageMaker automatically decompresses the data for the transform job accordingly. The default value is <code>None</code>.</p>
    pub fn compression_type(&self) -> std::option::Option<&crate::types::CompressionType> {
        self.compression_type.as_ref()
    }
    /// <p>The method to use to split the transform job's data files into smaller batches. Splitting is necessary when the total size of each object is too large to fit in a single request. You can also use data splitting to improve performance by processing multiple concurrent mini-batches. The default value for <code>SplitType</code> is <code>None</code>, which indicates that input data files are not split, and request payloads contain the entire contents of an input object. Set the value of this parameter to <code>Line</code> to split records on a newline character boundary. <code>SplitType</code> also supports a number of record-oriented binary data formats. Currently, the supported record formats are:</p>
    /// <ul>
    /// <li> <p>RecordIO</p> </li>
    /// <li> <p>TFRecord</p> </li>
    /// </ul>
    /// <p>When splitting is enabled, the size of a mini-batch depends on the values of the <code>BatchStrategy</code> and <code>MaxPayloadInMB</code> parameters. When the value of <code>BatchStrategy</code> is <code>MultiRecord</code>, Amazon SageMaker sends the maximum number of records in each request, up to the <code>MaxPayloadInMB</code> limit. If the value of <code>BatchStrategy</code> is <code>SingleRecord</code>, Amazon SageMaker sends individual records in each request.</p> <note>
    /// <p>Some data formats represent a record as a binary payload wrapped with extra padding bytes. When splitting is applied to a binary data format, padding is removed if the value of <code>BatchStrategy</code> is set to <code>SingleRecord</code>. Padding is not removed if the value of <code>BatchStrategy</code> is set to <code>MultiRecord</code>.</p>
    /// <p>For more information about <code>RecordIO</code>, see <a href="https://mxnet.apache.org/api/faq/recordio">Create a Dataset Using RecordIO</a> in the MXNet documentation. For more information about <code>TFRecord</code>, see <a href="https://www.tensorflow.org/guide/data#consuming_tfrecord_data">Consuming TFRecord data</a> in the TensorFlow documentation.</p>
    /// </note>
    pub fn split_type(&self) -> std::option::Option<&crate::types::SplitType> {
        self.split_type.as_ref()
    }
}
impl TransformInput {
    /// Creates a new builder-style object to manufacture [`TransformInput`](crate::types::TransformInput).
    pub fn builder() -> crate::types::builders::TransformInputBuilder {
        crate::types::builders::TransformInputBuilder::default()
    }
}

/// A builder for [`TransformInput`](crate::types::TransformInput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct TransformInputBuilder {
    pub(crate) data_source: std::option::Option<crate::types::TransformDataSource>,
    pub(crate) content_type: std::option::Option<std::string::String>,
    pub(crate) compression_type: std::option::Option<crate::types::CompressionType>,
    pub(crate) split_type: std::option::Option<crate::types::SplitType>,
}
impl TransformInputBuilder {
    /// <p>Describes the location of the channel data, which is, the S3 location of the input data that the model can consume.</p>
    pub fn data_source(mut self, input: crate::types::TransformDataSource) -> Self {
        self.data_source = Some(input);
        self
    }
    /// <p>Describes the location of the channel data, which is, the S3 location of the input data that the model can consume.</p>
    pub fn set_data_source(
        mut self,
        input: std::option::Option<crate::types::TransformDataSource>,
    ) -> Self {
        self.data_source = input;
        self
    }
    /// <p>The multipurpose internet mail extension (MIME) type of the data. Amazon SageMaker uses the MIME type with each http call to transfer data to the transform job.</p>
    pub fn content_type(mut self, input: impl Into<std::string::String>) -> Self {
        self.content_type = Some(input.into());
        self
    }
    /// <p>The multipurpose internet mail extension (MIME) type of the data. Amazon SageMaker uses the MIME type with each http call to transfer data to the transform job.</p>
    pub fn set_content_type(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.content_type = input;
        self
    }
    /// <p>If your transform data is compressed, specify the compression type. Amazon SageMaker automatically decompresses the data for the transform job accordingly. The default value is <code>None</code>.</p>
    pub fn compression_type(mut self, input: crate::types::CompressionType) -> Self {
        self.compression_type = Some(input);
        self
    }
    /// <p>If your transform data is compressed, specify the compression type. Amazon SageMaker automatically decompresses the data for the transform job accordingly. The default value is <code>None</code>.</p>
    pub fn set_compression_type(
        mut self,
        input: std::option::Option<crate::types::CompressionType>,
    ) -> Self {
        self.compression_type = input;
        self
    }
    /// <p>The method to use to split the transform job's data files into smaller batches. Splitting is necessary when the total size of each object is too large to fit in a single request. You can also use data splitting to improve performance by processing multiple concurrent mini-batches. The default value for <code>SplitType</code> is <code>None</code>, which indicates that input data files are not split, and request payloads contain the entire contents of an input object. Set the value of this parameter to <code>Line</code> to split records on a newline character boundary. <code>SplitType</code> also supports a number of record-oriented binary data formats. Currently, the supported record formats are:</p>
    /// <ul>
    /// <li> <p>RecordIO</p> </li>
    /// <li> <p>TFRecord</p> </li>
    /// </ul>
    /// <p>When splitting is enabled, the size of a mini-batch depends on the values of the <code>BatchStrategy</code> and <code>MaxPayloadInMB</code> parameters. When the value of <code>BatchStrategy</code> is <code>MultiRecord</code>, Amazon SageMaker sends the maximum number of records in each request, up to the <code>MaxPayloadInMB</code> limit. If the value of <code>BatchStrategy</code> is <code>SingleRecord</code>, Amazon SageMaker sends individual records in each request.</p> <note>
    /// <p>Some data formats represent a record as a binary payload wrapped with extra padding bytes. When splitting is applied to a binary data format, padding is removed if the value of <code>BatchStrategy</code> is set to <code>SingleRecord</code>. Padding is not removed if the value of <code>BatchStrategy</code> is set to <code>MultiRecord</code>.</p>
    /// <p>For more information about <code>RecordIO</code>, see <a href="https://mxnet.apache.org/api/faq/recordio">Create a Dataset Using RecordIO</a> in the MXNet documentation. For more information about <code>TFRecord</code>, see <a href="https://www.tensorflow.org/guide/data#consuming_tfrecord_data">Consuming TFRecord data</a> in the TensorFlow documentation.</p>
    /// </note>
    pub fn split_type(mut self, input: crate::types::SplitType) -> Self {
        self.split_type = Some(input);
        self
    }
    /// <p>The method to use to split the transform job's data files into smaller batches. Splitting is necessary when the total size of each object is too large to fit in a single request. You can also use data splitting to improve performance by processing multiple concurrent mini-batches. The default value for <code>SplitType</code> is <code>None</code>, which indicates that input data files are not split, and request payloads contain the entire contents of an input object. Set the value of this parameter to <code>Line</code> to split records on a newline character boundary. <code>SplitType</code> also supports a number of record-oriented binary data formats. Currently, the supported record formats are:</p>
    /// <ul>
    /// <li> <p>RecordIO</p> </li>
    /// <li> <p>TFRecord</p> </li>
    /// </ul>
    /// <p>When splitting is enabled, the size of a mini-batch depends on the values of the <code>BatchStrategy</code> and <code>MaxPayloadInMB</code> parameters. When the value of <code>BatchStrategy</code> is <code>MultiRecord</code>, Amazon SageMaker sends the maximum number of records in each request, up to the <code>MaxPayloadInMB</code> limit. If the value of <code>BatchStrategy</code> is <code>SingleRecord</code>, Amazon SageMaker sends individual records in each request.</p> <note>
    /// <p>Some data formats represent a record as a binary payload wrapped with extra padding bytes. When splitting is applied to a binary data format, padding is removed if the value of <code>BatchStrategy</code> is set to <code>SingleRecord</code>. Padding is not removed if the value of <code>BatchStrategy</code> is set to <code>MultiRecord</code>.</p>
    /// <p>For more information about <code>RecordIO</code>, see <a href="https://mxnet.apache.org/api/faq/recordio">Create a Dataset Using RecordIO</a> in the MXNet documentation. For more information about <code>TFRecord</code>, see <a href="https://www.tensorflow.org/guide/data#consuming_tfrecord_data">Consuming TFRecord data</a> in the TensorFlow documentation.</p>
    /// </note>
    pub fn set_split_type(mut self, input: std::option::Option<crate::types::SplitType>) -> Self {
        self.split_type = input;
        self
    }
    /// Consumes the builder and constructs a [`TransformInput`](crate::types::TransformInput).
    pub fn build(self) -> crate::types::TransformInput {
        crate::types::TransformInput {
            data_source: self.data_source,
            content_type: self.content_type,
            compression_type: self.compression_type,
            split_type: self.split_type,
        }
    }
}
