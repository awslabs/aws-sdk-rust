// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Contains information about the output location for the compiled model and the target device that the model runs on. <code>TargetDevice</code> and <code>TargetPlatform</code> are mutually exclusive, so you need to choose one between the two to specify your target device or platform. If you cannot find your device you want to use from the <code>TargetDevice</code> list, use <code>TargetPlatform</code> to describe the platform of your edge device and <code>CompilerOptions</code> if there are specific settings that are required or recommended to use for particular TargetPlatform.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct OutputConfig  {
    /// <p>Identifies the S3 bucket where you want Amazon SageMaker to store the model artifacts. For example, <code>s3://bucket-name/key-name-prefix</code>.</p>
    #[doc(hidden)]
    pub s3_output_location: std::option::Option<std::string::String>,
    /// <p>Identifies the target device or the machine learning instance that you want to run your model on after the compilation has completed. Alternatively, you can specify OS, architecture, and accelerator using <code>TargetPlatform</code> fields. It can be used instead of <code>TargetPlatform</code>.</p>
    #[doc(hidden)]
    pub target_device: std::option::Option<crate::types::TargetDevice>,
    /// <p>Contains information about a target platform that you want your model to run on, such as OS, architecture, and accelerators. It is an alternative of <code>TargetDevice</code>.</p> 
    /// <p>The following examples show how to configure the <code>TargetPlatform</code> and <code>CompilerOptions</code> JSON strings for popular target platforms: </p> 
    /// <ul> 
    /// <li> <p>Raspberry Pi 3 Model B+</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM_EABIHF"},</code> </p> <p> <code> "CompilerOptions": {'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>Jetson TX2</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'gpu-code': 'sm_62', 'trt-ver': '6.0.1', 'cuda-ver': '10.0'}</code> </p> </li> 
    /// <li> <p>EC2 m5.2xlarge instance OS</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "X86_64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p>RK3399</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "MALI"}</code> </p> </li> 
    /// <li> <p>ARMv7 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM_EABI"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 25, 'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>ARMv8 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM64"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 29}</code> </p> </li> 
    /// </ul>
    #[doc(hidden)]
    pub target_platform: std::option::Option<crate::types::TargetPlatform>,
    /// <p>Specifies additional parameters for compiler options in JSON format. The compiler options are <code>TargetPlatform</code> specific. It is required for NVIDIA accelerators and highly recommended for CPU compilations. For any other cases, it is optional to specify <code>CompilerOptions.</code> </p> 
    /// <ul> 
    /// <li> <p> <code>DTYPE</code>: Specifies the data type for the input. When compiling for <code>ml_*</code> (except for <code>ml_inf</code>) instances using PyTorch framework, provide the data type (dtype) of the model's input. <code>"float32"</code> is used if <code>"DTYPE"</code> is not specified. Options for data type are:</p> 
    /// <ul> 
    /// <li> <p>float32: Use either <code>"float"</code> or <code>"float32"</code>.</p> </li> 
    /// <li> <p>int64: Use either <code>"int64"</code> or <code>"long"</code>.</p> </li> 
    /// </ul> <p> For example, <code>{"dtype" : "float32"}</code>.</p> </li> 
    /// <li> <p> <code>CPU</code>: Compilation for CPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>mcpu</code>: CPU micro-architecture. For example, <code>{'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p> <code>mattr</code>: CPU flags. For example, <code>{'mattr': ['+neon', '+vfpv4']}</code> </p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>ARM</code>: Details of ARM CPU compilations.</p> 
    /// <ul> 
    /// <li> <p> <code>NEON</code>: NEON is an implementation of the Advanced SIMD extension used in ARMv7 processors.</p> <p>For example, add <code>{'mattr': ['+neon']}</code> to the compiler options if compiling for ARM 32-bit platform with the NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>NVIDIA</code>: Compilation for NVIDIA GPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>gpu_code</code>: Specifies the targeted architecture.</p> </li> 
    /// <li> <p> <code>trt-ver</code>: Specifies the TensorRT versions in x.y.z. format.</p> </li> 
    /// <li> <p> <code>cuda-ver</code>: Specifies the CUDA version in x.y format.</p> </li> 
    /// </ul> <p>For example, <code>{'gpu-code': 'sm_72', 'trt-ver': '6.0.1', 'cuda-ver': '10.1'}</code> </p> </li> 
    /// <li> <p> <code>ANDROID</code>: Compilation for the Android OS supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>ANDROID_PLATFORM</code>: Specifies the Android API levels. Available levels range from 21 to 29. For example, <code>{'ANDROID_PLATFORM': 28}</code>.</p> </li> 
    /// <li> <p> <code>mattr</code>: Add <code>{'mattr': ['+neon']}</code> to compiler options if compiling for ARM 32-bit platform with NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>INFERENTIA</code>: Compilation for target ml_inf1 uses compiler options passed in as a JSON string. For example, <code>"CompilerOptions": "\"--verbose 1 --num-neuroncores 2 -O2\""</code>. </p> <p>For information about supported compiler options, see <a href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-cc/command-line-reference.md"> Neuron Compiler CLI</a>. </p> </li> 
    /// <li> <p> <code>CoreML</code>: Compilation for the CoreML <code>OutputConfig$TargetDevice</code> supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>class_labels</code>: Specifies the classification labels file name inside input tar.gz file. For example, <code>{"class_labels": "imagenet_labels_1000.txt"}</code>. Labels inside the txt file should be separated by newlines.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>EIA</code>: Compilation for the Elastic Inference Accelerator supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>precision_mode</code>: Specifies the precision of compiled artifacts. Supported values are <code>"FP16"</code> and <code>"FP32"</code>. Default is <code>"FP32"</code>.</p> </li> 
    /// <li> <p> <code>signature_def_key</code>: Specifies the signature to use for models in SavedModel format. Defaults is TensorFlow's default signature def key.</p> </li> 
    /// <li> <p> <code>output_names</code>: Specifies a list of output tensor names for models in FrozenGraph format. Set at most one API field, either: <code>signature_def_key</code> or <code>output_names</code>.</p> </li> 
    /// </ul> <p>For example: <code>{"precision_mode": "FP32", "output_names": ["output:0"]}</code> </p> </li> 
    /// </ul>
    #[doc(hidden)]
    pub compiler_options: std::option::Option<std::string::String>,
    /// <p>The Amazon Web Services Key Management Service key (Amazon Web Services KMS) that Amazon SageMaker uses to encrypt your output models with Amazon S3 server-side encryption after compilation job. If you don't provide a KMS key ID, Amazon SageMaker uses the default KMS key for Amazon S3 for your role's account. For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html">KMS-Managed Encryption Keys</a> in the <i>Amazon Simple Storage Service Developer Guide.</i> </p> 
    /// <p>The KmsKeyId can be any of the following formats: </p> 
    /// <ul> 
    /// <li> <p>Key ID: <code>1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Key ARN: <code>arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Alias name: <code>alias/ExampleAlias</code> </p> </li> 
    /// <li> <p>Alias name ARN: <code>arn:aws:kms:us-west-2:111122223333:alias/ExampleAlias</code> </p> </li> 
    /// </ul>
    #[doc(hidden)]
    pub kms_key_id: std::option::Option<std::string::String>,
}
impl OutputConfig {
    /// <p>Identifies the S3 bucket where you want Amazon SageMaker to store the model artifacts. For example, <code>s3://bucket-name/key-name-prefix</code>.</p>
    pub fn s3_output_location(&self) -> std::option::Option<& str> {
        self.s3_output_location.as_deref()
    }
    /// <p>Identifies the target device or the machine learning instance that you want to run your model on after the compilation has completed. Alternatively, you can specify OS, architecture, and accelerator using <code>TargetPlatform</code> fields. It can be used instead of <code>TargetPlatform</code>.</p>
    pub fn target_device(&self) -> std::option::Option<& crate::types::TargetDevice> {
        self.target_device.as_ref()
    }
    /// <p>Contains information about a target platform that you want your model to run on, such as OS, architecture, and accelerators. It is an alternative of <code>TargetDevice</code>.</p> 
    /// <p>The following examples show how to configure the <code>TargetPlatform</code> and <code>CompilerOptions</code> JSON strings for popular target platforms: </p> 
    /// <ul> 
    /// <li> <p>Raspberry Pi 3 Model B+</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM_EABIHF"},</code> </p> <p> <code> "CompilerOptions": {'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>Jetson TX2</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'gpu-code': 'sm_62', 'trt-ver': '6.0.1', 'cuda-ver': '10.0'}</code> </p> </li> 
    /// <li> <p>EC2 m5.2xlarge instance OS</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "X86_64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p>RK3399</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "MALI"}</code> </p> </li> 
    /// <li> <p>ARMv7 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM_EABI"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 25, 'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>ARMv8 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM64"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 29}</code> </p> </li> 
    /// </ul>
    pub fn target_platform(&self) -> std::option::Option<& crate::types::TargetPlatform> {
        self.target_platform.as_ref()
    }
    /// <p>Specifies additional parameters for compiler options in JSON format. The compiler options are <code>TargetPlatform</code> specific. It is required for NVIDIA accelerators and highly recommended for CPU compilations. For any other cases, it is optional to specify <code>CompilerOptions.</code> </p> 
    /// <ul> 
    /// <li> <p> <code>DTYPE</code>: Specifies the data type for the input. When compiling for <code>ml_*</code> (except for <code>ml_inf</code>) instances using PyTorch framework, provide the data type (dtype) of the model's input. <code>"float32"</code> is used if <code>"DTYPE"</code> is not specified. Options for data type are:</p> 
    /// <ul> 
    /// <li> <p>float32: Use either <code>"float"</code> or <code>"float32"</code>.</p> </li> 
    /// <li> <p>int64: Use either <code>"int64"</code> or <code>"long"</code>.</p> </li> 
    /// </ul> <p> For example, <code>{"dtype" : "float32"}</code>.</p> </li> 
    /// <li> <p> <code>CPU</code>: Compilation for CPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>mcpu</code>: CPU micro-architecture. For example, <code>{'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p> <code>mattr</code>: CPU flags. For example, <code>{'mattr': ['+neon', '+vfpv4']}</code> </p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>ARM</code>: Details of ARM CPU compilations.</p> 
    /// <ul> 
    /// <li> <p> <code>NEON</code>: NEON is an implementation of the Advanced SIMD extension used in ARMv7 processors.</p> <p>For example, add <code>{'mattr': ['+neon']}</code> to the compiler options if compiling for ARM 32-bit platform with the NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>NVIDIA</code>: Compilation for NVIDIA GPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>gpu_code</code>: Specifies the targeted architecture.</p> </li> 
    /// <li> <p> <code>trt-ver</code>: Specifies the TensorRT versions in x.y.z. format.</p> </li> 
    /// <li> <p> <code>cuda-ver</code>: Specifies the CUDA version in x.y format.</p> </li> 
    /// </ul> <p>For example, <code>{'gpu-code': 'sm_72', 'trt-ver': '6.0.1', 'cuda-ver': '10.1'}</code> </p> </li> 
    /// <li> <p> <code>ANDROID</code>: Compilation for the Android OS supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>ANDROID_PLATFORM</code>: Specifies the Android API levels. Available levels range from 21 to 29. For example, <code>{'ANDROID_PLATFORM': 28}</code>.</p> </li> 
    /// <li> <p> <code>mattr</code>: Add <code>{'mattr': ['+neon']}</code> to compiler options if compiling for ARM 32-bit platform with NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>INFERENTIA</code>: Compilation for target ml_inf1 uses compiler options passed in as a JSON string. For example, <code>"CompilerOptions": "\"--verbose 1 --num-neuroncores 2 -O2\""</code>. </p> <p>For information about supported compiler options, see <a href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-cc/command-line-reference.md"> Neuron Compiler CLI</a>. </p> </li> 
    /// <li> <p> <code>CoreML</code>: Compilation for the CoreML <code>OutputConfig$TargetDevice</code> supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>class_labels</code>: Specifies the classification labels file name inside input tar.gz file. For example, <code>{"class_labels": "imagenet_labels_1000.txt"}</code>. Labels inside the txt file should be separated by newlines.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>EIA</code>: Compilation for the Elastic Inference Accelerator supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>precision_mode</code>: Specifies the precision of compiled artifacts. Supported values are <code>"FP16"</code> and <code>"FP32"</code>. Default is <code>"FP32"</code>.</p> </li> 
    /// <li> <p> <code>signature_def_key</code>: Specifies the signature to use for models in SavedModel format. Defaults is TensorFlow's default signature def key.</p> </li> 
    /// <li> <p> <code>output_names</code>: Specifies a list of output tensor names for models in FrozenGraph format. Set at most one API field, either: <code>signature_def_key</code> or <code>output_names</code>.</p> </li> 
    /// </ul> <p>For example: <code>{"precision_mode": "FP32", "output_names": ["output:0"]}</code> </p> </li> 
    /// </ul>
    pub fn compiler_options(&self) -> std::option::Option<& str> {
        self.compiler_options.as_deref()
    }
    /// <p>The Amazon Web Services Key Management Service key (Amazon Web Services KMS) that Amazon SageMaker uses to encrypt your output models with Amazon S3 server-side encryption after compilation job. If you don't provide a KMS key ID, Amazon SageMaker uses the default KMS key for Amazon S3 for your role's account. For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html">KMS-Managed Encryption Keys</a> in the <i>Amazon Simple Storage Service Developer Guide.</i> </p> 
    /// <p>The KmsKeyId can be any of the following formats: </p> 
    /// <ul> 
    /// <li> <p>Key ID: <code>1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Key ARN: <code>arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Alias name: <code>alias/ExampleAlias</code> </p> </li> 
    /// <li> <p>Alias name ARN: <code>arn:aws:kms:us-west-2:111122223333:alias/ExampleAlias</code> </p> </li> 
    /// </ul>
    pub fn kms_key_id(&self) -> std::option::Option<& str> {
        self.kms_key_id.as_deref()
    }
}
impl OutputConfig {
    /// Creates a new builder-style object to manufacture [`OutputConfig`](crate::types::OutputConfig).
    pub fn builder() -> crate::types::builders::OutputConfigBuilder {
        crate::types::builders::OutputConfigBuilder::default()
    }
}

/// A builder for [`OutputConfig`](crate::types::OutputConfig).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct OutputConfigBuilder {
    pub(crate) s3_output_location: std::option::Option<std::string::String>,
    pub(crate) target_device: std::option::Option<crate::types::TargetDevice>,
    pub(crate) target_platform: std::option::Option<crate::types::TargetPlatform>,
    pub(crate) compiler_options: std::option::Option<std::string::String>,
    pub(crate) kms_key_id: std::option::Option<std::string::String>,
}
impl OutputConfigBuilder {
    /// <p>Identifies the S3 bucket where you want Amazon SageMaker to store the model artifacts. For example, <code>s3://bucket-name/key-name-prefix</code>.</p>
    pub fn s3_output_location(mut self, input: impl Into<std::string::String>) -> Self {
        self.s3_output_location = Some(input.into());
        self
    }
    /// <p>Identifies the S3 bucket where you want Amazon SageMaker to store the model artifacts. For example, <code>s3://bucket-name/key-name-prefix</code>.</p>
    pub fn set_s3_output_location(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.s3_output_location = input; self
    }
    /// <p>Identifies the target device or the machine learning instance that you want to run your model on after the compilation has completed. Alternatively, you can specify OS, architecture, and accelerator using <code>TargetPlatform</code> fields. It can be used instead of <code>TargetPlatform</code>.</p>
    pub fn target_device(mut self, input: crate::types::TargetDevice) -> Self {
        self.target_device = Some(input);
        self
    }
    /// <p>Identifies the target device or the machine learning instance that you want to run your model on after the compilation has completed. Alternatively, you can specify OS, architecture, and accelerator using <code>TargetPlatform</code> fields. It can be used instead of <code>TargetPlatform</code>.</p>
    pub fn set_target_device(mut self, input: std::option::Option<crate::types::TargetDevice>) -> Self {
        self.target_device = input; self
    }
    /// <p>Contains information about a target platform that you want your model to run on, such as OS, architecture, and accelerators. It is an alternative of <code>TargetDevice</code>.</p> 
    /// <p>The following examples show how to configure the <code>TargetPlatform</code> and <code>CompilerOptions</code> JSON strings for popular target platforms: </p> 
    /// <ul> 
    /// <li> <p>Raspberry Pi 3 Model B+</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM_EABIHF"},</code> </p> <p> <code> "CompilerOptions": {'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>Jetson TX2</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'gpu-code': 'sm_62', 'trt-ver': '6.0.1', 'cuda-ver': '10.0'}</code> </p> </li> 
    /// <li> <p>EC2 m5.2xlarge instance OS</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "X86_64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p>RK3399</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "MALI"}</code> </p> </li> 
    /// <li> <p>ARMv7 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM_EABI"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 25, 'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>ARMv8 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM64"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 29}</code> </p> </li> 
    /// </ul>
    pub fn target_platform(mut self, input: crate::types::TargetPlatform) -> Self {
        self.target_platform = Some(input);
        self
    }
    /// <p>Contains information about a target platform that you want your model to run on, such as OS, architecture, and accelerators. It is an alternative of <code>TargetDevice</code>.</p> 
    /// <p>The following examples show how to configure the <code>TargetPlatform</code> and <code>CompilerOptions</code> JSON strings for popular target platforms: </p> 
    /// <ul> 
    /// <li> <p>Raspberry Pi 3 Model B+</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM_EABIHF"},</code> </p> <p> <code> "CompilerOptions": {'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>Jetson TX2</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'gpu-code': 'sm_62', 'trt-ver': '6.0.1', 'cuda-ver': '10.0'}</code> </p> </li> 
    /// <li> <p>EC2 m5.2xlarge instance OS</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "X86_64", "Accelerator": "NVIDIA"},</code> </p> <p> <code> "CompilerOptions": {'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p>RK3399</p> <p> <code>"TargetPlatform": {"Os": "LINUX", "Arch": "ARM64", "Accelerator": "MALI"}</code> </p> </li> 
    /// <li> <p>ARMv7 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM_EABI"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 25, 'mattr': ['+neon']}</code> </p> </li> 
    /// <li> <p>ARMv8 phone (CPU)</p> <p> <code>"TargetPlatform": {"Os": "ANDROID", "Arch": "ARM64"},</code> </p> <p> <code> "CompilerOptions": {'ANDROID_PLATFORM': 29}</code> </p> </li> 
    /// </ul>
    pub fn set_target_platform(mut self, input: std::option::Option<crate::types::TargetPlatform>) -> Self {
        self.target_platform = input; self
    }
    /// <p>Specifies additional parameters for compiler options in JSON format. The compiler options are <code>TargetPlatform</code> specific. It is required for NVIDIA accelerators and highly recommended for CPU compilations. For any other cases, it is optional to specify <code>CompilerOptions.</code> </p> 
    /// <ul> 
    /// <li> <p> <code>DTYPE</code>: Specifies the data type for the input. When compiling for <code>ml_*</code> (except for <code>ml_inf</code>) instances using PyTorch framework, provide the data type (dtype) of the model's input. <code>"float32"</code> is used if <code>"DTYPE"</code> is not specified. Options for data type are:</p> 
    /// <ul> 
    /// <li> <p>float32: Use either <code>"float"</code> or <code>"float32"</code>.</p> </li> 
    /// <li> <p>int64: Use either <code>"int64"</code> or <code>"long"</code>.</p> </li> 
    /// </ul> <p> For example, <code>{"dtype" : "float32"}</code>.</p> </li> 
    /// <li> <p> <code>CPU</code>: Compilation for CPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>mcpu</code>: CPU micro-architecture. For example, <code>{'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p> <code>mattr</code>: CPU flags. For example, <code>{'mattr': ['+neon', '+vfpv4']}</code> </p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>ARM</code>: Details of ARM CPU compilations.</p> 
    /// <ul> 
    /// <li> <p> <code>NEON</code>: NEON is an implementation of the Advanced SIMD extension used in ARMv7 processors.</p> <p>For example, add <code>{'mattr': ['+neon']}</code> to the compiler options if compiling for ARM 32-bit platform with the NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>NVIDIA</code>: Compilation for NVIDIA GPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>gpu_code</code>: Specifies the targeted architecture.</p> </li> 
    /// <li> <p> <code>trt-ver</code>: Specifies the TensorRT versions in x.y.z. format.</p> </li> 
    /// <li> <p> <code>cuda-ver</code>: Specifies the CUDA version in x.y format.</p> </li> 
    /// </ul> <p>For example, <code>{'gpu-code': 'sm_72', 'trt-ver': '6.0.1', 'cuda-ver': '10.1'}</code> </p> </li> 
    /// <li> <p> <code>ANDROID</code>: Compilation for the Android OS supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>ANDROID_PLATFORM</code>: Specifies the Android API levels. Available levels range from 21 to 29. For example, <code>{'ANDROID_PLATFORM': 28}</code>.</p> </li> 
    /// <li> <p> <code>mattr</code>: Add <code>{'mattr': ['+neon']}</code> to compiler options if compiling for ARM 32-bit platform with NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>INFERENTIA</code>: Compilation for target ml_inf1 uses compiler options passed in as a JSON string. For example, <code>"CompilerOptions": "\"--verbose 1 --num-neuroncores 2 -O2\""</code>. </p> <p>For information about supported compiler options, see <a href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-cc/command-line-reference.md"> Neuron Compiler CLI</a>. </p> </li> 
    /// <li> <p> <code>CoreML</code>: Compilation for the CoreML <code>OutputConfig$TargetDevice</code> supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>class_labels</code>: Specifies the classification labels file name inside input tar.gz file. For example, <code>{"class_labels": "imagenet_labels_1000.txt"}</code>. Labels inside the txt file should be separated by newlines.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>EIA</code>: Compilation for the Elastic Inference Accelerator supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>precision_mode</code>: Specifies the precision of compiled artifacts. Supported values are <code>"FP16"</code> and <code>"FP32"</code>. Default is <code>"FP32"</code>.</p> </li> 
    /// <li> <p> <code>signature_def_key</code>: Specifies the signature to use for models in SavedModel format. Defaults is TensorFlow's default signature def key.</p> </li> 
    /// <li> <p> <code>output_names</code>: Specifies a list of output tensor names for models in FrozenGraph format. Set at most one API field, either: <code>signature_def_key</code> or <code>output_names</code>.</p> </li> 
    /// </ul> <p>For example: <code>{"precision_mode": "FP32", "output_names": ["output:0"]}</code> </p> </li> 
    /// </ul>
    pub fn compiler_options(mut self, input: impl Into<std::string::String>) -> Self {
        self.compiler_options = Some(input.into());
        self
    }
    /// <p>Specifies additional parameters for compiler options in JSON format. The compiler options are <code>TargetPlatform</code> specific. It is required for NVIDIA accelerators and highly recommended for CPU compilations. For any other cases, it is optional to specify <code>CompilerOptions.</code> </p> 
    /// <ul> 
    /// <li> <p> <code>DTYPE</code>: Specifies the data type for the input. When compiling for <code>ml_*</code> (except for <code>ml_inf</code>) instances using PyTorch framework, provide the data type (dtype) of the model's input. <code>"float32"</code> is used if <code>"DTYPE"</code> is not specified. Options for data type are:</p> 
    /// <ul> 
    /// <li> <p>float32: Use either <code>"float"</code> or <code>"float32"</code>.</p> </li> 
    /// <li> <p>int64: Use either <code>"int64"</code> or <code>"long"</code>.</p> </li> 
    /// </ul> <p> For example, <code>{"dtype" : "float32"}</code>.</p> </li> 
    /// <li> <p> <code>CPU</code>: Compilation for CPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>mcpu</code>: CPU micro-architecture. For example, <code>{'mcpu': 'skylake-avx512'}</code> </p> </li> 
    /// <li> <p> <code>mattr</code>: CPU flags. For example, <code>{'mattr': ['+neon', '+vfpv4']}</code> </p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>ARM</code>: Details of ARM CPU compilations.</p> 
    /// <ul> 
    /// <li> <p> <code>NEON</code>: NEON is an implementation of the Advanced SIMD extension used in ARMv7 processors.</p> <p>For example, add <code>{'mattr': ['+neon']}</code> to the compiler options if compiling for ARM 32-bit platform with the NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>NVIDIA</code>: Compilation for NVIDIA GPU supports the following compiler options.</p> 
    /// <ul> 
    /// <li> <p> <code>gpu_code</code>: Specifies the targeted architecture.</p> </li> 
    /// <li> <p> <code>trt-ver</code>: Specifies the TensorRT versions in x.y.z. format.</p> </li> 
    /// <li> <p> <code>cuda-ver</code>: Specifies the CUDA version in x.y format.</p> </li> 
    /// </ul> <p>For example, <code>{'gpu-code': 'sm_72', 'trt-ver': '6.0.1', 'cuda-ver': '10.1'}</code> </p> </li> 
    /// <li> <p> <code>ANDROID</code>: Compilation for the Android OS supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>ANDROID_PLATFORM</code>: Specifies the Android API levels. Available levels range from 21 to 29. For example, <code>{'ANDROID_PLATFORM': 28}</code>.</p> </li> 
    /// <li> <p> <code>mattr</code>: Add <code>{'mattr': ['+neon']}</code> to compiler options if compiling for ARM 32-bit platform with NEON support.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>INFERENTIA</code>: Compilation for target ml_inf1 uses compiler options passed in as a JSON string. For example, <code>"CompilerOptions": "\"--verbose 1 --num-neuroncores 2 -O2\""</code>. </p> <p>For information about supported compiler options, see <a href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-cc/command-line-reference.md"> Neuron Compiler CLI</a>. </p> </li> 
    /// <li> <p> <code>CoreML</code>: Compilation for the CoreML <code>OutputConfig$TargetDevice</code> supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>class_labels</code>: Specifies the classification labels file name inside input tar.gz file. For example, <code>{"class_labels": "imagenet_labels_1000.txt"}</code>. Labels inside the txt file should be separated by newlines.</p> </li> 
    /// </ul> </li> 
    /// <li> <p> <code>EIA</code>: Compilation for the Elastic Inference Accelerator supports the following compiler options:</p> 
    /// <ul> 
    /// <li> <p> <code>precision_mode</code>: Specifies the precision of compiled artifacts. Supported values are <code>"FP16"</code> and <code>"FP32"</code>. Default is <code>"FP32"</code>.</p> </li> 
    /// <li> <p> <code>signature_def_key</code>: Specifies the signature to use for models in SavedModel format. Defaults is TensorFlow's default signature def key.</p> </li> 
    /// <li> <p> <code>output_names</code>: Specifies a list of output tensor names for models in FrozenGraph format. Set at most one API field, either: <code>signature_def_key</code> or <code>output_names</code>.</p> </li> 
    /// </ul> <p>For example: <code>{"precision_mode": "FP32", "output_names": ["output:0"]}</code> </p> </li> 
    /// </ul>
    pub fn set_compiler_options(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.compiler_options = input; self
    }
    /// <p>The Amazon Web Services Key Management Service key (Amazon Web Services KMS) that Amazon SageMaker uses to encrypt your output models with Amazon S3 server-side encryption after compilation job. If you don't provide a KMS key ID, Amazon SageMaker uses the default KMS key for Amazon S3 for your role's account. For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html">KMS-Managed Encryption Keys</a> in the <i>Amazon Simple Storage Service Developer Guide.</i> </p> 
    /// <p>The KmsKeyId can be any of the following formats: </p> 
    /// <ul> 
    /// <li> <p>Key ID: <code>1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Key ARN: <code>arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Alias name: <code>alias/ExampleAlias</code> </p> </li> 
    /// <li> <p>Alias name ARN: <code>arn:aws:kms:us-west-2:111122223333:alias/ExampleAlias</code> </p> </li> 
    /// </ul>
    pub fn kms_key_id(mut self, input: impl Into<std::string::String>) -> Self {
        self.kms_key_id = Some(input.into());
        self
    }
    /// <p>The Amazon Web Services Key Management Service key (Amazon Web Services KMS) that Amazon SageMaker uses to encrypt your output models with Amazon S3 server-side encryption after compilation job. If you don't provide a KMS key ID, Amazon SageMaker uses the default KMS key for Amazon S3 for your role's account. For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html">KMS-Managed Encryption Keys</a> in the <i>Amazon Simple Storage Service Developer Guide.</i> </p> 
    /// <p>The KmsKeyId can be any of the following formats: </p> 
    /// <ul> 
    /// <li> <p>Key ID: <code>1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Key ARN: <code>arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab</code> </p> </li> 
    /// <li> <p>Alias name: <code>alias/ExampleAlias</code> </p> </li> 
    /// <li> <p>Alias name ARN: <code>arn:aws:kms:us-west-2:111122223333:alias/ExampleAlias</code> </p> </li> 
    /// </ul>
    pub fn set_kms_key_id(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.kms_key_id = input; self
    }
    /// Consumes the builder and constructs a [`OutputConfig`](crate::types::OutputConfig).
    pub fn build(self) -> crate::types::OutputConfig {
        crate::types::OutputConfig {
            s3_output_location: self.s3_output_location
            ,
            target_device: self.target_device
            ,
            target_platform: self.target_platform
            ,
            compiler_options: self.compiler_options
            ,
            kms_key_id: self.kms_key_id
            ,
        }
    }
}

