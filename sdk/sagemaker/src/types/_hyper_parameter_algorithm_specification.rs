// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Specifies which training algorithm to use for training jobs that a hyperparameter tuning job launches and the metrics to monitor.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct HyperParameterAlgorithmSpecification  {
    /// <p> The registry path of the Docker image that contains the training algorithm. For information about Docker registry paths for built-in algorithms, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">Algorithms Provided by Amazon SageMaker: Common Parameters</a>. SageMaker supports both <code>registry/repository[:tag]</code> and <code>registry/repository[@digest]</code> image path formats. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html">Using Your Own Algorithms with Amazon SageMaker</a>.</p>
    #[doc(hidden)]
    pub training_image: std::option::Option<std::string::String>,
    /// <p>The training input mode that the algorithm supports. For more information about input modes, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">Algorithms</a>.</p> 
    /// <p> <b>Pipe mode</b> </p> 
    /// <p>If an algorithm supports <code>Pipe</code> mode, Amazon SageMaker streams data directly from Amazon S3 to the container.</p> 
    /// <p> <b>File mode</b> </p> 
    /// <p>If an algorithm supports <code>File</code> mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container.</p> 
    /// <p>You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any.</p> 
    /// <p>For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training.</p> 
    /// <p> <b>FastFile mode</b> </p> 
    /// <p>If an algorithm supports <code>FastFile</code> mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk.</p> 
    /// <p> <code>FastFile</code> mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.</p>
    #[doc(hidden)]
    pub training_input_mode: std::option::Option<crate::types::TrainingInputMode>,
    /// <p>The name of the resource algorithm to use for the hyperparameter tuning job. If you specify a value for this parameter, do not specify a value for <code>TrainingImage</code>.</p>
    #[doc(hidden)]
    pub algorithm_name: std::option::Option<std::string::String>,
    /// <p>An array of <code>MetricDefinition</code> objects that specify the metrics that the algorithm emits.</p>
    #[doc(hidden)]
    pub metric_definitions: std::option::Option<std::vec::Vec<crate::types::MetricDefinition>>,
}
impl HyperParameterAlgorithmSpecification {
    /// <p> The registry path of the Docker image that contains the training algorithm. For information about Docker registry paths for built-in algorithms, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">Algorithms Provided by Amazon SageMaker: Common Parameters</a>. SageMaker supports both <code>registry/repository[:tag]</code> and <code>registry/repository[@digest]</code> image path formats. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html">Using Your Own Algorithms with Amazon SageMaker</a>.</p>
    pub fn training_image(&self) -> std::option::Option<& str> {
        self.training_image.as_deref()
    }
    /// <p>The training input mode that the algorithm supports. For more information about input modes, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">Algorithms</a>.</p> 
    /// <p> <b>Pipe mode</b> </p> 
    /// <p>If an algorithm supports <code>Pipe</code> mode, Amazon SageMaker streams data directly from Amazon S3 to the container.</p> 
    /// <p> <b>File mode</b> </p> 
    /// <p>If an algorithm supports <code>File</code> mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container.</p> 
    /// <p>You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any.</p> 
    /// <p>For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training.</p> 
    /// <p> <b>FastFile mode</b> </p> 
    /// <p>If an algorithm supports <code>FastFile</code> mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk.</p> 
    /// <p> <code>FastFile</code> mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.</p>
    pub fn training_input_mode(&self) -> std::option::Option<& crate::types::TrainingInputMode> {
        self.training_input_mode.as_ref()
    }
    /// <p>The name of the resource algorithm to use for the hyperparameter tuning job. If you specify a value for this parameter, do not specify a value for <code>TrainingImage</code>.</p>
    pub fn algorithm_name(&self) -> std::option::Option<& str> {
        self.algorithm_name.as_deref()
    }
    /// <p>An array of <code>MetricDefinition</code> objects that specify the metrics that the algorithm emits.</p>
    pub fn metric_definitions(&self) -> std::option::Option<& [crate::types::MetricDefinition]> {
        self.metric_definitions.as_deref()
    }
}
impl HyperParameterAlgorithmSpecification {
    /// Creates a new builder-style object to manufacture [`HyperParameterAlgorithmSpecification`](crate::types::HyperParameterAlgorithmSpecification).
    pub fn builder() -> crate::types::builders::HyperParameterAlgorithmSpecificationBuilder {
        crate::types::builders::HyperParameterAlgorithmSpecificationBuilder::default()
    }
}

/// A builder for [`HyperParameterAlgorithmSpecification`](crate::types::HyperParameterAlgorithmSpecification).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct HyperParameterAlgorithmSpecificationBuilder {
    pub(crate) training_image: std::option::Option<std::string::String>,
    pub(crate) training_input_mode: std::option::Option<crate::types::TrainingInputMode>,
    pub(crate) algorithm_name: std::option::Option<std::string::String>,
    pub(crate) metric_definitions: std::option::Option<std::vec::Vec<crate::types::MetricDefinition>>,
}
impl HyperParameterAlgorithmSpecificationBuilder {
    /// <p> The registry path of the Docker image that contains the training algorithm. For information about Docker registry paths for built-in algorithms, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">Algorithms Provided by Amazon SageMaker: Common Parameters</a>. SageMaker supports both <code>registry/repository[:tag]</code> and <code>registry/repository[@digest]</code> image path formats. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html">Using Your Own Algorithms with Amazon SageMaker</a>.</p>
    pub fn training_image(mut self, input: impl Into<std::string::String>) -> Self {
        self.training_image = Some(input.into());
        self
    }
    /// <p> The registry path of the Docker image that contains the training algorithm. For information about Docker registry paths for built-in algorithms, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">Algorithms Provided by Amazon SageMaker: Common Parameters</a>. SageMaker supports both <code>registry/repository[:tag]</code> and <code>registry/repository[@digest]</code> image path formats. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html">Using Your Own Algorithms with Amazon SageMaker</a>.</p>
    pub fn set_training_image(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.training_image = input; self
    }
    /// <p>The training input mode that the algorithm supports. For more information about input modes, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">Algorithms</a>.</p> 
    /// <p> <b>Pipe mode</b> </p> 
    /// <p>If an algorithm supports <code>Pipe</code> mode, Amazon SageMaker streams data directly from Amazon S3 to the container.</p> 
    /// <p> <b>File mode</b> </p> 
    /// <p>If an algorithm supports <code>File</code> mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container.</p> 
    /// <p>You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any.</p> 
    /// <p>For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training.</p> 
    /// <p> <b>FastFile mode</b> </p> 
    /// <p>If an algorithm supports <code>FastFile</code> mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk.</p> 
    /// <p> <code>FastFile</code> mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.</p>
    pub fn training_input_mode(mut self, input: crate::types::TrainingInputMode) -> Self {
        self.training_input_mode = Some(input);
        self
    }
    /// <p>The training input mode that the algorithm supports. For more information about input modes, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">Algorithms</a>.</p> 
    /// <p> <b>Pipe mode</b> </p> 
    /// <p>If an algorithm supports <code>Pipe</code> mode, Amazon SageMaker streams data directly from Amazon S3 to the container.</p> 
    /// <p> <b>File mode</b> </p> 
    /// <p>If an algorithm supports <code>File</code> mode, SageMaker downloads the training data from S3 to the provisioned ML storage volume, and mounts the directory to the Docker volume for the training container.</p> 
    /// <p>You must provision the ML storage volume with sufficient capacity to accommodate the data downloaded from S3. In addition to the training data, the ML storage volume also stores the output model. The algorithm container uses the ML storage volume to also store intermediate information, if any.</p> 
    /// <p>For distributed algorithms, training data is distributed uniformly. Your training duration is predictable if the input data objects sizes are approximately the same. SageMaker does not split the files any further for model training. If the object sizes are skewed, training won't be optimal as the data distribution is also skewed when one host in a training cluster is overloaded, thus becoming a bottleneck in training.</p> 
    /// <p> <b>FastFile mode</b> </p> 
    /// <p>If an algorithm supports <code>FastFile</code> mode, SageMaker streams data directly from S3 to the container with no code changes, and provides file system access to the data. Users can author their training script to interact with these files as if they were stored on disk.</p> 
    /// <p> <code>FastFile</code> mode works best when the data is read sequentially. Augmented manifest files aren't supported. The startup time is lower when there are fewer files in the S3 bucket provided.</p>
    pub fn set_training_input_mode(mut self, input: std::option::Option<crate::types::TrainingInputMode>) -> Self {
        self.training_input_mode = input; self
    }
    /// <p>The name of the resource algorithm to use for the hyperparameter tuning job. If you specify a value for this parameter, do not specify a value for <code>TrainingImage</code>.</p>
    pub fn algorithm_name(mut self, input: impl Into<std::string::String>) -> Self {
        self.algorithm_name = Some(input.into());
        self
    }
    /// <p>The name of the resource algorithm to use for the hyperparameter tuning job. If you specify a value for this parameter, do not specify a value for <code>TrainingImage</code>.</p>
    pub fn set_algorithm_name(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.algorithm_name = input; self
    }
    /// Appends an item to `metric_definitions`.
    ///
    /// To override the contents of this collection use [`set_metric_definitions`](Self::set_metric_definitions).
    ///
    /// <p>An array of <code>MetricDefinition</code> objects that specify the metrics that the algorithm emits.</p>
    pub fn metric_definitions(mut self, input: crate::types::MetricDefinition) -> Self {
        let mut v = self.metric_definitions.unwrap_or_default();
                        v.push(input);
                        self.metric_definitions = Some(v);
                        self
    }
    /// <p>An array of <code>MetricDefinition</code> objects that specify the metrics that the algorithm emits.</p>
    pub fn set_metric_definitions(mut self, input: std::option::Option<std::vec::Vec<crate::types::MetricDefinition>>) -> Self {
        self.metric_definitions = input; self
    }
    /// Consumes the builder and constructs a [`HyperParameterAlgorithmSpecification`](crate::types::HyperParameterAlgorithmSpecification).
    pub fn build(self) -> crate::types::HyperParameterAlgorithmSpecification {
        crate::types::HyperParameterAlgorithmSpecification {
            training_image: self.training_image
            ,
            training_input_mode: self.training_input_mode
            ,
            algorithm_name: self.algorithm_name
            ,
            metric_definitions: self.metric_definitions
            ,
        }
    }
}

