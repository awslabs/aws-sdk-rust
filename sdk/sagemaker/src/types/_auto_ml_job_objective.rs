// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Specifies a metric to minimize or maximize as the objective of an AutoML job.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct AutoMlJobObjective {
    /// <p>The name of the objective metric used to measure the predictive quality of a machine learning system. During training, the model's parameters are updated iteratively to optimize its performance based on the feedback provided by the objective metric when evaluating the model on the validation dataset.</p>
    /// <p>The list of available metrics supported by Autopilot and the default metric applied when you do not specify a metric name explicitly depend on the problem type.</p>
    /// <ul>
    /// <li> <p>For tabular problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: </p>
    /// <ul>
    /// <li> <p> Regression: <code>InferenceLatency</code>, <code>MAE</code>, <code>MSE</code>, <code>R2</code>, <code>RMSE</code> </p> </li>
    /// <li> <p> Binary classification: <code>Accuracy</code>, <code>AUC</code>, <code>BalancedAccuracy</code>, <code>F1</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>Precision</code>, <code>Recall</code> </p> </li>
    /// <li> <p> Multiclass classification: <code>Accuracy</code>, <code>BalancedAccuracy</code>, <code>F1macro</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>PrecisionMacro</code>, <code>RecallMacro</code> </p> </li>
    /// </ul> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics for classification and regression</a>.</p> </li>
    /// <li> <p>Default objective metrics:</p>
    /// <ul>
    /// <li> <p>Regression: <code>MSE</code>.</p> </li>
    /// <li> <p>Binary classification: <code>F1</code>.</p> </li>
    /// <li> <p>Multiclass classification: <code>Accuracy</code>.</p> </li>
    /// </ul> </li>
    /// </ul> </li>
    /// <li> <p>For image or text classification problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>Accuracy</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-data-format-and-metric.html">Autopilot metrics for text and image classification</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>Accuracy</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For time-series forecasting problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>RMSE</code>, <code>wQL</code>, <code>Average wQL</code>, <code>MASE</code>, <code>MAPE</code>, <code>WAPE</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/timeseries-objective-metric.html">Autopilot metrics for time-series forecasting</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>AverageWeightedQuantileLoss</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For text generation problem types (LLMs fine-tuning): Fine-tuning language models in Autopilot does not require setting the <code>AutoMLJobObjective</code> field. Autopilot fine-tunes LLMs without requiring multiple candidates to be trained and evaluated. Instead, using your dataset, Autopilot directly fine-tunes your target model to enhance a default objective metric, the cross-entropy loss. After fine-tuning a language model, you can evaluate the quality of its generated text using different metrics. For a list of the available metrics, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/llms-finetuning-models.html">Metrics for fine-tuning LLMs in Autopilot</a>.</p> </li>
    /// </ul>
    pub metric_name: ::std::option::Option<crate::types::AutoMlMetricEnum>,
}
impl AutoMlJobObjective {
    /// <p>The name of the objective metric used to measure the predictive quality of a machine learning system. During training, the model's parameters are updated iteratively to optimize its performance based on the feedback provided by the objective metric when evaluating the model on the validation dataset.</p>
    /// <p>The list of available metrics supported by Autopilot and the default metric applied when you do not specify a metric name explicitly depend on the problem type.</p>
    /// <ul>
    /// <li> <p>For tabular problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: </p>
    /// <ul>
    /// <li> <p> Regression: <code>InferenceLatency</code>, <code>MAE</code>, <code>MSE</code>, <code>R2</code>, <code>RMSE</code> </p> </li>
    /// <li> <p> Binary classification: <code>Accuracy</code>, <code>AUC</code>, <code>BalancedAccuracy</code>, <code>F1</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>Precision</code>, <code>Recall</code> </p> </li>
    /// <li> <p> Multiclass classification: <code>Accuracy</code>, <code>BalancedAccuracy</code>, <code>F1macro</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>PrecisionMacro</code>, <code>RecallMacro</code> </p> </li>
    /// </ul> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics for classification and regression</a>.</p> </li>
    /// <li> <p>Default objective metrics:</p>
    /// <ul>
    /// <li> <p>Regression: <code>MSE</code>.</p> </li>
    /// <li> <p>Binary classification: <code>F1</code>.</p> </li>
    /// <li> <p>Multiclass classification: <code>Accuracy</code>.</p> </li>
    /// </ul> </li>
    /// </ul> </li>
    /// <li> <p>For image or text classification problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>Accuracy</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-data-format-and-metric.html">Autopilot metrics for text and image classification</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>Accuracy</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For time-series forecasting problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>RMSE</code>, <code>wQL</code>, <code>Average wQL</code>, <code>MASE</code>, <code>MAPE</code>, <code>WAPE</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/timeseries-objective-metric.html">Autopilot metrics for time-series forecasting</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>AverageWeightedQuantileLoss</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For text generation problem types (LLMs fine-tuning): Fine-tuning language models in Autopilot does not require setting the <code>AutoMLJobObjective</code> field. Autopilot fine-tunes LLMs without requiring multiple candidates to be trained and evaluated. Instead, using your dataset, Autopilot directly fine-tunes your target model to enhance a default objective metric, the cross-entropy loss. After fine-tuning a language model, you can evaluate the quality of its generated text using different metrics. For a list of the available metrics, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/llms-finetuning-models.html">Metrics for fine-tuning LLMs in Autopilot</a>.</p> </li>
    /// </ul>
    pub fn metric_name(&self) -> ::std::option::Option<&crate::types::AutoMlMetricEnum> {
        self.metric_name.as_ref()
    }
}
impl AutoMlJobObjective {
    /// Creates a new builder-style object to manufacture [`AutoMlJobObjective`](crate::types::AutoMlJobObjective).
    pub fn builder() -> crate::types::builders::AutoMlJobObjectiveBuilder {
        crate::types::builders::AutoMlJobObjectiveBuilder::default()
    }
}

/// A builder for [`AutoMlJobObjective`](crate::types::AutoMlJobObjective).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct AutoMlJobObjectiveBuilder {
    pub(crate) metric_name: ::std::option::Option<crate::types::AutoMlMetricEnum>,
}
impl AutoMlJobObjectiveBuilder {
    /// <p>The name of the objective metric used to measure the predictive quality of a machine learning system. During training, the model's parameters are updated iteratively to optimize its performance based on the feedback provided by the objective metric when evaluating the model on the validation dataset.</p>
    /// <p>The list of available metrics supported by Autopilot and the default metric applied when you do not specify a metric name explicitly depend on the problem type.</p>
    /// <ul>
    /// <li> <p>For tabular problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: </p>
    /// <ul>
    /// <li> <p> Regression: <code>InferenceLatency</code>, <code>MAE</code>, <code>MSE</code>, <code>R2</code>, <code>RMSE</code> </p> </li>
    /// <li> <p> Binary classification: <code>Accuracy</code>, <code>AUC</code>, <code>BalancedAccuracy</code>, <code>F1</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>Precision</code>, <code>Recall</code> </p> </li>
    /// <li> <p> Multiclass classification: <code>Accuracy</code>, <code>BalancedAccuracy</code>, <code>F1macro</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>PrecisionMacro</code>, <code>RecallMacro</code> </p> </li>
    /// </ul> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics for classification and regression</a>.</p> </li>
    /// <li> <p>Default objective metrics:</p>
    /// <ul>
    /// <li> <p>Regression: <code>MSE</code>.</p> </li>
    /// <li> <p>Binary classification: <code>F1</code>.</p> </li>
    /// <li> <p>Multiclass classification: <code>Accuracy</code>.</p> </li>
    /// </ul> </li>
    /// </ul> </li>
    /// <li> <p>For image or text classification problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>Accuracy</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-data-format-and-metric.html">Autopilot metrics for text and image classification</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>Accuracy</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For time-series forecasting problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>RMSE</code>, <code>wQL</code>, <code>Average wQL</code>, <code>MASE</code>, <code>MAPE</code>, <code>WAPE</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/timeseries-objective-metric.html">Autopilot metrics for time-series forecasting</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>AverageWeightedQuantileLoss</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For text generation problem types (LLMs fine-tuning): Fine-tuning language models in Autopilot does not require setting the <code>AutoMLJobObjective</code> field. Autopilot fine-tunes LLMs without requiring multiple candidates to be trained and evaluated. Instead, using your dataset, Autopilot directly fine-tunes your target model to enhance a default objective metric, the cross-entropy loss. After fine-tuning a language model, you can evaluate the quality of its generated text using different metrics. For a list of the available metrics, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/llms-finetuning-models.html">Metrics for fine-tuning LLMs in Autopilot</a>.</p> </li>
    /// </ul>
    /// This field is required.
    pub fn metric_name(mut self, input: crate::types::AutoMlMetricEnum) -> Self {
        self.metric_name = ::std::option::Option::Some(input);
        self
    }
    /// <p>The name of the objective metric used to measure the predictive quality of a machine learning system. During training, the model's parameters are updated iteratively to optimize its performance based on the feedback provided by the objective metric when evaluating the model on the validation dataset.</p>
    /// <p>The list of available metrics supported by Autopilot and the default metric applied when you do not specify a metric name explicitly depend on the problem type.</p>
    /// <ul>
    /// <li> <p>For tabular problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: </p>
    /// <ul>
    /// <li> <p> Regression: <code>InferenceLatency</code>, <code>MAE</code>, <code>MSE</code>, <code>R2</code>, <code>RMSE</code> </p> </li>
    /// <li> <p> Binary classification: <code>Accuracy</code>, <code>AUC</code>, <code>BalancedAccuracy</code>, <code>F1</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>Precision</code>, <code>Recall</code> </p> </li>
    /// <li> <p> Multiclass classification: <code>Accuracy</code>, <code>BalancedAccuracy</code>, <code>F1macro</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>PrecisionMacro</code>, <code>RecallMacro</code> </p> </li>
    /// </ul> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics for classification and regression</a>.</p> </li>
    /// <li> <p>Default objective metrics:</p>
    /// <ul>
    /// <li> <p>Regression: <code>MSE</code>.</p> </li>
    /// <li> <p>Binary classification: <code>F1</code>.</p> </li>
    /// <li> <p>Multiclass classification: <code>Accuracy</code>.</p> </li>
    /// </ul> </li>
    /// </ul> </li>
    /// <li> <p>For image or text classification problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>Accuracy</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-data-format-and-metric.html">Autopilot metrics for text and image classification</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>Accuracy</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For time-series forecasting problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>RMSE</code>, <code>wQL</code>, <code>Average wQL</code>, <code>MASE</code>, <code>MAPE</code>, <code>WAPE</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/timeseries-objective-metric.html">Autopilot metrics for time-series forecasting</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>AverageWeightedQuantileLoss</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For text generation problem types (LLMs fine-tuning): Fine-tuning language models in Autopilot does not require setting the <code>AutoMLJobObjective</code> field. Autopilot fine-tunes LLMs without requiring multiple candidates to be trained and evaluated. Instead, using your dataset, Autopilot directly fine-tunes your target model to enhance a default objective metric, the cross-entropy loss. After fine-tuning a language model, you can evaluate the quality of its generated text using different metrics. For a list of the available metrics, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/llms-finetuning-models.html">Metrics for fine-tuning LLMs in Autopilot</a>.</p> </li>
    /// </ul>
    pub fn set_metric_name(mut self, input: ::std::option::Option<crate::types::AutoMlMetricEnum>) -> Self {
        self.metric_name = input;
        self
    }
    /// <p>The name of the objective metric used to measure the predictive quality of a machine learning system. During training, the model's parameters are updated iteratively to optimize its performance based on the feedback provided by the objective metric when evaluating the model on the validation dataset.</p>
    /// <p>The list of available metrics supported by Autopilot and the default metric applied when you do not specify a metric name explicitly depend on the problem type.</p>
    /// <ul>
    /// <li> <p>For tabular problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: </p>
    /// <ul>
    /// <li> <p> Regression: <code>InferenceLatency</code>, <code>MAE</code>, <code>MSE</code>, <code>R2</code>, <code>RMSE</code> </p> </li>
    /// <li> <p> Binary classification: <code>Accuracy</code>, <code>AUC</code>, <code>BalancedAccuracy</code>, <code>F1</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>Precision</code>, <code>Recall</code> </p> </li>
    /// <li> <p> Multiclass classification: <code>Accuracy</code>, <code>BalancedAccuracy</code>, <code>F1macro</code>, <code>InferenceLatency</code>, <code>LogLoss</code>, <code>PrecisionMacro</code>, <code>RecallMacro</code> </p> </li>
    /// </ul> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics for classification and regression</a>.</p> </li>
    /// <li> <p>Default objective metrics:</p>
    /// <ul>
    /// <li> <p>Regression: <code>MSE</code>.</p> </li>
    /// <li> <p>Binary classification: <code>F1</code>.</p> </li>
    /// <li> <p>Multiclass classification: <code>Accuracy</code>.</p> </li>
    /// </ul> </li>
    /// </ul> </li>
    /// <li> <p>For image or text classification problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>Accuracy</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-data-format-and-metric.html">Autopilot metrics for text and image classification</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>Accuracy</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For time-series forecasting problem types:</p>
    /// <ul>
    /// <li> <p>List of available metrics: <code>RMSE</code>, <code>wQL</code>, <code>Average wQL</code>, <code>MASE</code>, <code>MAPE</code>, <code>WAPE</code> </p> <p>For a description of each metric, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/timeseries-objective-metric.html">Autopilot metrics for time-series forecasting</a>.</p> </li>
    /// <li> <p>Default objective metrics: <code>AverageWeightedQuantileLoss</code> </p> </li>
    /// </ul> </li>
    /// <li> <p>For text generation problem types (LLMs fine-tuning): Fine-tuning language models in Autopilot does not require setting the <code>AutoMLJobObjective</code> field. Autopilot fine-tunes LLMs without requiring multiple candidates to be trained and evaluated. Instead, using your dataset, Autopilot directly fine-tunes your target model to enhance a default objective metric, the cross-entropy loss. After fine-tuning a language model, you can evaluate the quality of its generated text using different metrics. For a list of the available metrics, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/llms-finetuning-models.html">Metrics for fine-tuning LLMs in Autopilot</a>.</p> </li>
    /// </ul>
    pub fn get_metric_name(&self) -> &::std::option::Option<crate::types::AutoMlMetricEnum> {
        &self.metric_name
    }
    /// Consumes the builder and constructs a [`AutoMlJobObjective`](crate::types::AutoMlJobObjective).
    pub fn build(self) -> crate::types::AutoMlJobObjective {
        crate::types::AutoMlJobObjective {
            metric_name: self.metric_name,
        }
    }
}
