// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct CreateDatasetImportJobInput {
    /// <p>The name for the dataset import job.</p>
    #[doc(hidden)]
    pub job_name: std::option::Option<std::string::String>,
    /// <p>The ARN of the dataset that receives the imported data.</p>
    #[doc(hidden)]
    pub dataset_arn: std::option::Option<std::string::String>,
    /// <p>The Amazon S3 bucket that contains the training data to import.</p>
    #[doc(hidden)]
    pub data_source: std::option::Option<crate::types::DataSource>,
    /// <p>The ARN of the IAM role that has permissions to read from the Amazon S3 data source.</p>
    #[doc(hidden)]
    pub role_arn: std::option::Option<std::string::String>,
    /// <p>A list of <a href="https://docs.aws.amazon.com/personalize/latest/dev/tagging-resources.html">tags</a> to apply to the dataset import job.</p>
    #[doc(hidden)]
    pub tags: std::option::Option<std::vec::Vec<crate::types::Tag>>,
    /// <p>Specify how to add the new records to an existing dataset. The default import mode is <code>FULL</code>. If you haven't imported bulk records into the dataset previously, you can only specify <code>FULL</code>.</p>
    /// <ul>
    /// <li> <p>Specify <code>FULL</code> to overwrite all existing bulk data in your dataset. Data you imported individually is not replaced.</p> </li>
    /// <li> <p>Specify <code>INCREMENTAL</code> to append the new records to the existing data in your dataset. Amazon Personalize replaces any record with the same ID with the new one.</p> </li>
    /// </ul>
    #[doc(hidden)]
    pub import_mode: std::option::Option<crate::types::ImportMode>,
    /// <p>If you created a metric attribution, specify whether to publish metrics for this import job to Amazon S3</p>
    #[doc(hidden)]
    pub publish_attribution_metrics_to_s3: std::option::Option<bool>,
}
impl CreateDatasetImportJobInput {
    /// <p>The name for the dataset import job.</p>
    pub fn job_name(&self) -> std::option::Option<&str> {
        self.job_name.as_deref()
    }
    /// <p>The ARN of the dataset that receives the imported data.</p>
    pub fn dataset_arn(&self) -> std::option::Option<&str> {
        self.dataset_arn.as_deref()
    }
    /// <p>The Amazon S3 bucket that contains the training data to import.</p>
    pub fn data_source(&self) -> std::option::Option<&crate::types::DataSource> {
        self.data_source.as_ref()
    }
    /// <p>The ARN of the IAM role that has permissions to read from the Amazon S3 data source.</p>
    pub fn role_arn(&self) -> std::option::Option<&str> {
        self.role_arn.as_deref()
    }
    /// <p>A list of <a href="https://docs.aws.amazon.com/personalize/latest/dev/tagging-resources.html">tags</a> to apply to the dataset import job.</p>
    pub fn tags(&self) -> std::option::Option<&[crate::types::Tag]> {
        self.tags.as_deref()
    }
    /// <p>Specify how to add the new records to an existing dataset. The default import mode is <code>FULL</code>. If you haven't imported bulk records into the dataset previously, you can only specify <code>FULL</code>.</p>
    /// <ul>
    /// <li> <p>Specify <code>FULL</code> to overwrite all existing bulk data in your dataset. Data you imported individually is not replaced.</p> </li>
    /// <li> <p>Specify <code>INCREMENTAL</code> to append the new records to the existing data in your dataset. Amazon Personalize replaces any record with the same ID with the new one.</p> </li>
    /// </ul>
    pub fn import_mode(&self) -> std::option::Option<&crate::types::ImportMode> {
        self.import_mode.as_ref()
    }
    /// <p>If you created a metric attribution, specify whether to publish metrics for this import job to Amazon S3</p>
    pub fn publish_attribution_metrics_to_s3(&self) -> std::option::Option<bool> {
        self.publish_attribution_metrics_to_s3
    }
}
impl CreateDatasetImportJobInput {
    /// Creates a new builder-style object to manufacture [`CreateDatasetImportJobInput`](crate::operation::create_dataset_import_job::CreateDatasetImportJobInput).
    pub fn builder(
    ) -> crate::operation::create_dataset_import_job::builders::CreateDatasetImportJobInputBuilder
    {
        crate::operation::create_dataset_import_job::builders::CreateDatasetImportJobInputBuilder::default()
    }
}

/// A builder for [`CreateDatasetImportJobInput`](crate::operation::create_dataset_import_job::CreateDatasetImportJobInput).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct CreateDatasetImportJobInputBuilder {
    pub(crate) job_name: std::option::Option<std::string::String>,
    pub(crate) dataset_arn: std::option::Option<std::string::String>,
    pub(crate) data_source: std::option::Option<crate::types::DataSource>,
    pub(crate) role_arn: std::option::Option<std::string::String>,
    pub(crate) tags: std::option::Option<std::vec::Vec<crate::types::Tag>>,
    pub(crate) import_mode: std::option::Option<crate::types::ImportMode>,
    pub(crate) publish_attribution_metrics_to_s3: std::option::Option<bool>,
}
impl CreateDatasetImportJobInputBuilder {
    /// <p>The name for the dataset import job.</p>
    pub fn job_name(mut self, input: impl Into<std::string::String>) -> Self {
        self.job_name = Some(input.into());
        self
    }
    /// <p>The name for the dataset import job.</p>
    pub fn set_job_name(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.job_name = input;
        self
    }
    /// <p>The ARN of the dataset that receives the imported data.</p>
    pub fn dataset_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.dataset_arn = Some(input.into());
        self
    }
    /// <p>The ARN of the dataset that receives the imported data.</p>
    pub fn set_dataset_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.dataset_arn = input;
        self
    }
    /// <p>The Amazon S3 bucket that contains the training data to import.</p>
    pub fn data_source(mut self, input: crate::types::DataSource) -> Self {
        self.data_source = Some(input);
        self
    }
    /// <p>The Amazon S3 bucket that contains the training data to import.</p>
    pub fn set_data_source(mut self, input: std::option::Option<crate::types::DataSource>) -> Self {
        self.data_source = input;
        self
    }
    /// <p>The ARN of the IAM role that has permissions to read from the Amazon S3 data source.</p>
    pub fn role_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.role_arn = Some(input.into());
        self
    }
    /// <p>The ARN of the IAM role that has permissions to read from the Amazon S3 data source.</p>
    pub fn set_role_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.role_arn = input;
        self
    }
    /// Appends an item to `tags`.
    ///
    /// To override the contents of this collection use [`set_tags`](Self::set_tags).
    ///
    /// <p>A list of <a href="https://docs.aws.amazon.com/personalize/latest/dev/tagging-resources.html">tags</a> to apply to the dataset import job.</p>
    pub fn tags(mut self, input: crate::types::Tag) -> Self {
        let mut v = self.tags.unwrap_or_default();
        v.push(input);
        self.tags = Some(v);
        self
    }
    /// <p>A list of <a href="https://docs.aws.amazon.com/personalize/latest/dev/tagging-resources.html">tags</a> to apply to the dataset import job.</p>
    pub fn set_tags(
        mut self,
        input: std::option::Option<std::vec::Vec<crate::types::Tag>>,
    ) -> Self {
        self.tags = input;
        self
    }
    /// <p>Specify how to add the new records to an existing dataset. The default import mode is <code>FULL</code>. If you haven't imported bulk records into the dataset previously, you can only specify <code>FULL</code>.</p>
    /// <ul>
    /// <li> <p>Specify <code>FULL</code> to overwrite all existing bulk data in your dataset. Data you imported individually is not replaced.</p> </li>
    /// <li> <p>Specify <code>INCREMENTAL</code> to append the new records to the existing data in your dataset. Amazon Personalize replaces any record with the same ID with the new one.</p> </li>
    /// </ul>
    pub fn import_mode(mut self, input: crate::types::ImportMode) -> Self {
        self.import_mode = Some(input);
        self
    }
    /// <p>Specify how to add the new records to an existing dataset. The default import mode is <code>FULL</code>. If you haven't imported bulk records into the dataset previously, you can only specify <code>FULL</code>.</p>
    /// <ul>
    /// <li> <p>Specify <code>FULL</code> to overwrite all existing bulk data in your dataset. Data you imported individually is not replaced.</p> </li>
    /// <li> <p>Specify <code>INCREMENTAL</code> to append the new records to the existing data in your dataset. Amazon Personalize replaces any record with the same ID with the new one.</p> </li>
    /// </ul>
    pub fn set_import_mode(mut self, input: std::option::Option<crate::types::ImportMode>) -> Self {
        self.import_mode = input;
        self
    }
    /// <p>If you created a metric attribution, specify whether to publish metrics for this import job to Amazon S3</p>
    pub fn publish_attribution_metrics_to_s3(mut self, input: bool) -> Self {
        self.publish_attribution_metrics_to_s3 = Some(input);
        self
    }
    /// <p>If you created a metric attribution, specify whether to publish metrics for this import job to Amazon S3</p>
    pub fn set_publish_attribution_metrics_to_s3(
        mut self,
        input: std::option::Option<bool>,
    ) -> Self {
        self.publish_attribution_metrics_to_s3 = input;
        self
    }
    /// Consumes the builder and constructs a [`CreateDatasetImportJobInput`](crate::operation::create_dataset_import_job::CreateDatasetImportJobInput).
    pub fn build(
        self,
    ) -> Result<
        crate::operation::create_dataset_import_job::CreateDatasetImportJobInput,
        aws_smithy_http::operation::error::BuildError,
    > {
        Ok(
            crate::operation::create_dataset_import_job::CreateDatasetImportJobInput {
                job_name: self.job_name,
                dataset_arn: self.dataset_arn,
                data_source: self.data_source,
                role_arn: self.role_arn,
                tags: self.tags,
                import_mode: self.import_mode,
                publish_attribution_metrics_to_s3: self.publish_attribution_metrics_to_s3,
            },
        )
    }
}
