// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Describes the data source that contains the data to upload to a dataset, or the list of records to delete from Amazon Personalize.</p>
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct DataSource {
    /// <p>For dataset import jobs, the path to the Amazon S3 bucket where the data that you want to upload to your dataset is stored. For data deletion jobs, the path to the Amazon S3 bucket that stores the list of records to delete.</p>
    /// <p>For example:</p>
    /// <p><code>s3://bucket-name/folder-name/fileName.csv</code></p>
    /// <p>If your CSV files are in a folder in your Amazon S3 bucket and you want your import job or data deletion job to consider multiple files, you can specify the path to the folder. With a data deletion job, Amazon Personalize uses all files in the folder and any sub folder. Use the following syntax with a <code>/</code> after the folder name:</p>
    /// <p><code>s3://bucket-name/folder-name/</code></p>
    pub data_location: ::std::option::Option<::std::string::String>,
}
impl DataSource {
    /// <p>For dataset import jobs, the path to the Amazon S3 bucket where the data that you want to upload to your dataset is stored. For data deletion jobs, the path to the Amazon S3 bucket that stores the list of records to delete.</p>
    /// <p>For example:</p>
    /// <p><code>s3://bucket-name/folder-name/fileName.csv</code></p>
    /// <p>If your CSV files are in a folder in your Amazon S3 bucket and you want your import job or data deletion job to consider multiple files, you can specify the path to the folder. With a data deletion job, Amazon Personalize uses all files in the folder and any sub folder. Use the following syntax with a <code>/</code> after the folder name:</p>
    /// <p><code>s3://bucket-name/folder-name/</code></p>
    pub fn data_location(&self) -> ::std::option::Option<&str> {
        self.data_location.as_deref()
    }
}
impl DataSource {
    /// Creates a new builder-style object to manufacture [`DataSource`](crate::types::DataSource).
    pub fn builder() -> crate::types::builders::DataSourceBuilder {
        crate::types::builders::DataSourceBuilder::default()
    }
}

/// A builder for [`DataSource`](crate::types::DataSource).
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
pub struct DataSourceBuilder {
    pub(crate) data_location: ::std::option::Option<::std::string::String>,
}
impl DataSourceBuilder {
    /// <p>For dataset import jobs, the path to the Amazon S3 bucket where the data that you want to upload to your dataset is stored. For data deletion jobs, the path to the Amazon S3 bucket that stores the list of records to delete.</p>
    /// <p>For example:</p>
    /// <p><code>s3://bucket-name/folder-name/fileName.csv</code></p>
    /// <p>If your CSV files are in a folder in your Amazon S3 bucket and you want your import job or data deletion job to consider multiple files, you can specify the path to the folder. With a data deletion job, Amazon Personalize uses all files in the folder and any sub folder. Use the following syntax with a <code>/</code> after the folder name:</p>
    /// <p><code>s3://bucket-name/folder-name/</code></p>
    pub fn data_location(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.data_location = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>For dataset import jobs, the path to the Amazon S3 bucket where the data that you want to upload to your dataset is stored. For data deletion jobs, the path to the Amazon S3 bucket that stores the list of records to delete.</p>
    /// <p>For example:</p>
    /// <p><code>s3://bucket-name/folder-name/fileName.csv</code></p>
    /// <p>If your CSV files are in a folder in your Amazon S3 bucket and you want your import job or data deletion job to consider multiple files, you can specify the path to the folder. With a data deletion job, Amazon Personalize uses all files in the folder and any sub folder. Use the following syntax with a <code>/</code> after the folder name:</p>
    /// <p><code>s3://bucket-name/folder-name/</code></p>
    pub fn set_data_location(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.data_location = input;
        self
    }
    /// <p>For dataset import jobs, the path to the Amazon S3 bucket where the data that you want to upload to your dataset is stored. For data deletion jobs, the path to the Amazon S3 bucket that stores the list of records to delete.</p>
    /// <p>For example:</p>
    /// <p><code>s3://bucket-name/folder-name/fileName.csv</code></p>
    /// <p>If your CSV files are in a folder in your Amazon S3 bucket and you want your import job or data deletion job to consider multiple files, you can specify the path to the folder. With a data deletion job, Amazon Personalize uses all files in the folder and any sub folder. Use the following syntax with a <code>/</code> after the folder name:</p>
    /// <p><code>s3://bucket-name/folder-name/</code></p>
    pub fn get_data_location(&self) -> &::std::option::Option<::std::string::String> {
        &self.data_location
    }
    /// Consumes the builder and constructs a [`DataSource`](crate::types::DataSource).
    pub fn build(self) -> crate::types::DataSource {
        crate::types::DataSource {
            data_location: self.data_location,
        }
    }
}
