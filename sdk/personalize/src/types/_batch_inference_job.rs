// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.

/// <p>Contains information on a batch inference job.</p>
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::fmt::Debug)]
pub struct BatchInferenceJob {
    /// <p>The name of the batch inference job.</p>
    #[doc(hidden)]
    pub job_name: std::option::Option<std::string::String>,
    /// <p>The Amazon Resource Name (ARN) of the batch inference job.</p>
    #[doc(hidden)]
    pub batch_inference_job_arn: std::option::Option<std::string::String>,
    /// <p>The ARN of the filter used on the batch inference job.</p>
    #[doc(hidden)]
    pub filter_arn: std::option::Option<std::string::String>,
    /// <p>If the batch inference job failed, the reason for the failure.</p>
    #[doc(hidden)]
    pub failure_reason: std::option::Option<std::string::String>,
    /// <p>The Amazon Resource Name (ARN) of the solution version from which the batch inference job was created.</p>
    #[doc(hidden)]
    pub solution_version_arn: std::option::Option<std::string::String>,
    /// <p>The number of recommendations generated by the batch inference job. This number includes the error messages generated for failed input records.</p>
    #[doc(hidden)]
    pub num_results: std::option::Option<i32>,
    /// <p>The Amazon S3 path that leads to the input data used to generate the batch inference job.</p>
    #[doc(hidden)]
    pub job_input: std::option::Option<crate::types::BatchInferenceJobInput>,
    /// <p>The Amazon S3 bucket that contains the output data generated by the batch inference job.</p>
    #[doc(hidden)]
    pub job_output: std::option::Option<crate::types::BatchInferenceJobOutput>,
    /// <p>A string to string map of the configuration details of a batch inference job.</p>
    #[doc(hidden)]
    pub batch_inference_job_config: std::option::Option<crate::types::BatchInferenceJobConfig>,
    /// <p>The ARN of the Amazon Identity and Access Management (IAM) role that requested the batch inference job.</p>
    #[doc(hidden)]
    pub role_arn: std::option::Option<std::string::String>,
    /// <p>The status of the batch inference job. The status is one of the following values:</p>
    /// <ul>
    /// <li> <p>PENDING</p> </li>
    /// <li> <p>IN PROGRESS</p> </li>
    /// <li> <p>ACTIVE</p> </li>
    /// <li> <p>CREATE FAILED</p> </li>
    /// </ul>
    #[doc(hidden)]
    pub status: std::option::Option<std::string::String>,
    /// <p>The time at which the batch inference job was created.</p>
    #[doc(hidden)]
    pub creation_date_time: std::option::Option<aws_smithy_types::DateTime>,
    /// <p>The time at which the batch inference job was last updated.</p>
    #[doc(hidden)]
    pub last_updated_date_time: std::option::Option<aws_smithy_types::DateTime>,
}
impl BatchInferenceJob {
    /// <p>The name of the batch inference job.</p>
    pub fn job_name(&self) -> std::option::Option<&str> {
        self.job_name.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) of the batch inference job.</p>
    pub fn batch_inference_job_arn(&self) -> std::option::Option<&str> {
        self.batch_inference_job_arn.as_deref()
    }
    /// <p>The ARN of the filter used on the batch inference job.</p>
    pub fn filter_arn(&self) -> std::option::Option<&str> {
        self.filter_arn.as_deref()
    }
    /// <p>If the batch inference job failed, the reason for the failure.</p>
    pub fn failure_reason(&self) -> std::option::Option<&str> {
        self.failure_reason.as_deref()
    }
    /// <p>The Amazon Resource Name (ARN) of the solution version from which the batch inference job was created.</p>
    pub fn solution_version_arn(&self) -> std::option::Option<&str> {
        self.solution_version_arn.as_deref()
    }
    /// <p>The number of recommendations generated by the batch inference job. This number includes the error messages generated for failed input records.</p>
    pub fn num_results(&self) -> std::option::Option<i32> {
        self.num_results
    }
    /// <p>The Amazon S3 path that leads to the input data used to generate the batch inference job.</p>
    pub fn job_input(&self) -> std::option::Option<&crate::types::BatchInferenceJobInput> {
        self.job_input.as_ref()
    }
    /// <p>The Amazon S3 bucket that contains the output data generated by the batch inference job.</p>
    pub fn job_output(&self) -> std::option::Option<&crate::types::BatchInferenceJobOutput> {
        self.job_output.as_ref()
    }
    /// <p>A string to string map of the configuration details of a batch inference job.</p>
    pub fn batch_inference_job_config(
        &self,
    ) -> std::option::Option<&crate::types::BatchInferenceJobConfig> {
        self.batch_inference_job_config.as_ref()
    }
    /// <p>The ARN of the Amazon Identity and Access Management (IAM) role that requested the batch inference job.</p>
    pub fn role_arn(&self) -> std::option::Option<&str> {
        self.role_arn.as_deref()
    }
    /// <p>The status of the batch inference job. The status is one of the following values:</p>
    /// <ul>
    /// <li> <p>PENDING</p> </li>
    /// <li> <p>IN PROGRESS</p> </li>
    /// <li> <p>ACTIVE</p> </li>
    /// <li> <p>CREATE FAILED</p> </li>
    /// </ul>
    pub fn status(&self) -> std::option::Option<&str> {
        self.status.as_deref()
    }
    /// <p>The time at which the batch inference job was created.</p>
    pub fn creation_date_time(&self) -> std::option::Option<&aws_smithy_types::DateTime> {
        self.creation_date_time.as_ref()
    }
    /// <p>The time at which the batch inference job was last updated.</p>
    pub fn last_updated_date_time(&self) -> std::option::Option<&aws_smithy_types::DateTime> {
        self.last_updated_date_time.as_ref()
    }
}
impl BatchInferenceJob {
    /// Creates a new builder-style object to manufacture [`BatchInferenceJob`](crate::types::BatchInferenceJob).
    pub fn builder() -> crate::types::builders::BatchInferenceJobBuilder {
        crate::types::builders::BatchInferenceJobBuilder::default()
    }
}

/// A builder for [`BatchInferenceJob`](crate::types::BatchInferenceJob).
#[non_exhaustive]
#[derive(std::clone::Clone, std::cmp::PartialEq, std::default::Default, std::fmt::Debug)]
pub struct BatchInferenceJobBuilder {
    pub(crate) job_name: std::option::Option<std::string::String>,
    pub(crate) batch_inference_job_arn: std::option::Option<std::string::String>,
    pub(crate) filter_arn: std::option::Option<std::string::String>,
    pub(crate) failure_reason: std::option::Option<std::string::String>,
    pub(crate) solution_version_arn: std::option::Option<std::string::String>,
    pub(crate) num_results: std::option::Option<i32>,
    pub(crate) job_input: std::option::Option<crate::types::BatchInferenceJobInput>,
    pub(crate) job_output: std::option::Option<crate::types::BatchInferenceJobOutput>,
    pub(crate) batch_inference_job_config:
        std::option::Option<crate::types::BatchInferenceJobConfig>,
    pub(crate) role_arn: std::option::Option<std::string::String>,
    pub(crate) status: std::option::Option<std::string::String>,
    pub(crate) creation_date_time: std::option::Option<aws_smithy_types::DateTime>,
    pub(crate) last_updated_date_time: std::option::Option<aws_smithy_types::DateTime>,
}
impl BatchInferenceJobBuilder {
    /// <p>The name of the batch inference job.</p>
    pub fn job_name(mut self, input: impl Into<std::string::String>) -> Self {
        self.job_name = Some(input.into());
        self
    }
    /// <p>The name of the batch inference job.</p>
    pub fn set_job_name(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.job_name = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the batch inference job.</p>
    pub fn batch_inference_job_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.batch_inference_job_arn = Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the batch inference job.</p>
    pub fn set_batch_inference_job_arn(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.batch_inference_job_arn = input;
        self
    }
    /// <p>The ARN of the filter used on the batch inference job.</p>
    pub fn filter_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.filter_arn = Some(input.into());
        self
    }
    /// <p>The ARN of the filter used on the batch inference job.</p>
    pub fn set_filter_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.filter_arn = input;
        self
    }
    /// <p>If the batch inference job failed, the reason for the failure.</p>
    pub fn failure_reason(mut self, input: impl Into<std::string::String>) -> Self {
        self.failure_reason = Some(input.into());
        self
    }
    /// <p>If the batch inference job failed, the reason for the failure.</p>
    pub fn set_failure_reason(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.failure_reason = input;
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the solution version from which the batch inference job was created.</p>
    pub fn solution_version_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.solution_version_arn = Some(input.into());
        self
    }
    /// <p>The Amazon Resource Name (ARN) of the solution version from which the batch inference job was created.</p>
    pub fn set_solution_version_arn(
        mut self,
        input: std::option::Option<std::string::String>,
    ) -> Self {
        self.solution_version_arn = input;
        self
    }
    /// <p>The number of recommendations generated by the batch inference job. This number includes the error messages generated for failed input records.</p>
    pub fn num_results(mut self, input: i32) -> Self {
        self.num_results = Some(input);
        self
    }
    /// <p>The number of recommendations generated by the batch inference job. This number includes the error messages generated for failed input records.</p>
    pub fn set_num_results(mut self, input: std::option::Option<i32>) -> Self {
        self.num_results = input;
        self
    }
    /// <p>The Amazon S3 path that leads to the input data used to generate the batch inference job.</p>
    pub fn job_input(mut self, input: crate::types::BatchInferenceJobInput) -> Self {
        self.job_input = Some(input);
        self
    }
    /// <p>The Amazon S3 path that leads to the input data used to generate the batch inference job.</p>
    pub fn set_job_input(
        mut self,
        input: std::option::Option<crate::types::BatchInferenceJobInput>,
    ) -> Self {
        self.job_input = input;
        self
    }
    /// <p>The Amazon S3 bucket that contains the output data generated by the batch inference job.</p>
    pub fn job_output(mut self, input: crate::types::BatchInferenceJobOutput) -> Self {
        self.job_output = Some(input);
        self
    }
    /// <p>The Amazon S3 bucket that contains the output data generated by the batch inference job.</p>
    pub fn set_job_output(
        mut self,
        input: std::option::Option<crate::types::BatchInferenceJobOutput>,
    ) -> Self {
        self.job_output = input;
        self
    }
    /// <p>A string to string map of the configuration details of a batch inference job.</p>
    pub fn batch_inference_job_config(
        mut self,
        input: crate::types::BatchInferenceJobConfig,
    ) -> Self {
        self.batch_inference_job_config = Some(input);
        self
    }
    /// <p>A string to string map of the configuration details of a batch inference job.</p>
    pub fn set_batch_inference_job_config(
        mut self,
        input: std::option::Option<crate::types::BatchInferenceJobConfig>,
    ) -> Self {
        self.batch_inference_job_config = input;
        self
    }
    /// <p>The ARN of the Amazon Identity and Access Management (IAM) role that requested the batch inference job.</p>
    pub fn role_arn(mut self, input: impl Into<std::string::String>) -> Self {
        self.role_arn = Some(input.into());
        self
    }
    /// <p>The ARN of the Amazon Identity and Access Management (IAM) role that requested the batch inference job.</p>
    pub fn set_role_arn(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.role_arn = input;
        self
    }
    /// <p>The status of the batch inference job. The status is one of the following values:</p>
    /// <ul>
    /// <li> <p>PENDING</p> </li>
    /// <li> <p>IN PROGRESS</p> </li>
    /// <li> <p>ACTIVE</p> </li>
    /// <li> <p>CREATE FAILED</p> </li>
    /// </ul>
    pub fn status(mut self, input: impl Into<std::string::String>) -> Self {
        self.status = Some(input.into());
        self
    }
    /// <p>The status of the batch inference job. The status is one of the following values:</p>
    /// <ul>
    /// <li> <p>PENDING</p> </li>
    /// <li> <p>IN PROGRESS</p> </li>
    /// <li> <p>ACTIVE</p> </li>
    /// <li> <p>CREATE FAILED</p> </li>
    /// </ul>
    pub fn set_status(mut self, input: std::option::Option<std::string::String>) -> Self {
        self.status = input;
        self
    }
    /// <p>The time at which the batch inference job was created.</p>
    pub fn creation_date_time(mut self, input: aws_smithy_types::DateTime) -> Self {
        self.creation_date_time = Some(input);
        self
    }
    /// <p>The time at which the batch inference job was created.</p>
    pub fn set_creation_date_time(
        mut self,
        input: std::option::Option<aws_smithy_types::DateTime>,
    ) -> Self {
        self.creation_date_time = input;
        self
    }
    /// <p>The time at which the batch inference job was last updated.</p>
    pub fn last_updated_date_time(mut self, input: aws_smithy_types::DateTime) -> Self {
        self.last_updated_date_time = Some(input);
        self
    }
    /// <p>The time at which the batch inference job was last updated.</p>
    pub fn set_last_updated_date_time(
        mut self,
        input: std::option::Option<aws_smithy_types::DateTime>,
    ) -> Self {
        self.last_updated_date_time = input;
        self
    }
    /// Consumes the builder and constructs a [`BatchInferenceJob`](crate::types::BatchInferenceJob).
    pub fn build(self) -> crate::types::BatchInferenceJob {
        crate::types::BatchInferenceJob {
            job_name: self.job_name,
            batch_inference_job_arn: self.batch_inference_job_arn,
            filter_arn: self.filter_arn,
            failure_reason: self.failure_reason,
            solution_version_arn: self.solution_version_arn,
            num_results: self.num_results,
            job_input: self.job_input,
            job_output: self.job_output,
            batch_inference_job_config: self.batch_inference_job_config,
            role_arn: self.role_arn,
            status: self.status,
            creation_date_time: self.creation_date_time,
            last_updated_date_time: self.last_updated_date_time,
        }
    }
}
